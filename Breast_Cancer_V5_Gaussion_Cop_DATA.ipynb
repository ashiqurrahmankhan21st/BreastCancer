{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashiqurrahmankhan21st/BreastCancer/blob/main/Breast_Cancer_V5_Gaussion_Cop_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EUMYU_d6_J-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "#import keras_metrics\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gaussion_Cop\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/GC_SDV_BreastCancer.csv\")\n",
        "del df['Unnamed: 0']\n",
        "df"
      ],
      "metadata": {
        "id": "Ntdqq4T_aGXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "942696e4-3266-43ca-ddfd-628766eb59fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0            B       12.759         17.02           80.93      509.4   \n",
              "1            M       11.306         19.79           74.48      390.1   \n",
              "2            M       12.325         24.63           81.38      484.2   \n",
              "3            B       10.635         24.89           68.67      341.9   \n",
              "4            M       28.110         29.74          188.50     2286.8   \n",
              "...        ...          ...           ...             ...        ...   \n",
              "9995         M       13.590         25.97           88.89      597.2   \n",
              "9996         B       13.292         19.63           86.72      554.2   \n",
              "9997         B        9.346         22.18           60.79      275.9   \n",
              "9998         B       10.727         19.67           68.48      352.5   \n",
              "9999         B       11.389         15.72           71.71      418.7   \n",
              "\n",
              "      smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0             0.06687           0.03913        0.025046             0.016961   \n",
              "1             0.10894           0.10373        0.114211             0.053879   \n",
              "2             0.11996           0.16272        0.067551             0.047048   \n",
              "3             0.08418           0.08830        0.027259             0.006019   \n",
              "4             0.06442           0.06746        0.191878             0.115942   \n",
              "...               ...               ...             ...                  ...   \n",
              "9995          0.09826           0.10244        0.048636             0.036183   \n",
              "9996          0.09190           0.13122        0.059075             0.031142   \n",
              "9997          0.10120           0.09687        0.040308             0.022831   \n",
              "9998          0.08418           0.07468        0.078046             0.030488   \n",
              "9999          0.08828           0.04033        0.008568             0.006621   \n",
              "\n",
              "      symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
              "0            0.1493  ...        13.858          23.49            88.06   \n",
              "1            0.2135  ...        13.389          27.58            90.55   \n",
              "2            0.2181  ...        13.615          31.28            92.97   \n",
              "3            0.1938  ...        11.340          29.58            75.24   \n",
              "4            0.1850  ...        36.040          36.01           232.15   \n",
              "...             ...  ...           ...            ...              ...   \n",
              "9995         0.1586  ...        14.961          34.97            96.72   \n",
              "9996         0.1773  ...        16.561          27.23           109.80   \n",
              "9997         0.1485  ...        10.957          34.94            71.87   \n",
              "9998         0.1610  ...        11.669          30.02            75.03   \n",
              "9999         0.1473  ...        12.433          17.77            78.42   \n",
              "\n",
              "      area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "0          590.4           0.12370            0.13034         0.145731   \n",
              "1          548.5           0.14599            0.26635         0.364920   \n",
              "2          611.2           0.14752            0.37139         0.172141   \n",
              "3          397.9           0.13029            0.15898         0.044588   \n",
              "4         3615.1           0.11879            0.19572         0.576466   \n",
              "...          ...               ...                ...              ...   \n",
              "9995       719.5           0.14368            0.23676         0.144657   \n",
              "9996       875.1           0.13810            0.30831         0.222822   \n",
              "9997       362.1           0.14212            0.20370         0.096103   \n",
              "9998       408.7           0.13676            0.18993         0.211117   \n",
              "9999       486.1           0.12812            0.06561         0.037096   \n",
              "\n",
              "      concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
              "0                 0.075146          0.2831                  0.07432  \n",
              "1                 0.154583          0.4364                  0.10487  \n",
              "2                 0.110892          0.3186                  0.10515  \n",
              "3                 0.052535          0.2564                  0.08163  \n",
              "4                 0.216770          0.4201                  0.06176  \n",
              "...                    ...             ...                      ...  \n",
              "9995              0.096611          0.2627                  0.08677  \n",
              "9996              0.105450          0.3257                  0.08708  \n",
              "9997              0.080021          0.2338                  0.10824  \n",
              "9998              0.100522          0.2035                  0.08088  \n",
              "9999              0.028837          0.1847                  0.05774  \n",
              "\n",
              "[10000 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd3bdb9f-ab82-42bf-8c53-859c9e0ad2b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B</td>\n",
              "      <td>12.759</td>\n",
              "      <td>17.02</td>\n",
              "      <td>80.93</td>\n",
              "      <td>509.4</td>\n",
              "      <td>0.06687</td>\n",
              "      <td>0.03913</td>\n",
              "      <td>0.025046</td>\n",
              "      <td>0.016961</td>\n",
              "      <td>0.1493</td>\n",
              "      <td>...</td>\n",
              "      <td>13.858</td>\n",
              "      <td>23.49</td>\n",
              "      <td>88.06</td>\n",
              "      <td>590.4</td>\n",
              "      <td>0.12370</td>\n",
              "      <td>0.13034</td>\n",
              "      <td>0.145731</td>\n",
              "      <td>0.075146</td>\n",
              "      <td>0.2831</td>\n",
              "      <td>0.07432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>11.306</td>\n",
              "      <td>19.79</td>\n",
              "      <td>74.48</td>\n",
              "      <td>390.1</td>\n",
              "      <td>0.10894</td>\n",
              "      <td>0.10373</td>\n",
              "      <td>0.114211</td>\n",
              "      <td>0.053879</td>\n",
              "      <td>0.2135</td>\n",
              "      <td>...</td>\n",
              "      <td>13.389</td>\n",
              "      <td>27.58</td>\n",
              "      <td>90.55</td>\n",
              "      <td>548.5</td>\n",
              "      <td>0.14599</td>\n",
              "      <td>0.26635</td>\n",
              "      <td>0.364920</td>\n",
              "      <td>0.154583</td>\n",
              "      <td>0.4364</td>\n",
              "      <td>0.10487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>12.325</td>\n",
              "      <td>24.63</td>\n",
              "      <td>81.38</td>\n",
              "      <td>484.2</td>\n",
              "      <td>0.11996</td>\n",
              "      <td>0.16272</td>\n",
              "      <td>0.067551</td>\n",
              "      <td>0.047048</td>\n",
              "      <td>0.2181</td>\n",
              "      <td>...</td>\n",
              "      <td>13.615</td>\n",
              "      <td>31.28</td>\n",
              "      <td>92.97</td>\n",
              "      <td>611.2</td>\n",
              "      <td>0.14752</td>\n",
              "      <td>0.37139</td>\n",
              "      <td>0.172141</td>\n",
              "      <td>0.110892</td>\n",
              "      <td>0.3186</td>\n",
              "      <td>0.10515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B</td>\n",
              "      <td>10.635</td>\n",
              "      <td>24.89</td>\n",
              "      <td>68.67</td>\n",
              "      <td>341.9</td>\n",
              "      <td>0.08418</td>\n",
              "      <td>0.08830</td>\n",
              "      <td>0.027259</td>\n",
              "      <td>0.006019</td>\n",
              "      <td>0.1938</td>\n",
              "      <td>...</td>\n",
              "      <td>11.340</td>\n",
              "      <td>29.58</td>\n",
              "      <td>75.24</td>\n",
              "      <td>397.9</td>\n",
              "      <td>0.13029</td>\n",
              "      <td>0.15898</td>\n",
              "      <td>0.044588</td>\n",
              "      <td>0.052535</td>\n",
              "      <td>0.2564</td>\n",
              "      <td>0.08163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>28.110</td>\n",
              "      <td>29.74</td>\n",
              "      <td>188.50</td>\n",
              "      <td>2286.8</td>\n",
              "      <td>0.06442</td>\n",
              "      <td>0.06746</td>\n",
              "      <td>0.191878</td>\n",
              "      <td>0.115942</td>\n",
              "      <td>0.1850</td>\n",
              "      <td>...</td>\n",
              "      <td>36.040</td>\n",
              "      <td>36.01</td>\n",
              "      <td>232.15</td>\n",
              "      <td>3615.1</td>\n",
              "      <td>0.11879</td>\n",
              "      <td>0.19572</td>\n",
              "      <td>0.576466</td>\n",
              "      <td>0.216770</td>\n",
              "      <td>0.4201</td>\n",
              "      <td>0.06176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>M</td>\n",
              "      <td>13.590</td>\n",
              "      <td>25.97</td>\n",
              "      <td>88.89</td>\n",
              "      <td>597.2</td>\n",
              "      <td>0.09826</td>\n",
              "      <td>0.10244</td>\n",
              "      <td>0.048636</td>\n",
              "      <td>0.036183</td>\n",
              "      <td>0.1586</td>\n",
              "      <td>...</td>\n",
              "      <td>14.961</td>\n",
              "      <td>34.97</td>\n",
              "      <td>96.72</td>\n",
              "      <td>719.5</td>\n",
              "      <td>0.14368</td>\n",
              "      <td>0.23676</td>\n",
              "      <td>0.144657</td>\n",
              "      <td>0.096611</td>\n",
              "      <td>0.2627</td>\n",
              "      <td>0.08677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>B</td>\n",
              "      <td>13.292</td>\n",
              "      <td>19.63</td>\n",
              "      <td>86.72</td>\n",
              "      <td>554.2</td>\n",
              "      <td>0.09190</td>\n",
              "      <td>0.13122</td>\n",
              "      <td>0.059075</td>\n",
              "      <td>0.031142</td>\n",
              "      <td>0.1773</td>\n",
              "      <td>...</td>\n",
              "      <td>16.561</td>\n",
              "      <td>27.23</td>\n",
              "      <td>109.80</td>\n",
              "      <td>875.1</td>\n",
              "      <td>0.13810</td>\n",
              "      <td>0.30831</td>\n",
              "      <td>0.222822</td>\n",
              "      <td>0.105450</td>\n",
              "      <td>0.3257</td>\n",
              "      <td>0.08708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>B</td>\n",
              "      <td>9.346</td>\n",
              "      <td>22.18</td>\n",
              "      <td>60.79</td>\n",
              "      <td>275.9</td>\n",
              "      <td>0.10120</td>\n",
              "      <td>0.09687</td>\n",
              "      <td>0.040308</td>\n",
              "      <td>0.022831</td>\n",
              "      <td>0.1485</td>\n",
              "      <td>...</td>\n",
              "      <td>10.957</td>\n",
              "      <td>34.94</td>\n",
              "      <td>71.87</td>\n",
              "      <td>362.1</td>\n",
              "      <td>0.14212</td>\n",
              "      <td>0.20370</td>\n",
              "      <td>0.096103</td>\n",
              "      <td>0.080021</td>\n",
              "      <td>0.2338</td>\n",
              "      <td>0.10824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>B</td>\n",
              "      <td>10.727</td>\n",
              "      <td>19.67</td>\n",
              "      <td>68.48</td>\n",
              "      <td>352.5</td>\n",
              "      <td>0.08418</td>\n",
              "      <td>0.07468</td>\n",
              "      <td>0.078046</td>\n",
              "      <td>0.030488</td>\n",
              "      <td>0.1610</td>\n",
              "      <td>...</td>\n",
              "      <td>11.669</td>\n",
              "      <td>30.02</td>\n",
              "      <td>75.03</td>\n",
              "      <td>408.7</td>\n",
              "      <td>0.13676</td>\n",
              "      <td>0.18993</td>\n",
              "      <td>0.211117</td>\n",
              "      <td>0.100522</td>\n",
              "      <td>0.2035</td>\n",
              "      <td>0.08088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>B</td>\n",
              "      <td>11.389</td>\n",
              "      <td>15.72</td>\n",
              "      <td>71.71</td>\n",
              "      <td>418.7</td>\n",
              "      <td>0.08828</td>\n",
              "      <td>0.04033</td>\n",
              "      <td>0.008568</td>\n",
              "      <td>0.006621</td>\n",
              "      <td>0.1473</td>\n",
              "      <td>...</td>\n",
              "      <td>12.433</td>\n",
              "      <td>17.77</td>\n",
              "      <td>78.42</td>\n",
              "      <td>486.1</td>\n",
              "      <td>0.12812</td>\n",
              "      <td>0.06561</td>\n",
              "      <td>0.037096</td>\n",
              "      <td>0.028837</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.05774</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd3bdb9f-ab82-42bf-8c53-859c9e0ad2b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd3bdb9f-ab82-42bf-8c53-859c9e0ad2b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd3bdb9f-ab82-42bf-8c53-859c9e0ad2b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()"
      ],
      "metadata": {
        "id": "4cimS259ti6F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diagnosis'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv9sG-P8M1Fe",
        "outputId": "34f1b6b7-4ecb-4568-f20c-88cdcd165d48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    6085\n",
              "M    3915\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()\n",
        "print(\"Original data : \",df.shape)\n",
        "print(\"tarin         : \",X_train.shape)\n",
        "print(\"test          : \",X_test.shape[0])\n",
        "#print(\"validation    : \",X_val.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpyVwv13CTut",
        "outputId": "bb734b61-d78b-499a-eac6-5fa1a8e42a74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data :  (10000, 31)\n",
            "tarin         :  (8000, 30)\n",
            "test          :  2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "st = time.time()\n",
        "svm = SVC(C=0.1, gamma='auto', kernel = 'rbf',probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "send = time.time() - st\n",
        "STr = svm.score(X_train, y_train)\n",
        "STe = svm.score(X_test, y_test)\n",
        "y_pred_svm = svm.predict(X_test)"
      ],
      "metadata": {
        "id": "wnRYFOkyEEZ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANN\n",
        "st = time.time()\n",
        "tf.random.set_seed(123)\n",
        "ANNmodel = Sequential()\n",
        "ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "ANNmodel.add(Dense(10, activation='relu'))\n",
        "ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "aend = time.time() - st\n",
        "ATr = ANNmodel.evaluate(X_train,y_train,verbose=0)[1]\n",
        "ATe = ANNmodel.evaluate(X_test,y_test,verbose=0)[1]\n",
        "y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZX7DbN-OIyu",
        "outputId": "13f6e2f6-a6fd-43ac-898b-f6a531c6a42c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 6ms/step - loss: 0.6124 - accuracy: 0.6303 - val_loss: 0.5388 - val_accuracy: 0.7387\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5481 - accuracy: 0.7358 - val_loss: 0.5074 - val_accuracy: 0.7713\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.7449 - val_loss: 0.4875 - val_accuracy: 0.7663\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7492 - val_loss: 0.4821 - val_accuracy: 0.7688\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7499 - val_loss: 0.4802 - val_accuracy: 0.7738\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5035 - accuracy: 0.7528 - val_loss: 0.4785 - val_accuracy: 0.7800\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.7540 - val_loss: 0.4800 - val_accuracy: 0.7788\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4990 - accuracy: 0.7533 - val_loss: 0.4791 - val_accuracy: 0.7775\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4980 - accuracy: 0.7535 - val_loss: 0.4783 - val_accuracy: 0.7800\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.7572 - val_loss: 0.4782 - val_accuracy: 0.7788\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.7561 - val_loss: 0.4795 - val_accuracy: 0.7800\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4942 - accuracy: 0.7574 - val_loss: 0.4791 - val_accuracy: 0.7775\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7585 - val_loss: 0.4784 - val_accuracy: 0.7800\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.7590 - val_loss: 0.4795 - val_accuracy: 0.7812\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7607 - val_loss: 0.4796 - val_accuracy: 0.7812\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4901 - accuracy: 0.7601 - val_loss: 0.4796 - val_accuracy: 0.7800\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4897 - accuracy: 0.7606 - val_loss: 0.4798 - val_accuracy: 0.7775\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.7636 - val_loss: 0.4791 - val_accuracy: 0.7775\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.7619 - val_loss: 0.4799 - val_accuracy: 0.7800\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.7639 - val_loss: 0.4808 - val_accuracy: 0.7837\n",
            "63/63 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "st = time.time()\n",
        "xgb = XGBClassifier(objective='binary:logistic',max_depth= 6,alpha= 50,learning_rate= 0.01,n_estimators=250)\n",
        "xgb.fit(X_train, y_train)\n",
        "xend = time.time() - st\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "XTr = accuracy_score(y_train, xgb.predict(X_train))\n",
        "XTe = accuracy_score(y_test, xgb.predict(X_test))\n",
        "XTr,XTe"
      ],
      "metadata": {
        "id": "PCP82YZ9RjgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b9b418-c15b-4f1f-ef26-828856672e1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.775375, 0.7555)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "# Define the range of n_neighbors values to test\n",
        "n_neighbors_values = [1,3, 5, 7, 9, 11]\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_n_neighbors = None\n",
        "\n",
        "for n_neighbors in n_neighbors_values:\n",
        "    print(\"Number of Neighbors:\", n_neighbors)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, knn.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "    print('KNN model train accuracy score: {0:0.4f}'.format(train_accuracy))\n",
        "    print('KNN model test accuracy score: {0:0.4f}'.format(test_accuracy))\n",
        "    print()\n",
        "\n",
        "    # Check if the current test accuracy is better than the previous best\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_n_neighbors = n_neighbors\n",
        "print(\"best neighbours: \", best_n_neighbors)\n",
        "\n",
        "st = time.time()\n",
        "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "knn.fit(X_train, y_train)\n",
        "kend = time.time() - st\n",
        "KTr = accuracy_score(y_train, knn.predict(X_train))\n",
        "KTe = accuracy_score(y_test, knn.predict(X_test))\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "KTr,KTe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y_6b33A1imy",
        "outputId": "9136de8a-3d8c-4bd5-fd6b-591e6a010d4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Neighbors: 1\n",
            "KNN model train accuracy score: 1.0000\n",
            "KNN model test accuracy score: 0.6770\n",
            "\n",
            "Number of Neighbors: 3\n",
            "KNN model train accuracy score: 0.8323\n",
            "KNN model test accuracy score: 0.6955\n",
            "\n",
            "Number of Neighbors: 5\n",
            "KNN model train accuracy score: 0.7936\n",
            "KNN model test accuracy score: 0.7090\n",
            "\n",
            "Number of Neighbors: 7\n",
            "KNN model train accuracy score: 0.7846\n",
            "KNN model test accuracy score: 0.7160\n",
            "\n",
            "Number of Neighbors: 9\n",
            "KNN model train accuracy score: 0.7774\n",
            "KNN model test accuracy score: 0.7275\n",
            "\n",
            "Number of Neighbors: 11\n",
            "KNN model train accuracy score: 0.7722\n",
            "KNN model test accuracy score: 0.7340\n",
            "\n",
            "best neighbours:  11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.77225, 0.734)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RF\n",
        "st = time.time()\n",
        "rf = RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123)\n",
        "rf.fit(X_train, y_train)\n",
        "rend = time.time() - st\n",
        "RTr = accuracy_score(y_train, rf.predict(X_train))\n",
        "RTe = accuracy_score(y_test, rf.predict(X_test))\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "RTr,RTe"
      ],
      "metadata": {
        "id": "k_kyk_E92bSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607dc06d-f015-459b-903f-815c02fc9ab4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.75175, 0.7565)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LR\n",
        "st = time.time()\n",
        "lr = LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123)\n",
        "lr.fit(X_train, y_train)\n",
        "lend = time.time() - st\n",
        "LTr = accuracy_score(y_train, lr.predict(X_train))\n",
        "LTe = accuracy_score(y_test, lr.predict(X_test))\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "LTr,LTe"
      ],
      "metadata": {
        "id": "OWkZmwL23Fm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291373a2-3eb4-44a7-c445-ff4886062bf4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7615, 0.7565)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E9_l6r5f0w5n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CVal(ML):\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/GC_SDV_BreastCancer.csv\")\n",
        "  del df['Unnamed: 0']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "\n",
        "    ML.fit(X_train, y_train)\n",
        "    y_pred_ML = ML.predict(X_test)\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ML)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]+1\n",
        "\n",
        "  y_pred_final = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_final.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "XqaJZ1Mb0xAe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CValANN():\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/GC_SDV_BreastCancer.csv\")\n",
        "  del df['Unnamed: 0']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "    tf.random.set_seed(123)\n",
        "    ANNmodel = Sequential()\n",
        "    ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    ANNmodel.add(Dense(10, activation='relu'))\n",
        "    ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "    ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "    ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "    y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ANN)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]\n",
        "\n",
        "  y_pred_fina = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_fina.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  y_pred_final = []\n",
        "\n",
        "  for i in range(len(y_pred_fina)):\n",
        "    y_pred_final.append(y_pred_fina[i][0])\n",
        "\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "jRa_a7EY0y6B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = pd.DataFrame({\n",
        "    \"SVM\": CVal(SVC(C=0.1, gamma='auto', kernel = 'rbf'))\n",
        "})\n",
        "newdata[\"KNN\"] = CVal(KNeighborsClassifier(n_neighbors=3))\n",
        "newdata[\"RF\"]  = CVal(RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123))\n",
        "newdata['LR']  = CVal(LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123))\n",
        "newdata[\"ANN\"] = CValANN()\n",
        "newdata[\"XGB\"] = CVal(XGBClassifier(objective='binary:logistic',max_depth= 7,alpha= 10,learning_rate= 1,n_estimators=100))\n",
        "newdata['y_test'] = encoder.fit_transform(df['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CGgdO1V0zA9",
        "outputId": "478c542f-120d-4026-8267-2c5c7dde5e79"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 5ms/step - loss: 0.6368 - accuracy: 0.6394 - val_loss: 0.5287 - val_accuracy: 0.7337\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7483 - val_loss: 0.4995 - val_accuracy: 0.7487\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5063 - accuracy: 0.7521 - val_loss: 0.4919 - val_accuracy: 0.7600\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7557 - val_loss: 0.4895 - val_accuracy: 0.7575\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.7569 - val_loss: 0.4899 - val_accuracy: 0.7600\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.7585 - val_loss: 0.4903 - val_accuracy: 0.7563\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7589 - val_loss: 0.4891 - val_accuracy: 0.7538\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7625 - val_loss: 0.4892 - val_accuracy: 0.7538\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7613 - val_loss: 0.4875 - val_accuracy: 0.7525\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4916 - accuracy: 0.7635 - val_loss: 0.4881 - val_accuracy: 0.7525\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.7636 - val_loss: 0.4897 - val_accuracy: 0.7487\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7624 - val_loss: 0.4901 - val_accuracy: 0.7563\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.7636 - val_loss: 0.4892 - val_accuracy: 0.7563\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7624 - val_loss: 0.4894 - val_accuracy: 0.7575\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.7649 - val_loss: 0.4889 - val_accuracy: 0.7563\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.7667 - val_loss: 0.4902 - val_accuracy: 0.7563\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.7667 - val_loss: 0.4897 - val_accuracy: 0.7575\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7683 - val_loss: 0.4898 - val_accuracy: 0.7538\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.7679 - val_loss: 0.4904 - val_accuracy: 0.7550\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.7671 - val_loss: 0.4889 - val_accuracy: 0.7538\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 7ms/step - loss: 0.5572 - accuracy: 0.7246 - val_loss: 0.5152 - val_accuracy: 0.7437\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5137 - accuracy: 0.7456 - val_loss: 0.5005 - val_accuracy: 0.7513\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5074 - accuracy: 0.7478 - val_loss: 0.4951 - val_accuracy: 0.7550\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7493 - val_loss: 0.4928 - val_accuracy: 0.7500\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7521 - val_loss: 0.4921 - val_accuracy: 0.7550\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.7554 - val_loss: 0.4910 - val_accuracy: 0.7600\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.7553 - val_loss: 0.4902 - val_accuracy: 0.7575\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.7544 - val_loss: 0.4903 - val_accuracy: 0.7550\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.7565 - val_loss: 0.4880 - val_accuracy: 0.7613\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7575 - val_loss: 0.4876 - val_accuracy: 0.7563\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7581 - val_loss: 0.4892 - val_accuracy: 0.7525\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7589 - val_loss: 0.4879 - val_accuracy: 0.7588\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.7601 - val_loss: 0.4890 - val_accuracy: 0.7525\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.7610 - val_loss: 0.4884 - val_accuracy: 0.7525\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.7618 - val_loss: 0.4878 - val_accuracy: 0.7500\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.7628 - val_loss: 0.4873 - val_accuracy: 0.7513\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4871 - accuracy: 0.7638 - val_loss: 0.4875 - val_accuracy: 0.7550\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.7625 - val_loss: 0.4876 - val_accuracy: 0.7550\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7638 - val_loss: 0.4884 - val_accuracy: 0.7550\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4848 - accuracy: 0.7639 - val_loss: 0.4877 - val_accuracy: 0.7563\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 5ms/step - loss: 0.5942 - accuracy: 0.6892 - val_loss: 0.5217 - val_accuracy: 0.7237\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5222 - accuracy: 0.7383 - val_loss: 0.5039 - val_accuracy: 0.7513\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7454 - val_loss: 0.4974 - val_accuracy: 0.7487\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5110 - accuracy: 0.7460 - val_loss: 0.4940 - val_accuracy: 0.7563\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.7492 - val_loss: 0.4922 - val_accuracy: 0.7588\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5064 - accuracy: 0.7507 - val_loss: 0.4919 - val_accuracy: 0.7588\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7535 - val_loss: 0.4904 - val_accuracy: 0.7588\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.7532 - val_loss: 0.4898 - val_accuracy: 0.7613\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7532 - val_loss: 0.4896 - val_accuracy: 0.7625\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.7547 - val_loss: 0.4894 - val_accuracy: 0.7613\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7536 - val_loss: 0.4889 - val_accuracy: 0.7625\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.7549 - val_loss: 0.4886 - val_accuracy: 0.7638\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.7579 - val_loss: 0.4900 - val_accuracy: 0.7625\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7574 - val_loss: 0.4876 - val_accuracy: 0.7625\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7590 - val_loss: 0.4878 - val_accuracy: 0.7675\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.7578 - val_loss: 0.4873 - val_accuracy: 0.7613\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.7582 - val_loss: 0.4859 - val_accuracy: 0.7563\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7579 - val_loss: 0.4868 - val_accuracy: 0.7575\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7611 - val_loss: 0.4868 - val_accuracy: 0.7713\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7606 - val_loss: 0.4866 - val_accuracy: 0.7675\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 5ms/step - loss: 0.6028 - accuracy: 0.6553 - val_loss: 0.5162 - val_accuracy: 0.7387\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7418 - val_loss: 0.4988 - val_accuracy: 0.7462\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7506 - val_loss: 0.4931 - val_accuracy: 0.7563\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7547 - val_loss: 0.4923 - val_accuracy: 0.7563\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4996 - accuracy: 0.7557 - val_loss: 0.4905 - val_accuracy: 0.7538\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4975 - accuracy: 0.7568 - val_loss: 0.4901 - val_accuracy: 0.7538\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4949 - accuracy: 0.7590 - val_loss: 0.4903 - val_accuracy: 0.7563\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7590 - val_loss: 0.4890 - val_accuracy: 0.7538\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4920 - accuracy: 0.7585 - val_loss: 0.4908 - val_accuracy: 0.7575\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4908 - accuracy: 0.7600 - val_loss: 0.4887 - val_accuracy: 0.7563\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4897 - accuracy: 0.7606 - val_loss: 0.4880 - val_accuracy: 0.7563\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4889 - accuracy: 0.7618 - val_loss: 0.4876 - val_accuracy: 0.7563\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4879 - accuracy: 0.7636 - val_loss: 0.4882 - val_accuracy: 0.7613\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4874 - accuracy: 0.7643 - val_loss: 0.4874 - val_accuracy: 0.7525\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.7644 - val_loss: 0.4862 - val_accuracy: 0.7538\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.7638 - val_loss: 0.4862 - val_accuracy: 0.7563\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7656 - val_loss: 0.4861 - val_accuracy: 0.7550\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7644 - val_loss: 0.4886 - val_accuracy: 0.7588\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7669 - val_loss: 0.4880 - val_accuracy: 0.7588\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.7658 - val_loss: 0.4871 - val_accuracy: 0.7538\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 5ms/step - loss: 0.6037 - accuracy: 0.6676 - val_loss: 0.5286 - val_accuracy: 0.7462\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7489 - val_loss: 0.5126 - val_accuracy: 0.7600\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7510 - val_loss: 0.5071 - val_accuracy: 0.7575\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7528 - val_loss: 0.5041 - val_accuracy: 0.7600\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.7539 - val_loss: 0.5016 - val_accuracy: 0.7625\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.7553 - val_loss: 0.5018 - val_accuracy: 0.7663\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.7589 - val_loss: 0.5016 - val_accuracy: 0.7675\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7601 - val_loss: 0.5013 - val_accuracy: 0.7688\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7599 - val_loss: 0.5001 - val_accuracy: 0.7688\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.7596 - val_loss: 0.5005 - val_accuracy: 0.7700\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4882 - accuracy: 0.7611 - val_loss: 0.5001 - val_accuracy: 0.7688\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.7613 - val_loss: 0.4996 - val_accuracy: 0.7713\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4862 - accuracy: 0.7639 - val_loss: 0.5020 - val_accuracy: 0.7638\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.7624 - val_loss: 0.5010 - val_accuracy: 0.7700\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.7642 - val_loss: 0.5009 - val_accuracy: 0.7625\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7636 - val_loss: 0.5020 - val_accuracy: 0.7638\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7642 - val_loss: 0.5004 - val_accuracy: 0.7675\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.7667 - val_loss: 0.5002 - val_accuracy: 0.7675\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4802 - accuracy: 0.7669 - val_loss: 0.5004 - val_accuracy: 0.7575\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.7654 - val_loss: 0.5033 - val_accuracy: 0.7625\n",
            "63/63 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newdata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "05ryfvyLzFVK",
        "outputId": "abd0e947-b31d-4404-f812-847ba193373e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SVM  KNN  RF  LR  ANN  XGB  y_test\n",
              "0    0    0   0   0    0    0       0\n",
              "1    0    0   0   0    0    0       1\n",
              "2    1    1   1   0    1    1       1\n",
              "3    0    0   0   0    0    1       0\n",
              "4    1    1   1   1    1    1       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36d61b34-8ae0-4f20-aefa-1d044a2219f1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SVM</th>\n",
              "      <th>KNN</th>\n",
              "      <th>RF</th>\n",
              "      <th>LR</th>\n",
              "      <th>ANN</th>\n",
              "      <th>XGB</th>\n",
              "      <th>y_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36d61b34-8ae0-4f20-aefa-1d044a2219f1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-36d61b34-8ae0-4f20-aefa-1d044a2219f1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-36d61b34-8ae0-4f20-aefa-1d044a2219f1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DNN model\n",
        "DNNX = newdata.drop(columns=['y_test']).copy()\n",
        "DNNY = newdata.y_test.copy()\n",
        "DX_train, DX_test, Dy_train, Dy_test = train_test_split(\n",
        "    DNNX, DNNY, test_size=0.2, random_state=123)\n",
        "\n",
        "st = time.time()\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(30, activation='relu', input_shape=DNNX.shape[1:]),\n",
        "    tf.keras.layers.Dense(25, activation='relu'),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dense(15, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(DX_train, Dy_train, epochs=500, batch_size=64, validation_split=0.2)\n",
        "dend = time.time() - st"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4kf97a3yX15",
        "outputId": "9521b6bc-1bc0-4f32-ec01-7d00315921d1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "100/100 [==============================] - 2s 5ms/step - loss: 0.6670 - accuracy: 0.7422 - val_loss: 0.6331 - val_accuracy: 0.7581\n",
            "Epoch 2/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6292 - accuracy: 0.7552 - val_loss: 0.6174 - val_accuracy: 0.7681\n",
            "Epoch 3/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6137 - accuracy: 0.7555 - val_loss: 0.6023 - val_accuracy: 0.7675\n",
            "Epoch 4/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.6026 - accuracy: 0.7563 - val_loss: 0.5906 - val_accuracy: 0.7663\n",
            "Epoch 5/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5933 - accuracy: 0.7545 - val_loss: 0.5805 - val_accuracy: 0.7719\n",
            "Epoch 6/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5855 - accuracy: 0.7544 - val_loss: 0.5729 - val_accuracy: 0.7681\n",
            "Epoch 7/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5775 - accuracy: 0.7553 - val_loss: 0.5645 - val_accuracy: 0.7656\n",
            "Epoch 8/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5722 - accuracy: 0.7555 - val_loss: 0.5598 - val_accuracy: 0.7719\n",
            "Epoch 9/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5673 - accuracy: 0.7553 - val_loss: 0.5535 - val_accuracy: 0.7656\n",
            "Epoch 10/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5624 - accuracy: 0.7558 - val_loss: 0.5489 - val_accuracy: 0.7656\n",
            "Epoch 11/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.7566 - val_loss: 0.5452 - val_accuracy: 0.7650\n",
            "Epoch 12/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5561 - accuracy: 0.7561 - val_loss: 0.5425 - val_accuracy: 0.7719\n",
            "Epoch 13/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5524 - accuracy: 0.7553 - val_loss: 0.5385 - val_accuracy: 0.7694\n",
            "Epoch 14/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5498 - accuracy: 0.7553 - val_loss: 0.5367 - val_accuracy: 0.7725\n",
            "Epoch 15/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5487 - accuracy: 0.7553 - val_loss: 0.5352 - val_accuracy: 0.7663\n",
            "Epoch 16/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5464 - accuracy: 0.7556 - val_loss: 0.5323 - val_accuracy: 0.7719\n",
            "Epoch 17/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5451 - accuracy: 0.7569 - val_loss: 0.5302 - val_accuracy: 0.7669\n",
            "Epoch 18/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7578 - val_loss: 0.5294 - val_accuracy: 0.7675\n",
            "Epoch 19/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5427 - accuracy: 0.7561 - val_loss: 0.5283 - val_accuracy: 0.7681\n",
            "Epoch 20/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.7563 - val_loss: 0.5273 - val_accuracy: 0.7669\n",
            "Epoch 21/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5409 - accuracy: 0.7581 - val_loss: 0.5295 - val_accuracy: 0.7719\n",
            "Epoch 22/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.7575 - val_loss: 0.5287 - val_accuracy: 0.7675\n",
            "Epoch 23/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5394 - accuracy: 0.7566 - val_loss: 0.5295 - val_accuracy: 0.7725\n",
            "Epoch 24/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5394 - accuracy: 0.7575 - val_loss: 0.5289 - val_accuracy: 0.7675\n",
            "Epoch 25/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.7564 - val_loss: 0.5277 - val_accuracy: 0.7694\n",
            "Epoch 26/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7581 - val_loss: 0.5255 - val_accuracy: 0.7706\n",
            "Epoch 27/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5382 - accuracy: 0.7567 - val_loss: 0.5243 - val_accuracy: 0.7681\n",
            "Epoch 28/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7563 - val_loss: 0.5242 - val_accuracy: 0.7675\n",
            "Epoch 29/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5373 - accuracy: 0.7558 - val_loss: 0.5241 - val_accuracy: 0.7700\n",
            "Epoch 30/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7578 - val_loss: 0.5238 - val_accuracy: 0.7700\n",
            "Epoch 31/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7567 - val_loss: 0.5242 - val_accuracy: 0.7700\n",
            "Epoch 32/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7569 - val_loss: 0.5247 - val_accuracy: 0.7700\n",
            "Epoch 33/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5369 - accuracy: 0.7583 - val_loss: 0.5261 - val_accuracy: 0.7688\n",
            "Epoch 34/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5368 - accuracy: 0.7583 - val_loss: 0.5268 - val_accuracy: 0.7688\n",
            "Epoch 35/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5372 - accuracy: 0.7563 - val_loss: 0.5238 - val_accuracy: 0.7700\n",
            "Epoch 36/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5371 - accuracy: 0.7586 - val_loss: 0.5241 - val_accuracy: 0.7700\n",
            "Epoch 37/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5369 - accuracy: 0.7578 - val_loss: 0.5236 - val_accuracy: 0.7700\n",
            "Epoch 38/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.7573 - val_loss: 0.5243 - val_accuracy: 0.7700\n",
            "Epoch 39/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5364 - accuracy: 0.7566 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 40/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5366 - accuracy: 0.7577 - val_loss: 0.5238 - val_accuracy: 0.7675\n",
            "Epoch 41/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.7566 - val_loss: 0.5241 - val_accuracy: 0.7675\n",
            "Epoch 42/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5362 - accuracy: 0.7573 - val_loss: 0.5237 - val_accuracy: 0.7694\n",
            "Epoch 43/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5362 - accuracy: 0.7564 - val_loss: 0.5243 - val_accuracy: 0.7663\n",
            "Epoch 44/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5364 - accuracy: 0.7563 - val_loss: 0.5238 - val_accuracy: 0.7694\n",
            "Epoch 45/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5366 - accuracy: 0.7591 - val_loss: 0.5247 - val_accuracy: 0.7675\n",
            "Epoch 46/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5367 - accuracy: 0.7597 - val_loss: 0.5269 - val_accuracy: 0.7688\n",
            "Epoch 47/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5364 - accuracy: 0.7575 - val_loss: 0.5246 - val_accuracy: 0.7700\n",
            "Epoch 48/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7591 - val_loss: 0.5237 - val_accuracy: 0.7706\n",
            "Epoch 49/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5360 - accuracy: 0.7580 - val_loss: 0.5245 - val_accuracy: 0.7700\n",
            "Epoch 50/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5360 - accuracy: 0.7580 - val_loss: 0.5259 - val_accuracy: 0.7688\n",
            "Epoch 51/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7577 - val_loss: 0.5270 - val_accuracy: 0.7675\n",
            "Epoch 52/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7583 - val_loss: 0.5241 - val_accuracy: 0.7688\n",
            "Epoch 53/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5365 - accuracy: 0.7567 - val_loss: 0.5247 - val_accuracy: 0.7681\n",
            "Epoch 54/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.7605 - val_loss: 0.5250 - val_accuracy: 0.7675\n",
            "Epoch 55/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5362 - accuracy: 0.7573 - val_loss: 0.5242 - val_accuracy: 0.7675\n",
            "Epoch 56/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5367 - accuracy: 0.7573 - val_loss: 0.5264 - val_accuracy: 0.7675\n",
            "Epoch 57/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7584 - val_loss: 0.5250 - val_accuracy: 0.7663\n",
            "Epoch 58/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5358 - accuracy: 0.7592 - val_loss: 0.5251 - val_accuracy: 0.7688\n",
            "Epoch 59/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7569 - val_loss: 0.5262 - val_accuracy: 0.7688\n",
            "Epoch 60/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7580 - val_loss: 0.5254 - val_accuracy: 0.7681\n",
            "Epoch 61/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7580 - val_loss: 0.5248 - val_accuracy: 0.7681\n",
            "Epoch 62/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7588 - val_loss: 0.5254 - val_accuracy: 0.7669\n",
            "Epoch 63/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7577 - val_loss: 0.5255 - val_accuracy: 0.7681\n",
            "Epoch 64/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.7592 - val_loss: 0.5252 - val_accuracy: 0.7675\n",
            "Epoch 65/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7578 - val_loss: 0.5263 - val_accuracy: 0.7688\n",
            "Epoch 66/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.7575 - val_loss: 0.5255 - val_accuracy: 0.7663\n",
            "Epoch 67/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7583 - val_loss: 0.5266 - val_accuracy: 0.7669\n",
            "Epoch 68/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7575 - val_loss: 0.5252 - val_accuracy: 0.7675\n",
            "Epoch 69/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7573 - val_loss: 0.5267 - val_accuracy: 0.7669\n",
            "Epoch 70/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7581 - val_loss: 0.5259 - val_accuracy: 0.7681\n",
            "Epoch 71/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5355 - accuracy: 0.7581 - val_loss: 0.5258 - val_accuracy: 0.7656\n",
            "Epoch 72/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.7577 - val_loss: 0.5256 - val_accuracy: 0.7681\n",
            "Epoch 73/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7584 - val_loss: 0.5259 - val_accuracy: 0.7663\n",
            "Epoch 74/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7578 - val_loss: 0.5279 - val_accuracy: 0.7675\n",
            "Epoch 75/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7570 - val_loss: 0.5263 - val_accuracy: 0.7663\n",
            "Epoch 76/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7583 - val_loss: 0.5258 - val_accuracy: 0.7663\n",
            "Epoch 77/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7591 - val_loss: 0.5280 - val_accuracy: 0.7656\n",
            "Epoch 78/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7572 - val_loss: 0.5260 - val_accuracy: 0.7688\n",
            "Epoch 79/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7592 - val_loss: 0.5273 - val_accuracy: 0.7663\n",
            "Epoch 80/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7581 - val_loss: 0.5273 - val_accuracy: 0.7663\n",
            "Epoch 81/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7583 - val_loss: 0.5265 - val_accuracy: 0.7663\n",
            "Epoch 82/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7583 - val_loss: 0.5267 - val_accuracy: 0.7663\n",
            "Epoch 83/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7588 - val_loss: 0.5264 - val_accuracy: 0.7650\n",
            "Epoch 84/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.7595 - val_loss: 0.5264 - val_accuracy: 0.7663\n",
            "Epoch 85/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7591 - val_loss: 0.5265 - val_accuracy: 0.7650\n",
            "Epoch 86/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7586 - val_loss: 0.5263 - val_accuracy: 0.7675\n",
            "Epoch 87/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7592 - val_loss: 0.5266 - val_accuracy: 0.7656\n",
            "Epoch 88/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5357 - accuracy: 0.7570 - val_loss: 0.5267 - val_accuracy: 0.7650\n",
            "Epoch 89/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5353 - accuracy: 0.7588 - val_loss: 0.5267 - val_accuracy: 0.7663\n",
            "Epoch 90/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5348 - accuracy: 0.7586 - val_loss: 0.5273 - val_accuracy: 0.7656\n",
            "Epoch 91/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7578 - val_loss: 0.5264 - val_accuracy: 0.7663\n",
            "Epoch 92/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7595 - val_loss: 0.5269 - val_accuracy: 0.7669\n",
            "Epoch 93/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7592 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 94/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7594 - val_loss: 0.5267 - val_accuracy: 0.7681\n",
            "Epoch 95/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7581 - val_loss: 0.5264 - val_accuracy: 0.7644\n",
            "Epoch 96/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7597 - val_loss: 0.5272 - val_accuracy: 0.7675\n",
            "Epoch 97/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7581 - val_loss: 0.5276 - val_accuracy: 0.7675\n",
            "Epoch 98/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5356 - accuracy: 0.7595 - val_loss: 0.5282 - val_accuracy: 0.7644\n",
            "Epoch 99/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7589 - val_loss: 0.5266 - val_accuracy: 0.7650\n",
            "Epoch 100/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7586 - val_loss: 0.5266 - val_accuracy: 0.7656\n",
            "Epoch 101/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7583 - val_loss: 0.5269 - val_accuracy: 0.7656\n",
            "Epoch 102/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7600 - val_loss: 0.5278 - val_accuracy: 0.7656\n",
            "Epoch 103/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7586 - val_loss: 0.5272 - val_accuracy: 0.7644\n",
            "Epoch 104/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7584 - val_loss: 0.5269 - val_accuracy: 0.7644\n",
            "Epoch 105/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7591 - val_loss: 0.5272 - val_accuracy: 0.7644\n",
            "Epoch 106/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7586 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 107/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7588 - val_loss: 0.5267 - val_accuracy: 0.7656\n",
            "Epoch 108/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7588 - val_loss: 0.5287 - val_accuracy: 0.7650\n",
            "Epoch 109/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5360 - accuracy: 0.7581 - val_loss: 0.5269 - val_accuracy: 0.7650\n",
            "Epoch 110/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5367 - accuracy: 0.7592 - val_loss: 0.5267 - val_accuracy: 0.7656\n",
            "Epoch 111/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7588 - val_loss: 0.5271 - val_accuracy: 0.7644\n",
            "Epoch 112/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7581 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 113/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7577 - val_loss: 0.5272 - val_accuracy: 0.7650\n",
            "Epoch 114/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7583 - val_loss: 0.5276 - val_accuracy: 0.7650\n",
            "Epoch 115/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7588 - val_loss: 0.5274 - val_accuracy: 0.7663\n",
            "Epoch 116/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7573 - val_loss: 0.5272 - val_accuracy: 0.7656\n",
            "Epoch 117/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7588 - val_loss: 0.5278 - val_accuracy: 0.7656\n",
            "Epoch 118/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7580 - val_loss: 0.5268 - val_accuracy: 0.7663\n",
            "Epoch 119/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.7573 - val_loss: 0.5274 - val_accuracy: 0.7663\n",
            "Epoch 120/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7581 - val_loss: 0.5275 - val_accuracy: 0.7650\n",
            "Epoch 121/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5355 - accuracy: 0.7572 - val_loss: 0.5267 - val_accuracy: 0.7638\n",
            "Epoch 122/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7586 - val_loss: 0.5275 - val_accuracy: 0.7656\n",
            "Epoch 123/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7589 - val_loss: 0.5272 - val_accuracy: 0.7625\n",
            "Epoch 124/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7589 - val_loss: 0.5277 - val_accuracy: 0.7663\n",
            "Epoch 125/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.7595 - val_loss: 0.5270 - val_accuracy: 0.7638\n",
            "Epoch 126/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7588 - val_loss: 0.5271 - val_accuracy: 0.7638\n",
            "Epoch 127/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7589 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 128/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7591 - val_loss: 0.5297 - val_accuracy: 0.7663\n",
            "Epoch 129/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5355 - accuracy: 0.7586 - val_loss: 0.5282 - val_accuracy: 0.7663\n",
            "Epoch 130/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7566 - val_loss: 0.5291 - val_accuracy: 0.7656\n",
            "Epoch 131/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5359 - accuracy: 0.7583 - val_loss: 0.5286 - val_accuracy: 0.7650\n",
            "Epoch 132/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5346 - accuracy: 0.7591 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 133/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.7591 - val_loss: 0.5273 - val_accuracy: 0.7650\n",
            "Epoch 134/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7583 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 135/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7583 - val_loss: 0.5286 - val_accuracy: 0.7663\n",
            "Epoch 136/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.7597 - val_loss: 0.5274 - val_accuracy: 0.7638\n",
            "Epoch 137/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7581 - val_loss: 0.5271 - val_accuracy: 0.7650\n",
            "Epoch 138/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7598 - val_loss: 0.5272 - val_accuracy: 0.7650\n",
            "Epoch 139/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7605 - val_loss: 0.5314 - val_accuracy: 0.7663\n",
            "Epoch 140/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5355 - accuracy: 0.7589 - val_loss: 0.5271 - val_accuracy: 0.7638\n",
            "Epoch 141/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7581 - val_loss: 0.5272 - val_accuracy: 0.7638\n",
            "Epoch 142/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7589 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 143/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7586 - val_loss: 0.5285 - val_accuracy: 0.7669\n",
            "Epoch 144/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7586 - val_loss: 0.5276 - val_accuracy: 0.7638\n",
            "Epoch 145/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7589 - val_loss: 0.5272 - val_accuracy: 0.7638\n",
            "Epoch 146/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7588 - val_loss: 0.5310 - val_accuracy: 0.7644\n",
            "Epoch 147/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7588 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 148/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7584 - val_loss: 0.5272 - val_accuracy: 0.7650\n",
            "Epoch 149/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7581 - val_loss: 0.5285 - val_accuracy: 0.7650\n",
            "Epoch 150/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7606 - val_loss: 0.5271 - val_accuracy: 0.7638\n",
            "Epoch 151/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7594 - val_loss: 0.5269 - val_accuracy: 0.7656\n",
            "Epoch 152/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7584 - val_loss: 0.5272 - val_accuracy: 0.7644\n",
            "Epoch 153/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7586 - val_loss: 0.5285 - val_accuracy: 0.7638\n",
            "Epoch 154/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7594 - val_loss: 0.5270 - val_accuracy: 0.7650\n",
            "Epoch 155/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7584 - val_loss: 0.5279 - val_accuracy: 0.7638\n",
            "Epoch 156/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7573 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 157/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7581 - val_loss: 0.5278 - val_accuracy: 0.7656\n",
            "Epoch 158/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7592 - val_loss: 0.5300 - val_accuracy: 0.7675\n",
            "Epoch 159/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5355 - accuracy: 0.7594 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 160/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7592 - val_loss: 0.5280 - val_accuracy: 0.7663\n",
            "Epoch 161/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7592 - val_loss: 0.5277 - val_accuracy: 0.7656\n",
            "Epoch 162/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7591 - val_loss: 0.5277 - val_accuracy: 0.7650\n",
            "Epoch 163/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7591 - val_loss: 0.5275 - val_accuracy: 0.7638\n",
            "Epoch 164/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7588 - val_loss: 0.5274 - val_accuracy: 0.7638\n",
            "Epoch 165/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7586 - val_loss: 0.5278 - val_accuracy: 0.7663\n",
            "Epoch 166/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7592 - val_loss: 0.5277 - val_accuracy: 0.7656\n",
            "Epoch 167/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7583 - val_loss: 0.5281 - val_accuracy: 0.7638\n",
            "Epoch 168/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7592 - val_loss: 0.5275 - val_accuracy: 0.7638\n",
            "Epoch 169/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7588 - val_loss: 0.5279 - val_accuracy: 0.7638\n",
            "Epoch 170/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5348 - accuracy: 0.7586 - val_loss: 0.5274 - val_accuracy: 0.7638\n",
            "Epoch 171/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5346 - accuracy: 0.7580 - val_loss: 0.5279 - val_accuracy: 0.7644\n",
            "Epoch 172/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5346 - accuracy: 0.7588 - val_loss: 0.5274 - val_accuracy: 0.7638\n",
            "Epoch 173/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7592 - val_loss: 0.5279 - val_accuracy: 0.7663\n",
            "Epoch 174/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.7583 - val_loss: 0.5290 - val_accuracy: 0.7650\n",
            "Epoch 175/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7592 - val_loss: 0.5286 - val_accuracy: 0.7631\n",
            "Epoch 176/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7578 - val_loss: 0.5277 - val_accuracy: 0.7638\n",
            "Epoch 177/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7589 - val_loss: 0.5284 - val_accuracy: 0.7644\n",
            "Epoch 178/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7588 - val_loss: 0.5274 - val_accuracy: 0.7663\n",
            "Epoch 179/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7583 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 180/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7581 - val_loss: 0.5279 - val_accuracy: 0.7656\n",
            "Epoch 181/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7592 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 182/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7595 - val_loss: 0.5273 - val_accuracy: 0.7650\n",
            "Epoch 183/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7597 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 184/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7597 - val_loss: 0.5285 - val_accuracy: 0.7638\n",
            "Epoch 185/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5351 - accuracy: 0.7584 - val_loss: 0.5286 - val_accuracy: 0.7663\n",
            "Epoch 186/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7588 - val_loss: 0.5278 - val_accuracy: 0.7663\n",
            "Epoch 187/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7597 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 188/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7592 - val_loss: 0.5273 - val_accuracy: 0.7663\n",
            "Epoch 189/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7586 - val_loss: 0.5279 - val_accuracy: 0.7656\n",
            "Epoch 190/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7584 - val_loss: 0.5273 - val_accuracy: 0.7644\n",
            "Epoch 191/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7592 - val_loss: 0.5276 - val_accuracy: 0.7669\n",
            "Epoch 192/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7591 - val_loss: 0.5283 - val_accuracy: 0.7656\n",
            "Epoch 193/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7581 - val_loss: 0.5267 - val_accuracy: 0.7650\n",
            "Epoch 194/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7580 - val_loss: 0.5272 - val_accuracy: 0.7663\n",
            "Epoch 195/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7592 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 196/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7594 - val_loss: 0.5298 - val_accuracy: 0.7663\n",
            "Epoch 197/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7592 - val_loss: 0.5292 - val_accuracy: 0.7656\n",
            "Epoch 198/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7597 - val_loss: 0.5278 - val_accuracy: 0.7644\n",
            "Epoch 199/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7592 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 200/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7584 - val_loss: 0.5281 - val_accuracy: 0.7656\n",
            "Epoch 201/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7594 - val_loss: 0.5274 - val_accuracy: 0.7656\n",
            "Epoch 202/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7594 - val_loss: 0.5283 - val_accuracy: 0.7656\n",
            "Epoch 203/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7594 - val_loss: 0.5277 - val_accuracy: 0.7663\n",
            "Epoch 204/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7591 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 205/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7578 - val_loss: 0.5289 - val_accuracy: 0.7663\n",
            "Epoch 206/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7588 - val_loss: 0.5278 - val_accuracy: 0.7644\n",
            "Epoch 207/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7583 - val_loss: 0.5281 - val_accuracy: 0.7669\n",
            "Epoch 208/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7584 - val_loss: 0.5285 - val_accuracy: 0.7663\n",
            "Epoch 209/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7580 - val_loss: 0.5297 - val_accuracy: 0.7644\n",
            "Epoch 210/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5353 - accuracy: 0.7592 - val_loss: 0.5278 - val_accuracy: 0.7663\n",
            "Epoch 211/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7598 - val_loss: 0.5277 - val_accuracy: 0.7631\n",
            "Epoch 212/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5351 - accuracy: 0.7592 - val_loss: 0.5271 - val_accuracy: 0.7638\n",
            "Epoch 213/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7597 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 214/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7598 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 215/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7595 - val_loss: 0.5285 - val_accuracy: 0.7656\n",
            "Epoch 216/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7580 - val_loss: 0.5295 - val_accuracy: 0.7631\n",
            "Epoch 217/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7584 - val_loss: 0.5291 - val_accuracy: 0.7663\n",
            "Epoch 218/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7602 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 219/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7592 - val_loss: 0.5291 - val_accuracy: 0.7663\n",
            "Epoch 220/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7592 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 221/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7580 - val_loss: 0.5274 - val_accuracy: 0.7656\n",
            "Epoch 222/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7588 - val_loss: 0.5280 - val_accuracy: 0.7644\n",
            "Epoch 223/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7594 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 224/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7598 - val_loss: 0.5285 - val_accuracy: 0.7631\n",
            "Epoch 225/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7592 - val_loss: 0.5285 - val_accuracy: 0.7656\n",
            "Epoch 226/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7583 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 227/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7608 - val_loss: 0.5279 - val_accuracy: 0.7644\n",
            "Epoch 228/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7594 - val_loss: 0.5276 - val_accuracy: 0.7656\n",
            "Epoch 229/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7595 - val_loss: 0.5290 - val_accuracy: 0.7663\n",
            "Epoch 230/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7589 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 231/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7603 - val_loss: 0.5287 - val_accuracy: 0.7656\n",
            "Epoch 232/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7588 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 233/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7597 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 234/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7594 - val_loss: 0.5282 - val_accuracy: 0.7631\n",
            "Epoch 235/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7578 - val_loss: 0.5301 - val_accuracy: 0.7669\n",
            "Epoch 236/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7580 - val_loss: 0.5275 - val_accuracy: 0.7650\n",
            "Epoch 237/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7591 - val_loss: 0.5280 - val_accuracy: 0.7656\n",
            "Epoch 238/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7591 - val_loss: 0.5276 - val_accuracy: 0.7638\n",
            "Epoch 239/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7595 - val_loss: 0.5283 - val_accuracy: 0.7663\n",
            "Epoch 240/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7592 - val_loss: 0.5281 - val_accuracy: 0.7663\n",
            "Epoch 241/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7586 - val_loss: 0.5278 - val_accuracy: 0.7663\n",
            "Epoch 242/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7598 - val_loss: 0.5283 - val_accuracy: 0.7650\n",
            "Epoch 243/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7595 - val_loss: 0.5278 - val_accuracy: 0.7650\n",
            "Epoch 244/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7591 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 245/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7591 - val_loss: 0.5282 - val_accuracy: 0.7644\n",
            "Epoch 246/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7589 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 247/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7583 - val_loss: 0.5289 - val_accuracy: 0.7663\n",
            "Epoch 248/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7586 - val_loss: 0.5280 - val_accuracy: 0.7631\n",
            "Epoch 249/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5342 - accuracy: 0.7588 - val_loss: 0.5282 - val_accuracy: 0.7669\n",
            "Epoch 250/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5350 - accuracy: 0.7578 - val_loss: 0.5292 - val_accuracy: 0.7631\n",
            "Epoch 251/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5340 - accuracy: 0.7592 - val_loss: 0.5278 - val_accuracy: 0.7644\n",
            "Epoch 252/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5347 - accuracy: 0.7592 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 253/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5342 - accuracy: 0.7594 - val_loss: 0.5277 - val_accuracy: 0.7650\n",
            "Epoch 254/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7589 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 255/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7592 - val_loss: 0.5272 - val_accuracy: 0.7644\n",
            "Epoch 256/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7597 - val_loss: 0.5285 - val_accuracy: 0.7644\n",
            "Epoch 257/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7597 - val_loss: 0.5289 - val_accuracy: 0.7644\n",
            "Epoch 258/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7594 - val_loss: 0.5290 - val_accuracy: 0.7669\n",
            "Epoch 259/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7592 - val_loss: 0.5271 - val_accuracy: 0.7656\n",
            "Epoch 260/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7586 - val_loss: 0.5301 - val_accuracy: 0.7638\n",
            "Epoch 261/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7598 - val_loss: 0.5291 - val_accuracy: 0.7631\n",
            "Epoch 262/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7598 - val_loss: 0.5278 - val_accuracy: 0.7644\n",
            "Epoch 263/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7586 - val_loss: 0.5279 - val_accuracy: 0.7663\n",
            "Epoch 264/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7589 - val_loss: 0.5279 - val_accuracy: 0.7644\n",
            "Epoch 265/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7586 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 266/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7586 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 267/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7594 - val_loss: 0.5277 - val_accuracy: 0.7669\n",
            "Epoch 268/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7592 - val_loss: 0.5275 - val_accuracy: 0.7638\n",
            "Epoch 269/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7588 - val_loss: 0.5279 - val_accuracy: 0.7656\n",
            "Epoch 270/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7602 - val_loss: 0.5291 - val_accuracy: 0.7638\n",
            "Epoch 271/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7594 - val_loss: 0.5300 - val_accuracy: 0.7638\n",
            "Epoch 272/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7575 - val_loss: 0.5276 - val_accuracy: 0.7650\n",
            "Epoch 273/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7591 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 274/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7588 - val_loss: 0.5290 - val_accuracy: 0.7631\n",
            "Epoch 275/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7591 - val_loss: 0.5271 - val_accuracy: 0.7650\n",
            "Epoch 276/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7603 - val_loss: 0.5315 - val_accuracy: 0.7663\n",
            "Epoch 277/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7589 - val_loss: 0.5276 - val_accuracy: 0.7663\n",
            "Epoch 278/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7598 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 279/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7595 - val_loss: 0.5292 - val_accuracy: 0.7631\n",
            "Epoch 280/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7598 - val_loss: 0.5281 - val_accuracy: 0.7650\n",
            "Epoch 281/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7580 - val_loss: 0.5280 - val_accuracy: 0.7663\n",
            "Epoch 282/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7581 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 283/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7591 - val_loss: 0.5278 - val_accuracy: 0.7644\n",
            "Epoch 284/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7597 - val_loss: 0.5279 - val_accuracy: 0.7644\n",
            "Epoch 285/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7597 - val_loss: 0.5281 - val_accuracy: 0.7663\n",
            "Epoch 286/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7597 - val_loss: 0.5284 - val_accuracy: 0.7644\n",
            "Epoch 287/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7586 - val_loss: 0.5276 - val_accuracy: 0.7650\n",
            "Epoch 288/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7584 - val_loss: 0.5287 - val_accuracy: 0.7656\n",
            "Epoch 289/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5342 - accuracy: 0.7597 - val_loss: 0.5276 - val_accuracy: 0.7663\n",
            "Epoch 290/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5346 - accuracy: 0.7588 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 291/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5346 - accuracy: 0.7592 - val_loss: 0.5290 - val_accuracy: 0.7656\n",
            "Epoch 292/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5349 - accuracy: 0.7594 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 293/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7594 - val_loss: 0.5280 - val_accuracy: 0.7631\n",
            "Epoch 294/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7584 - val_loss: 0.5288 - val_accuracy: 0.7663\n",
            "Epoch 295/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7597 - val_loss: 0.5281 - val_accuracy: 0.7663\n",
            "Epoch 296/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7589 - val_loss: 0.5271 - val_accuracy: 0.7650\n",
            "Epoch 297/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7589 - val_loss: 0.5271 - val_accuracy: 0.7644\n",
            "Epoch 298/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7598 - val_loss: 0.5290 - val_accuracy: 0.7656\n",
            "Epoch 299/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7592 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 300/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7592 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 301/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7592 - val_loss: 0.5272 - val_accuracy: 0.7644\n",
            "Epoch 302/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7580 - val_loss: 0.5276 - val_accuracy: 0.7663\n",
            "Epoch 303/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7600 - val_loss: 0.5280 - val_accuracy: 0.7644\n",
            "Epoch 304/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7584 - val_loss: 0.5279 - val_accuracy: 0.7663\n",
            "Epoch 305/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7588 - val_loss: 0.5275 - val_accuracy: 0.7669\n",
            "Epoch 306/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7592 - val_loss: 0.5297 - val_accuracy: 0.7638\n",
            "Epoch 307/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7592 - val_loss: 0.5283 - val_accuracy: 0.7631\n",
            "Epoch 308/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7600 - val_loss: 0.5293 - val_accuracy: 0.7631\n",
            "Epoch 309/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7603 - val_loss: 0.5278 - val_accuracy: 0.7631\n",
            "Epoch 310/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7592 - val_loss: 0.5294 - val_accuracy: 0.7663\n",
            "Epoch 311/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7600 - val_loss: 0.5289 - val_accuracy: 0.7644\n",
            "Epoch 312/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7581 - val_loss: 0.5278 - val_accuracy: 0.7650\n",
            "Epoch 313/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7581 - val_loss: 0.5300 - val_accuracy: 0.7644\n",
            "Epoch 314/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7591 - val_loss: 0.5273 - val_accuracy: 0.7631\n",
            "Epoch 315/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7581 - val_loss: 0.5274 - val_accuracy: 0.7656\n",
            "Epoch 316/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7589 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 317/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7602 - val_loss: 0.5285 - val_accuracy: 0.7663\n",
            "Epoch 318/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7594 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 319/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7589 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 320/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7584 - val_loss: 0.5290 - val_accuracy: 0.7663\n",
            "Epoch 321/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7598 - val_loss: 0.5278 - val_accuracy: 0.7644\n",
            "Epoch 322/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7588 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 323/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7594 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 324/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7583 - val_loss: 0.5281 - val_accuracy: 0.7638\n",
            "Epoch 325/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7592 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 326/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7589 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 327/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5346 - accuracy: 0.7594 - val_loss: 0.5282 - val_accuracy: 0.7656\n",
            "Epoch 328/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5338 - accuracy: 0.7595 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 329/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5340 - accuracy: 0.7595 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 330/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5341 - accuracy: 0.7589 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 331/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7583 - val_loss: 0.5274 - val_accuracy: 0.7656\n",
            "Epoch 332/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7580 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 333/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7586 - val_loss: 0.5273 - val_accuracy: 0.7650\n",
            "Epoch 334/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7592 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 335/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7598 - val_loss: 0.5278 - val_accuracy: 0.7669\n",
            "Epoch 336/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7597 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 337/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7589 - val_loss: 0.5294 - val_accuracy: 0.7625\n",
            "Epoch 338/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7589 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 339/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7591 - val_loss: 0.5271 - val_accuracy: 0.7638\n",
            "Epoch 340/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7591 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 341/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7592 - val_loss: 0.5277 - val_accuracy: 0.7631\n",
            "Epoch 342/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7584 - val_loss: 0.5271 - val_accuracy: 0.7644\n",
            "Epoch 343/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7594 - val_loss: 0.5278 - val_accuracy: 0.7663\n",
            "Epoch 344/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7589 - val_loss: 0.5281 - val_accuracy: 0.7631\n",
            "Epoch 345/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7597 - val_loss: 0.5275 - val_accuracy: 0.7656\n",
            "Epoch 346/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7591 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 347/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7589 - val_loss: 0.5295 - val_accuracy: 0.7663\n",
            "Epoch 348/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7595 - val_loss: 0.5285 - val_accuracy: 0.7663\n",
            "Epoch 349/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7588 - val_loss: 0.5279 - val_accuracy: 0.7663\n",
            "Epoch 350/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7586 - val_loss: 0.5281 - val_accuracy: 0.7644\n",
            "Epoch 351/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7589 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 352/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7591 - val_loss: 0.5275 - val_accuracy: 0.7669\n",
            "Epoch 353/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7592 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 354/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7598 - val_loss: 0.5275 - val_accuracy: 0.7631\n",
            "Epoch 355/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7605 - val_loss: 0.5273 - val_accuracy: 0.7663\n",
            "Epoch 356/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7580 - val_loss: 0.5278 - val_accuracy: 0.7663\n",
            "Epoch 357/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7595 - val_loss: 0.5273 - val_accuracy: 0.7656\n",
            "Epoch 358/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7586 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 359/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7598 - val_loss: 0.5277 - val_accuracy: 0.7663\n",
            "Epoch 360/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7600 - val_loss: 0.5277 - val_accuracy: 0.7663\n",
            "Epoch 361/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7598 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 362/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7584 - val_loss: 0.5273 - val_accuracy: 0.7644\n",
            "Epoch 363/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7589 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 364/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7594 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 365/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7586 - val_loss: 0.5276 - val_accuracy: 0.7663\n",
            "Epoch 366/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7584 - val_loss: 0.5274 - val_accuracy: 0.7656\n",
            "Epoch 367/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5341 - accuracy: 0.7584 - val_loss: 0.5279 - val_accuracy: 0.7644\n",
            "Epoch 368/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5342 - accuracy: 0.7589 - val_loss: 0.5282 - val_accuracy: 0.7663\n",
            "Epoch 369/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5341 - accuracy: 0.7588 - val_loss: 0.5277 - val_accuracy: 0.7638\n",
            "Epoch 370/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7606 - val_loss: 0.5272 - val_accuracy: 0.7669\n",
            "Epoch 371/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7569 - val_loss: 0.5274 - val_accuracy: 0.7631\n",
            "Epoch 372/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7594 - val_loss: 0.5271 - val_accuracy: 0.7644\n",
            "Epoch 373/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7588 - val_loss: 0.5272 - val_accuracy: 0.7644\n",
            "Epoch 374/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7592 - val_loss: 0.5277 - val_accuracy: 0.7638\n",
            "Epoch 375/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7598 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 376/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7589 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 377/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7589 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 378/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7583 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 379/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7589 - val_loss: 0.5285 - val_accuracy: 0.7663\n",
            "Epoch 380/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7591 - val_loss: 0.5278 - val_accuracy: 0.7631\n",
            "Epoch 381/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7591 - val_loss: 0.5278 - val_accuracy: 0.7631\n",
            "Epoch 382/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7580 - val_loss: 0.5279 - val_accuracy: 0.7663\n",
            "Epoch 383/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7597 - val_loss: 0.5274 - val_accuracy: 0.7663\n",
            "Epoch 384/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7589 - val_loss: 0.5276 - val_accuracy: 0.7663\n",
            "Epoch 385/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7591 - val_loss: 0.5275 - val_accuracy: 0.7650\n",
            "Epoch 386/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7592 - val_loss: 0.5272 - val_accuracy: 0.7644\n",
            "Epoch 387/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7594 - val_loss: 0.5271 - val_accuracy: 0.7638\n",
            "Epoch 388/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7598 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 389/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7591 - val_loss: 0.5281 - val_accuracy: 0.7656\n",
            "Epoch 390/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7589 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 391/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7594 - val_loss: 0.5283 - val_accuracy: 0.7656\n",
            "Epoch 392/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7586 - val_loss: 0.5275 - val_accuracy: 0.7656\n",
            "Epoch 393/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7592 - val_loss: 0.5278 - val_accuracy: 0.7656\n",
            "Epoch 394/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7589 - val_loss: 0.5285 - val_accuracy: 0.7631\n",
            "Epoch 395/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7584 - val_loss: 0.5273 - val_accuracy: 0.7644\n",
            "Epoch 396/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7600 - val_loss: 0.5277 - val_accuracy: 0.7625\n",
            "Epoch 397/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7591 - val_loss: 0.5278 - val_accuracy: 0.7638\n",
            "Epoch 398/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7588 - val_loss: 0.5282 - val_accuracy: 0.7663\n",
            "Epoch 399/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7605 - val_loss: 0.5272 - val_accuracy: 0.7644\n",
            "Epoch 400/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7594 - val_loss: 0.5277 - val_accuracy: 0.7638\n",
            "Epoch 401/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7588 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 402/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7597 - val_loss: 0.5282 - val_accuracy: 0.7663\n",
            "Epoch 403/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7592 - val_loss: 0.5283 - val_accuracy: 0.7663\n",
            "Epoch 404/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5353 - accuracy: 0.7583 - val_loss: 0.5290 - val_accuracy: 0.7656\n",
            "Epoch 405/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5342 - accuracy: 0.7592 - val_loss: 0.5274 - val_accuracy: 0.7638\n",
            "Epoch 406/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7588 - val_loss: 0.5288 - val_accuracy: 0.7663\n",
            "Epoch 407/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7586 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 408/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5338 - accuracy: 0.7592 - val_loss: 0.5271 - val_accuracy: 0.7675\n",
            "Epoch 409/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5340 - accuracy: 0.7591 - val_loss: 0.5278 - val_accuracy: 0.7631\n",
            "Epoch 410/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7597 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 411/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7605 - val_loss: 0.5290 - val_accuracy: 0.7669\n",
            "Epoch 412/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7588 - val_loss: 0.5276 - val_accuracy: 0.7650\n",
            "Epoch 413/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7578 - val_loss: 0.5270 - val_accuracy: 0.7650\n",
            "Epoch 414/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7586 - val_loss: 0.5275 - val_accuracy: 0.7638\n",
            "Epoch 415/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7595 - val_loss: 0.5281 - val_accuracy: 0.7656\n",
            "Epoch 416/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7597 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 417/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7581 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 418/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7592 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 419/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7597 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 420/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7578 - val_loss: 0.5275 - val_accuracy: 0.7638\n",
            "Epoch 421/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7595 - val_loss: 0.5279 - val_accuracy: 0.7625\n",
            "Epoch 422/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7602 - val_loss: 0.5284 - val_accuracy: 0.7663\n",
            "Epoch 423/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7592 - val_loss: 0.5289 - val_accuracy: 0.7638\n",
            "Epoch 424/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7581 - val_loss: 0.5284 - val_accuracy: 0.7638\n",
            "Epoch 425/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7603 - val_loss: 0.5271 - val_accuracy: 0.7650\n",
            "Epoch 426/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7614 - val_loss: 0.5329 - val_accuracy: 0.7625\n",
            "Epoch 427/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7592 - val_loss: 0.5271 - val_accuracy: 0.7644\n",
            "Epoch 428/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7584 - val_loss: 0.5274 - val_accuracy: 0.7638\n",
            "Epoch 429/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7592 - val_loss: 0.5276 - val_accuracy: 0.7663\n",
            "Epoch 430/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7591 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 431/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7594 - val_loss: 0.5272 - val_accuracy: 0.7650\n",
            "Epoch 432/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7595 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 433/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7595 - val_loss: 0.5276 - val_accuracy: 0.7631\n",
            "Epoch 434/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7591 - val_loss: 0.5272 - val_accuracy: 0.7644\n",
            "Epoch 435/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7586 - val_loss: 0.5278 - val_accuracy: 0.7650\n",
            "Epoch 436/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7597 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 437/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7595 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 438/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7584 - val_loss: 0.5289 - val_accuracy: 0.7656\n",
            "Epoch 439/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7591 - val_loss: 0.5273 - val_accuracy: 0.7650\n",
            "Epoch 440/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7581 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 441/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7589 - val_loss: 0.5273 - val_accuracy: 0.7644\n",
            "Epoch 442/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7597 - val_loss: 0.5273 - val_accuracy: 0.7656\n",
            "Epoch 443/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5349 - accuracy: 0.7589 - val_loss: 0.5273 - val_accuracy: 0.7625\n",
            "Epoch 444/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5339 - accuracy: 0.7588 - val_loss: 0.5278 - val_accuracy: 0.7656\n",
            "Epoch 445/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7597 - val_loss: 0.5283 - val_accuracy: 0.7638\n",
            "Epoch 446/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5339 - accuracy: 0.7591 - val_loss: 0.5271 - val_accuracy: 0.7638\n",
            "Epoch 447/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5338 - accuracy: 0.7588 - val_loss: 0.5273 - val_accuracy: 0.7663\n",
            "Epoch 448/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5341 - accuracy: 0.7589 - val_loss: 0.5280 - val_accuracy: 0.7663\n",
            "Epoch 449/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7606 - val_loss: 0.5280 - val_accuracy: 0.7663\n",
            "Epoch 450/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7589 - val_loss: 0.5277 - val_accuracy: 0.7663\n",
            "Epoch 451/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7588 - val_loss: 0.5282 - val_accuracy: 0.7663\n",
            "Epoch 452/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7584 - val_loss: 0.5272 - val_accuracy: 0.7650\n",
            "Epoch 453/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7598 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 454/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7583 - val_loss: 0.5279 - val_accuracy: 0.7650\n",
            "Epoch 455/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7595 - val_loss: 0.5278 - val_accuracy: 0.7644\n",
            "Epoch 456/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7595 - val_loss: 0.5279 - val_accuracy: 0.7638\n",
            "Epoch 457/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7594 - val_loss: 0.5273 - val_accuracy: 0.7663\n",
            "Epoch 458/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7594 - val_loss: 0.5280 - val_accuracy: 0.7631\n",
            "Epoch 459/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7591 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 460/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7591 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 461/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7586 - val_loss: 0.5280 - val_accuracy: 0.7638\n",
            "Epoch 462/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7595 - val_loss: 0.5275 - val_accuracy: 0.7656\n",
            "Epoch 463/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7592 - val_loss: 0.5271 - val_accuracy: 0.7638\n",
            "Epoch 464/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7595 - val_loss: 0.5274 - val_accuracy: 0.7656\n",
            "Epoch 465/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7595 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 466/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7586 - val_loss: 0.5276 - val_accuracy: 0.7631\n",
            "Epoch 467/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7591 - val_loss: 0.5275 - val_accuracy: 0.7631\n",
            "Epoch 468/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7583 - val_loss: 0.5279 - val_accuracy: 0.7656\n",
            "Epoch 469/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7594 - val_loss: 0.5274 - val_accuracy: 0.7644\n",
            "Epoch 470/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7588 - val_loss: 0.5284 - val_accuracy: 0.7638\n",
            "Epoch 471/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7597 - val_loss: 0.5278 - val_accuracy: 0.7638\n",
            "Epoch 472/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7588 - val_loss: 0.5273 - val_accuracy: 0.7663\n",
            "Epoch 473/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7591 - val_loss: 0.5278 - val_accuracy: 0.7663\n",
            "Epoch 474/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7595 - val_loss: 0.5292 - val_accuracy: 0.7638\n",
            "Epoch 475/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7586 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 476/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7598 - val_loss: 0.5285 - val_accuracy: 0.7663\n",
            "Epoch 477/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7589 - val_loss: 0.5272 - val_accuracy: 0.7638\n",
            "Epoch 478/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7602 - val_loss: 0.5279 - val_accuracy: 0.7663\n",
            "Epoch 479/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7578 - val_loss: 0.5279 - val_accuracy: 0.7663\n",
            "Epoch 480/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7594 - val_loss: 0.5280 - val_accuracy: 0.7663\n",
            "Epoch 481/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7588 - val_loss: 0.5277 - val_accuracy: 0.7669\n",
            "Epoch 482/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7600 - val_loss: 0.5271 - val_accuracy: 0.7644\n",
            "Epoch 483/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7592 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 484/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5344 - accuracy: 0.7584 - val_loss: 0.5273 - val_accuracy: 0.7650\n",
            "Epoch 485/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5339 - accuracy: 0.7592 - val_loss: 0.5271 - val_accuracy: 0.7650\n",
            "Epoch 486/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5339 - accuracy: 0.7592 - val_loss: 0.5287 - val_accuracy: 0.7638\n",
            "Epoch 487/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.7603 - val_loss: 0.5275 - val_accuracy: 0.7663\n",
            "Epoch 488/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7605 - val_loss: 0.5275 - val_accuracy: 0.7650\n",
            "Epoch 489/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7583 - val_loss: 0.5276 - val_accuracy: 0.7656\n",
            "Epoch 490/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7578 - val_loss: 0.5283 - val_accuracy: 0.7663\n",
            "Epoch 491/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7592 - val_loss: 0.5274 - val_accuracy: 0.7669\n",
            "Epoch 492/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7600 - val_loss: 0.5278 - val_accuracy: 0.7638\n",
            "Epoch 493/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7597 - val_loss: 0.5276 - val_accuracy: 0.7669\n",
            "Epoch 494/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7592 - val_loss: 0.5274 - val_accuracy: 0.7650\n",
            "Epoch 495/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7600 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 496/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7592 - val_loss: 0.5280 - val_accuracy: 0.7656\n",
            "Epoch 497/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7595 - val_loss: 0.5281 - val_accuracy: 0.7663\n",
            "Epoch 498/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7595 - val_loss: 0.5269 - val_accuracy: 0.7650\n",
            "Epoch 499/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7592 - val_loss: 0.5294 - val_accuracy: 0.7638\n",
            "Epoch 500/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7589 - val_loss: 0.5296 - val_accuracy: 0.7663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred_DNN = (model.predict(DNNX) > 0.5).astype(\"int32\")\n",
        "DTr = model.evaluate(DX_train, Dy_train,verbose=0)[1]\n",
        "DTe = model.evaluate(DX_test, Dy_test,verbose=0)[1]\n",
        "DTr,DTe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhG0YCbhASH9",
        "outputId": "baa526b6-37b6-48ff-d5d7-ced1022d7ee6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7607499957084656, 0.753000020980835)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_DNN = (model.predict(DX_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKbxrS57kOBE",
        "outputId": "ba511f5c-ac6b-41bb-d5c6-60b775e127cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"DNN\":[DTr,DTe]})\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "v_YjlCdOO1JY",
        "outputId": "7f970957-a639-4600-b989-e9edaf8d07e6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        train    test\n",
              "SVM  0.755750  0.7555\n",
              "KNN  0.772250  0.7340\n",
              "RF   0.751750  0.7565\n",
              "LR   0.761500  0.7565\n",
              "ANN  0.767500  0.7525\n",
              "XGB  0.775375  0.7555\n",
              "DNN  0.760750  0.7530"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4fbb4723-7b34-4528-be0e-6bf79b5f2113\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.755750</td>\n",
              "      <td>0.7555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>0.772250</td>\n",
              "      <td>0.7340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>0.751750</td>\n",
              "      <td>0.7565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.7565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>0.767500</td>\n",
              "      <td>0.7525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>0.775375</td>\n",
              "      <td>0.7555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>0.760750</td>\n",
              "      <td>0.7530</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4fbb4723-7b34-4528-be0e-6bf79b5f2113')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4fbb4723-7b34-4528-be0e-6bf79b5f2113 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4fbb4723-7b34-4528-be0e-6bf79b5f2113');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AutoML Individual and AutoML DNN**"
      ],
      "metadata": {
        "id": "nJaAMLDKKfKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H2O AutoML"
      ],
      "metadata": {
        "id": "EhNt_UkjDKcV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h2o\n",
        "import h2o\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "h2o.init()\n",
        "from h2o.model.segment_models import H2OFrame\n",
        "from h2o.automl import H2OAutoML\n",
        "print(\"All Library Loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "iwMF9ROeDOzF",
        "outputId": "902a50aa-b0f9-4169-82f3-6e0ec68f27e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h2o\n",
            "  Downloading h2o-3.40.0.4.tar.gz (177.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.8.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from h2o) (0.18.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.4)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.40.0.4-py2.py3-none-any.whl size=177697886 sha256=3f94ce178cecaa0ec2570dda9256118e6c01226ac51d251966c89183a140e0fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/f2/b0/5bb4d702a0467e82d77c45088db3eef25114c26b0eec8e7f6a\n",
            "Successfully built h2o\n",
            "Installing collected packages: h2o\n",
            "Successfully installed h2o-3.40.0.4\n",
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.19\" 2023-04-18; OpenJDK Runtime Environment (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1); OpenJDK 64-Bit Server VM (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.10/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpgwkxhhgo\n",
            "  JVM stdout: /tmp/tmpgwkxhhgo/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpgwkxhhgo/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         02 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.40.0.4\n",
              "H2O_cluster_version_age:    2 months and 7 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_6ueufm\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.170 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-1.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-1 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-1 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-1 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table th,\n",
              "#h2o-table-1 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>02 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.40.0.4</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>2 months and 7 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_6ueufm</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.170 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Library Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWdoOLbsF2qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-QiglXeF_XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8P1HTOHbugE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asKywHvIu83f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5wVvB0pvWHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train, valid = hdf.split_frame(ratios=[.8], seed=123)\n",
        "#hdf = h2o.H2OFrame(df)\n",
        "#hdf[\"diagnosis\"] = hdf[\"diagnosis\"].asfactor()\n",
        "hy = \"diagnosis\"\n",
        "hx = list(df.columns)\n",
        "hx.remove(hy)\n",
        "hdf  = df.copy()\n",
        "hdf.iloc[:,1:] = StandardScaler().fit_transform(hdf.iloc[:,1:])\n",
        "hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n",
        "hdf.iloc[:,0] = hdf.iloc[:,0].astype('category')\n",
        "train1, valid1 = train_test_split(hdf, test_size=0.2,random_state=123)\n",
        "train = h2o.H2OFrame(train1)\n",
        "valid = h2o.H2OFrame(valid1)\n",
        "train[\"diagnosis\"] = train[\"diagnosis\"].asfactor()\n",
        "valid[\"diagnosis\"] = valid[\"diagnosis\"].asfactor()"
      ],
      "metadata": {
        "id": "NVoFNbYCGxGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9928e669-ed74-4311-aea9-a46b6d21ba88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-460708a37676>:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = time.time()\n",
        "aml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\",\n",
        "                nfolds=10, sort_metric='accuracy')\n",
        "aml.train(x = hx, y = hy, training_frame = train,\n",
        "          validation_frame = valid)\n",
        "autoend = time.time() - st"
      ],
      "metadata": {
        "id": "7JNO6XnVHH5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6caa511a-9532-4c03-a6e8-98da58e90552"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoML progress: |\n",
            "16:26:10.259: Project: AutoML_1_20230705_162610\n",
            "16:26:10.260: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "16:26:10.270: Setting stopping tolerance adaptively based on the training frame: 0.011180339887498949\n",
            "16:26:10.270: Build control seed: 123\n",
            "16:26:10.274: training frame: Frame key: AutoML_1_20230705_162610_training_py_1_sid_956a    cols: 31    rows: 8000  chunks: 2    size: 1926161  checksum: -5743320669190601360\n",
            "16:26:10.275: validation frame: Frame key: py_2_sid_956a    cols: 31    rows: 2000  chunks: 1    size: 482912  checksum: 7051888715556309816\n",
            "16:26:10.276: leaderboard frame: NULL\n",
            "16:26:10.276: blending frame: NULL\n",
            "16:26:10.276: response column: diagnosis\n",
            "16:26:10.276: fold column: null\n",
            "16:26:10.276: weights column: null\n",
            "16:26:10.306: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "16:26:10.339: AutoML job created: 2023.07.05 16:26:10.215\n",
            "16:26:10.340: AutoML build started: 2023.07.05 16:26:10.339\n",
            "16:26:10.388: AutoML: starting XGBoost_1_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "16:26:35.832: New leader: XGBoost_1_AutoML_1_20230705_162610, accuracy: 0.731375\n",
            "16:26:35.838: AutoML: starting GLM_1_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "16:26:51.684: AutoML: starting GBM_1_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆ\n",
            "16:27:19.99: AutoML: starting XGBoost_2_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆ\n",
            "16:27:36.12: AutoML: starting DRF_1_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "16:28:18.446: AutoML: starting GBM_2_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆ\n",
            "16:28:37.686: AutoML: starting GBM_3_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆ\n",
            "16:28:58.119: AutoML: starting GBM_4_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "16:29:20.194: AutoML: starting XGBoost_3_AutoML_1_20230705_162610 model training\n",
            "\n",
            "\n",
            "16:29:31.742: AutoML: starting XRT_1_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆ\n",
            "16:30:23.818: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "16:30:23.833: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "16:30:35.622: AutoML: starting StackedEnsemble_AllModels_1_AutoML_1_20230705_162610 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "\n",
            "16:30:49.867: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "16:30:49.868: AutoML build stopped: 2023.07.05 16:30:49.867\n",
            "16:30:49.868: AutoML build done: built 10 models\n",
            "16:30:49.868: AutoML duration:  4 min 39.528 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lbo606kFH4Zc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lb = aml.leaderboard\n",
        "lb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "gmCXw_D_4y1D",
        "outputId": "7a6f6c5d-c2e7-44f9-c699-8bab3a55f520"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_id                                                accuracy       auc    logloss     aucpr    mean_per_class_error      rmse       mse\n",
              "----------------------------------------------------  ----------  --------  ---------  --------  ----------------------  --------  --------\n",
              "XGBoost_1_AutoML_1_20230705_162610                      0.731375  0.789913   0.561354  0.704715                0.281571  0.431458  0.186156\n",
              "XGBoost_2_AutoML_1_20230705_162610                      0.732875  0.786979   0.569302  0.710217                0.292189  0.432906  0.187407\n",
              "GBM_4_AutoML_1_20230705_162610                          0.74325   0.804633   0.521738  0.73211                 0.271387  0.416969  0.173863\n",
              "XGBoost_3_AutoML_1_20230705_162610                      0.746125  0.807007   0.522189  0.733116                0.269031  0.41621   0.173231\n",
              "DRF_1_AutoML_1_20230705_162610                          0.747375  0.804947   0.538154  0.731415                0.271721  0.415924  0.172993\n",
              "XRT_1_AutoML_1_20230705_162610                          0.747875  0.806856   0.550448  0.732292                0.261301  0.415221  0.172409\n",
              "GBM_3_AutoML_1_20230705_162610                          0.750125  0.812317   0.513038  0.737965                0.266742  0.412843  0.170439\n",
              "GBM_2_AutoML_1_20230705_162610                          0.7515    0.813887   0.51051   0.741607                0.261279  0.411848  0.169619\n",
              "GBM_1_AutoML_1_20230705_162610                          0.752625  0.816537   0.506866  0.747142                0.259403  0.409777  0.167917\n",
              "StackedEnsemble_AllModels_1_AutoML_1_20230705_162610    0.760375  0.830748   0.489772  0.765725                0.249518  0.402301  0.161846\n",
              "[10 rows x 8 columns]\n"
            ],
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th>model_id                                            </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>XGBoost_1_AutoML_1_20230705_162610                  </td><td style=\"text-align: right;\">  0.731375</td><td style=\"text-align: right;\">0.789913</td><td style=\"text-align: right;\"> 0.561354</td><td style=\"text-align: right;\">0.704715</td><td style=\"text-align: right;\">              0.281571</td><td style=\"text-align: right;\">0.431458</td><td style=\"text-align: right;\">0.186156</td></tr>\n",
              "<tr><td>XGBoost_2_AutoML_1_20230705_162610                  </td><td style=\"text-align: right;\">  0.732875</td><td style=\"text-align: right;\">0.786979</td><td style=\"text-align: right;\"> 0.569302</td><td style=\"text-align: right;\">0.710217</td><td style=\"text-align: right;\">              0.292189</td><td style=\"text-align: right;\">0.432906</td><td style=\"text-align: right;\">0.187407</td></tr>\n",
              "<tr><td>GBM_4_AutoML_1_20230705_162610                      </td><td style=\"text-align: right;\">  0.74325 </td><td style=\"text-align: right;\">0.804633</td><td style=\"text-align: right;\"> 0.521738</td><td style=\"text-align: right;\">0.73211 </td><td style=\"text-align: right;\">              0.271387</td><td style=\"text-align: right;\">0.416969</td><td style=\"text-align: right;\">0.173863</td></tr>\n",
              "<tr><td>XGBoost_3_AutoML_1_20230705_162610                  </td><td style=\"text-align: right;\">  0.746125</td><td style=\"text-align: right;\">0.807007</td><td style=\"text-align: right;\"> 0.522189</td><td style=\"text-align: right;\">0.733116</td><td style=\"text-align: right;\">              0.269031</td><td style=\"text-align: right;\">0.41621 </td><td style=\"text-align: right;\">0.173231</td></tr>\n",
              "<tr><td>DRF_1_AutoML_1_20230705_162610                      </td><td style=\"text-align: right;\">  0.747375</td><td style=\"text-align: right;\">0.804947</td><td style=\"text-align: right;\"> 0.538154</td><td style=\"text-align: right;\">0.731415</td><td style=\"text-align: right;\">              0.271721</td><td style=\"text-align: right;\">0.415924</td><td style=\"text-align: right;\">0.172993</td></tr>\n",
              "<tr><td>XRT_1_AutoML_1_20230705_162610                      </td><td style=\"text-align: right;\">  0.747875</td><td style=\"text-align: right;\">0.806856</td><td style=\"text-align: right;\"> 0.550448</td><td style=\"text-align: right;\">0.732292</td><td style=\"text-align: right;\">              0.261301</td><td style=\"text-align: right;\">0.415221</td><td style=\"text-align: right;\">0.172409</td></tr>\n",
              "<tr><td>GBM_3_AutoML_1_20230705_162610                      </td><td style=\"text-align: right;\">  0.750125</td><td style=\"text-align: right;\">0.812317</td><td style=\"text-align: right;\"> 0.513038</td><td style=\"text-align: right;\">0.737965</td><td style=\"text-align: right;\">              0.266742</td><td style=\"text-align: right;\">0.412843</td><td style=\"text-align: right;\">0.170439</td></tr>\n",
              "<tr><td>GBM_2_AutoML_1_20230705_162610                      </td><td style=\"text-align: right;\">  0.7515  </td><td style=\"text-align: right;\">0.813887</td><td style=\"text-align: right;\"> 0.51051 </td><td style=\"text-align: right;\">0.741607</td><td style=\"text-align: right;\">              0.261279</td><td style=\"text-align: right;\">0.411848</td><td style=\"text-align: right;\">0.169619</td></tr>\n",
              "<tr><td>GBM_1_AutoML_1_20230705_162610                      </td><td style=\"text-align: right;\">  0.752625</td><td style=\"text-align: right;\">0.816537</td><td style=\"text-align: right;\"> 0.506866</td><td style=\"text-align: right;\">0.747142</td><td style=\"text-align: right;\">              0.259403</td><td style=\"text-align: right;\">0.409777</td><td style=\"text-align: right;\">0.167917</td></tr>\n",
              "<tr><td>StackedEnsemble_AllModels_1_AutoML_1_20230705_162610</td><td style=\"text-align: right;\">  0.760375</td><td style=\"text-align: right;\">0.830748</td><td style=\"text-align: right;\"> 0.489772</td><td style=\"text-align: right;\">0.765725</td><td style=\"text-align: right;\">              0.249518</td><td style=\"text-align: right;\">0.402301</td><td style=\"text-align: right;\">0.161846</td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10 rows x 8 columns]</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "best_model"
      ],
      "metadata": {
        "id": "vlH8zwtisQU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca656833-e3c6-49d5-aaba-f54bfce9c7a1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230705_162610\n",
              "\n",
              "\n",
              "Model Summary: \n",
              "    number_of_trees\n",
              "--  -----------------\n",
              "    30\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.0862762318888866\n",
              "RMSE: 0.2937281598500331\n",
              "LogLoss: 0.2961195502729631\n",
              "Mean Per-Class Error: 0.10560430092631945\n",
              "AUC: 0.9574856020131994\n",
              "AUCPR: 0.9379440756884645\n",
              "Gini: 0.9149712040263989\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40771300310180303\n",
              "       0     1     Error    Rate\n",
              "-----  ----  ----  -------  --------------\n",
              "0      4317  551   0.1132   (551.0/4868.0)\n",
              "1      307   2825  0.098    (307.0/3132.0)\n",
              "Total  4624  3376  0.1072   (858.0/8000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.407713     0.868162  213\n",
              "max f2                       0.291031     0.900738  254\n",
              "max f0point5                 0.608403     0.885993  148\n",
              "max accuracy                 0.44304      0.894625  201\n",
              "max precision                0.993199     1         0\n",
              "max recall                   0.0188069    1         390\n",
              "max specificity              0.993199     1         0\n",
              "max absolute_mcc             0.426081     0.780124  207\n",
              "max min_per_class_accuracy   0.419471     0.89272   209\n",
              "max mean_per_class_accuracy  0.383103     0.894835  222\n",
              "max tns                      0.993199     4868      0\n",
              "max fns                      0.993199     3120      0\n",
              "max fps                      0.0052119    4868      399\n",
              "max tps                      0.0188069    3132      390\n",
              "max tnr                      0.993199     1         0\n",
              "max fnr                      0.993199     0.996169  0\n",
              "max fpr                      0.0052119    1         399\n",
              "max tpr                      0.0188069    1         390\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 39.32 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.983273           2.55428    2.55428            1                0.988143   1                           0.988143            0.0255428       0.0255428                  155.428   155.428            0.0255428\n",
              "2        0.02                        0.974009           2.55428    2.55428            1                0.978908   1                           0.983526            0.0255428       0.0510856                  155.428   155.428            0.0510856\n",
              "3        0.03                        0.965              2.52235    2.54364            0.9875           0.969758   0.995833                    0.978936            0.0252235       0.0763091                  152.235   154.364            0.0761036\n",
              "4        0.04                        0.956618           2.55428    2.5463             1                0.960834   0.996875                    0.974411            0.0255428       0.101852                   155.428   154.63             0.101646\n",
              "5        0.05                        0.948257           2.49042    2.53512            0.975            0.952464   0.9925                      0.970021            0.0249042       0.126756                   149.042   153.512            0.12614\n",
              "6        0.1                         0.891808           2.52235    2.52874            0.9875           0.92015    0.99                        0.945086            0.126117        0.252874                   152.235   152.874            0.25123\n",
              "7        0.15                        0.834331           2.49042    2.51596            0.975            0.864595   0.985                       0.918255            0.124521        0.377395                   149.042   151.596            0.373697\n",
              "8        0.2                         0.778986           2.40741    2.48883            0.9425           0.807305   0.974375                    0.890518            0.12037         0.497765                   140.741   148.883            0.489343\n",
              "9        0.3                         0.626419           2.20626    2.39464            0.86375          0.704981   0.9375                      0.828672            0.220626        0.718391                   120.626   139.464            0.687577\n",
              "10       0.4                         0.439384           1.57727    2.19029            0.6175           0.53251    0.8575                      0.754632            0.157727        0.876117                   57.7267   119.029            0.782445\n",
              "11       0.5                         0.291733           0.740741   1.90038            0.29             0.362017   0.744                       0.676109            0.0740741       0.950192                   -25.9259  90.0383            0.739838\n",
              "12       0.6                         0.186727           0.322478   1.6374             0.12625          0.235522   0.641042                    0.602678            0.0322478       0.982439                   -67.7522  63.7399            0.628495\n",
              "13       0.7                         0.118291           0.0957854  1.41717            0.0375           0.14983    0.554821                    0.537985            0.00957854      0.992018                   -90.4215  41.7168            0.479898\n",
              "14       0.8                         0.0704665          0.0510856  1.24641            0.02             0.0932805  0.487969                    0.482397            0.00510856      0.997126                   -94.8914  24.6408            0.323955\n",
              "15       0.9                         0.0350014          0.0159642  1.10969            0.00625          0.0520768  0.434444                    0.434584            0.00159642      0.998723                   -98.4036  10.9692            0.16224\n",
              "16       1                           0.00378296         0.0127714  1                  0.005            0.0206824  0.3915                      0.393194            0.00127714      1                          -98.7229  0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.18840019637319533\n",
              "RMSE: 0.4340509144941355\n",
              "LogLoss: 0.5694655292500879\n",
              "Mean Per-Class Error: 0.29928818116277384\n",
              "AUC: 0.7805786689418005\n",
              "AUCPR: 0.7132335417937792\n",
              "Gini: 0.561157337883601\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24615386625130972\n",
              "       0    1     Error    Rate\n",
              "-----  ---  ----  -------  --------------\n",
              "0      731  486   0.3993   (486.0/1217.0)\n",
              "1      156  627   0.1992   (156.0/783.0)\n",
              "Total  887  1113  0.321    (642.0/2000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.246154     0.661392  271\n",
              "max f2                       0.0822697    0.783171  350\n",
              "max f0point5                 0.628715     0.680179  134\n",
              "max accuracy                 0.628715     0.7385    134\n",
              "max precision                0.996239     1         0\n",
              "max recall                   0.00914458   1         397\n",
              "max specificity              0.996239     1         0\n",
              "max absolute_mcc             0.628715     0.435211  134\n",
              "max min_per_class_accuracy   0.354121     0.699872  226\n",
              "max mean_per_class_accuracy  0.552612     0.706514  157\n",
              "max tns                      0.996239     1217      0\n",
              "max fns                      0.996239     782       0\n",
              "max fps                      0.00487721   1217      399\n",
              "max tps                      0.00914458   783       397\n",
              "max tnr                      0.996239     1         0\n",
              "max fnr                      0.996239     0.998723  0\n",
              "max fpr                      0.00487721   1         399\n",
              "max tpr                      0.00914458   1         397\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.46 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.977798           2.55428   2.55428            1                0.985284   1                           0.985284            0.0255428       0.0255428                  155.428   155.428            0.0255428\n",
              "2        0.02                        0.964573           2.55428   2.55428            1                0.972878   1                           0.979081            0.0255428       0.0510856                  155.428   155.428            0.0510856\n",
              "3        0.03                        0.954787           2.29885   2.46914            0.9              0.958362   0.966667                    0.972175            0.0229885       0.0740741                  129.885   146.914            0.0724307\n",
              "4        0.04                        0.943778           2.17114   2.39464            0.85             0.950531   0.9375                      0.966764            0.0217114       0.0957854                  117.114   139.464            0.091677\n",
              "5        0.05                        0.928851           2.04342   2.32439            0.8              0.935592   0.91                        0.960529            0.0204342       0.11622                    104.342   132.439            0.108824\n",
              "6        0.1                         0.874916           1.94125   2.13282            0.76             0.902028   0.835                       0.931279            0.0970626       0.213282                   94.1252   113.282            0.186166\n",
              "7        0.15                        0.800584           1.96679   2.07748            0.77             0.837897   0.813333                    0.900151            0.0983397       0.311622                   96.6794   107.748            0.265607\n",
              "8        0.2                         0.727813           1.76245   1.99872            0.69             0.76954    0.7825                      0.867499            0.0881226       0.399745                   76.2452   99.8723            0.328257\n",
              "9        0.3                         0.572579           1.45594   1.81779            0.57             0.649716   0.711667                    0.794904            0.145594        0.545338                   45.5939   81.7795            0.403186\n",
              "10       0.4                         0.427932           0.970626  1.606              0.38             0.50131    0.62875                     0.721506            0.0970626       0.642401                   -2.93742  60.6003            0.398358\n",
              "11       0.5                         0.303219           0.91954   1.46871            0.36             0.363956   0.575                       0.649996            0.091954        0.734355                   -8.04598  46.871             0.385136\n",
              "12       0.6                         0.208844           0.881226  1.3708             0.345            0.252344   0.536667                    0.58372             0.0881226       0.822478                   -11.8774  37.0796            0.365617\n",
              "13       0.7                         0.130075           0.702427  1.27531            0.275            0.165416   0.499286                    0.523963            0.0702427       0.89272                    -29.7573  27.5315            0.316714\n",
              "14       0.8                         0.0777253          0.523627  1.18135            0.205            0.10211    0.4625                      0.471231            0.0523627       0.945083                   -47.6373  18.1354            0.238427\n",
              "15       0.9                         0.0354974          0.332056  1.08699            0.13             0.0543392  0.425556                    0.42491             0.0332056       0.978289                   -66.7944  8.69874            0.128658\n",
              "16       1                           0.00410937         0.217114  1                  0.085            0.0213709  0.3915                      0.384556            0.0217114       1                          -78.2886  0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.18615605059426837\n",
              "RMSE: 0.43145805195206216\n",
              "LogLoss: 0.5613540167599764\n",
              "Mean Per-Class Error: 0.28157062936622623\n",
              "AUC: 0.7899126007045779\n",
              "AUCPR: 0.704715036081932\n",
              "Gini: 0.5798252014091558\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3172192946076393\n",
              "       0     1     Error    Rate\n",
              "-----  ----  ----  -------  ---------------\n",
              "0      3297  1571  0.3227   (1571.0/4868.0)\n",
              "1      753   2379  0.2404   (753.0/3132.0)\n",
              "Total  4050  3950  0.2905   (2324.0/8000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.317219     0.671844  248\n",
              "max f2                       0.0903366    0.790684  347\n",
              "max f0point5                 0.562613     0.661942  158\n",
              "max accuracy                 0.548578     0.731375  163\n",
              "max precision                0.984683     0.941176  2\n",
              "max recall                   0.00676566   1         398\n",
              "max specificity              0.991555     0.999795  0\n",
              "max absolute_mcc             0.395143     0.434769  217\n",
              "max min_per_class_accuracy   0.369361     0.71871   227\n",
              "max mean_per_class_accuracy  0.395143     0.720713  217\n",
              "max tns                      0.991555     4867      0\n",
              "max fns                      0.991555     3120      0\n",
              "max fps                      0.00360703   4868      399\n",
              "max tps                      0.00676566   3132      398\n",
              "max tnr                      0.991555     0.999795  0\n",
              "max fnr                      0.991555     0.996169  0\n",
              "max fpr                      0.00360703   1         399\n",
              "max tpr                      0.00676566   1         398\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.96 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.976782           2.29885   2.29885            0.9              0.98354    0.9                         0.98354             0.0229885       0.0229885                  129.885   129.885            0.0213451\n",
              "2        0.02                        0.96877            2.36271   2.33078            0.925            0.972395   0.9125                      0.977967            0.0236271       0.0466156                  136.271   133.078            0.0437397\n",
              "3        0.03                        0.958395           2.26692   2.30949            0.8875           0.963849   0.904167                    0.973261            0.0226692       0.0692848                  126.692   130.949            0.0645601\n",
              "4        0.04                        0.947065           2.26692   2.29885            0.8875           0.952408   0.9                         0.968048            0.0226692       0.091954                   126.692   129.885            0.0853805\n",
              "5        0.05                        0.93624            2.23499   2.28608            0.875            0.941784   0.895                       0.962795            0.0223499       0.114304                   123.499   128.608            0.105676\n",
              "6        0.1                         0.879107           1.94764   2.11686            0.7625           0.908667   0.82875                     0.935731            0.0973819       0.211686                   94.7637   111.686            0.183543\n",
              "7        0.15                        0.814872           1.81354   2.01575            0.71             0.846586   0.789167                    0.906016            0.0906769       0.302363                   81.3538   101.575            0.250391\n",
              "8        0.2                         0.743607           1.67305   1.93008            0.655            0.781333   0.755625                    0.874845            0.0836526       0.386015                   67.3052   93.0077            0.305695\n",
              "9        0.3                         0.588081           1.47829   1.77948            0.57875          0.666473   0.696667                    0.805388            0.147829        0.533844                   47.8289   77.9481            0.384296\n",
              "10       0.4                         0.443764           1.28033   1.65469            0.50125          0.512584   0.647813                    0.732187            0.128033        0.661877                   28.0332   65.4693            0.430365\n",
              "11       0.5                         0.306942           1.01533   1.52682            0.3975           0.37215    0.59775                     0.66018             0.101533        0.76341                    1.53257   52.682             0.432884\n",
              "12       0.6                         0.202824           0.766284  1.40006            0.3              0.251989   0.548125                    0.592148            0.0766284       0.840038                   -23.3716  40.0064            0.394475\n",
              "13       0.7                         0.130317           0.664112  1.29493            0.26             0.163729   0.506964                    0.530945            0.0664112       0.90645                    -33.5888  29.4928            0.339276\n",
              "14       0.8                         0.0758979          0.472542  1.19213            0.185            0.101729   0.466719                    0.477293            0.0472542       0.953704                   -52.7458  19.213             0.252594\n",
              "15       0.9                         0.0371696          0.322478  1.0955             0.12625          0.0557451  0.428889                    0.430455            0.0322478       0.985951                   -67.7522  9.55016            0.141251\n",
              "16       1                           0.00228818         0.140485  1                  0.055            0.0215713  0.3915                      0.389566            0.0140485       1                          -85.9515  0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                         mean      sd         cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid    cv_6_valid    cv_7_valid    cv_8_valid    cv_9_valid    cv_10_valid\n",
              "-----------------------  --------  ---------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  -------------\n",
              "accuracy                 0.70825   0.0371661  0.7275        0.74375       0.74          0.70375       0.61875       0.6975        0.7375        0.705         0.72375       0.685\n",
              "auc                      0.789564  0.0191126  0.791147      0.797844      0.78883       0.801228      0.766344      0.776762      0.831018      0.78962       0.788937      0.763911\n",
              "err                      0.29175   0.0371661  0.2725        0.25625       0.26          0.29625       0.38125       0.3025        0.2625        0.295         0.27625       0.315\n",
              "err_count                233.4     29.7329    218           205           208           237           305           242           210           236           221           252\n",
              "f0point5                 0.629483  0.0312905  0.641696      0.648259      0.655488      0.60241       0.570673      0.637743      0.675287      0.638809      0.629467      0.595\n",
              "f1                       0.679528  0.0206461  0.669697      0.678179      0.673981      0.669456      0.664466      0.689744      0.728682      0.692708      0.674521      0.653846\n",
              "f2                       0.740414  0.0352846  0.700253      0.710994      0.693548      0.753296      0.795155      0.750977      0.791246      0.756542      0.726523      0.72561\n",
              "lift_top_group           2.34294   0.336948   1.96078       2.38095       2.63158       2.73973       2.42424       1.78042       2.08333       2.12121       2.67559       2.63158\n",
              "logloss                  0.561354  0.0306372  0.563873      0.538003      0.554789      0.533056      0.596285      0.5928        0.506254      0.576416      0.552059      0.600005\n",
              "max_per_class_error      0.354375  0.0925364  0.277778      0.265306      0.292763      0.364173      0.589362      0.37581       0.336207      0.365957      0.301397      0.375\n",
              "mcc                      0.437511  0.0391816  0.443246      0.471221      0.459964      0.441607      0.358681      0.419569      0.498854      0.43528       0.450001      0.396688\n",
              "mean_per_class_accuracy  0.721276  0.0247419  0.726496      0.741853      0.733659      0.728872      0.662895      0.711205      0.751539      0.720052      0.732245      0.703947\n",
              "mean_per_class_error     0.278724  0.0247419  0.273504      0.258147      0.266341      0.271128      0.337105      0.288795      0.248461      0.279948      0.267755      0.296053\n",
              "mse                      0.186156  0.0112054  0.185587      0.175314      0.181778      0.177739      0.200074      0.198868      0.167041      0.191238      0.184604      0.199318\n",
              "pr_auc                   0.706007  0.0296863  0.684173      0.702552      0.70519       0.707094      0.706578      0.687794      0.782989      0.702086      0.709587      0.672029\n",
              "precision                0.600636  0.0398683  0.624294      0.629738      0.643713      0.564706      0.521589      0.607223      0.643836      0.607306      0.602632      0.561321\n",
              "r2                       0.21708   0.0442381  0.214258      0.245779      0.228448      0.233138      0.174422      0.184293      0.314283      0.210882      0.2113        0.153998\n",
              "recall                   0.789357  0.0619735  0.722222      0.734694      0.707237      0.821918      0.915152      0.79822       0.839286      0.806061      0.765886      0.782895\n",
              "rmse                     0.431281  0.0130248  0.430798      0.418705      0.426354      0.421592      0.447296      0.445946      0.408706      0.437308      0.429656      0.446451\n",
              "specificity              0.653196  0.0999643  0.730769      0.749012      0.760081      0.635827      0.410638      0.62419       0.663793      0.634043      0.698603      0.625\n",
              "\n",
              "Scoring History: \n",
              "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
              "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
              "    2023-07-05 16:26:33  23.379 sec  0                  0.5              0.693147            0.5             0.3915             1                0.6085                           0.5                0.693147              0.5               0.3915               1                  0.6085\n",
              "    2023-07-05 16:26:34  23.694 sec  5                  0.387143         0.469449            0.8686          0.815761           2.52235          0.211                            0.420583           0.532308              0.797085          0.718904             2.17114            0.2715\n",
              "    2023-07-05 16:26:34  23.931 sec  10                 0.35995          0.410703            0.896726        0.85392            2.55428          0.182                            0.420456           0.531574              0.795512          0.722968             2.42656            0.2735\n",
              "    2023-07-05 16:26:34  24.226 sec  15                 0.340863         0.374872            0.917331        0.881451           2.55428          0.15525                          0.425685           0.546093              0.788397          0.717152             2.42656            0.3105\n",
              "    2023-07-05 16:26:34  24.533 sec  20                 0.322802         0.343603            0.934676        0.906835           2.55428          0.135625                         0.428904           0.556092              0.784902          0.714457             2.55428            0.2845\n",
              "    2023-07-05 16:26:35  24.855 sec  25                 0.30702          0.31811             0.947853        0.924756           2.55428          0.118625                         0.431961           0.564559              0.781737          0.712846             2.55428            0.2935\n",
              "    2023-07-05 16:26:35  25.134 sec  30                 0.293728         0.29612             0.957486        0.937944           2.55428          0.10725                          0.434051           0.569466              0.780579          0.713234             2.55428            0.321\n",
              "\n",
              "Variable Importances: \n",
              "variable                 relative_importance    scaled_importance    percentage\n",
              "-----------------------  ---------------------  -------------------  --------------------\n",
              "area_se                  1904.8582763671875     1.0                  0.17710163529899145\n",
              "perimeter_worst          670.3985595703125      0.35194143726474486  0.06232940406906372\n",
              "smoothness_worst         490.8770751953125      0.25769742625235037  0.0456386356016325\n",
              "texture_worst            471.06378173828125     0.24729597344987847  0.04379652130082945\n",
              "radius_se                406.78167724609375     0.2135495759935901   0.03781997912587105\n",
              "symmetry_se              383.16064453125        0.2011491612184331   0.03562384539080497\n",
              "area_worst               379.52288818359375     0.19923943575865039  0.03528562988890534\n",
              "texture_se               376.7027893066406      0.19775895875312144  0.035023434990203606\n",
              "radius_worst             356.6051025390625      0.18720820701640586  0.03315487960399761\n",
              "symmetry_mean            341.8689880371094      0.17947213830999437  0.031784809185306774\n",
              "---                      ---                    ---                  ---\n",
              "compactness_mean         257.6020812988281      0.13523425049243495  0.02395020691064367\n",
              "smoothness_se            247.90921020507812     0.13014575062133926  0.023049025262253914\n",
              "fractal_dimension_worst  239.37608337402344     0.12566608568409865  0.02225566927627705\n",
              "perimeter_mean           235.8315887451172      0.1238053201495277   0.021926124657196535\n",
              "fractal_dimension_mean   229.67420959472656     0.12057285964221189  0.021353650615311495\n",
              "compactness_worst        214.68325805664062     0.11270300826057754  0.019959887066064025\n",
              "concave points_mean      202.2340850830078      0.10616752311289768  0.01880244195893765\n",
              "radius_mean              184.11338806152344     0.09665463848189883  0.01711769453437711\n",
              "area_mean                175.94384765625        0.09236584676094528  0.01635814250713946\n",
              "compactness_se           169.79820251464844     0.08913954629657578  0.015786759418933725\n",
              "[30 rows x 4 columns]\n",
              "\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ],
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230705_162610\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-2.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-2 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-2 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-2 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table th,\n",
              "#h2o-table-2 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>number_of_trees</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>30.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.0862762318888866\n",
              "RMSE: 0.2937281598500331\n",
              "LogLoss: 0.2961195502729631\n",
              "Mean Per-Class Error: 0.10560430092631945\n",
              "AUC: 0.9574856020131994\n",
              "AUCPR: 0.9379440756884645\n",
              "Gini: 0.9149712040263989</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-3.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-3 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-3 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-3 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table th,\n",
              "#h2o-table-3 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40771300310180303</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>4317.0</td>\n",
              "<td>551.0</td>\n",
              "<td>0.1132</td>\n",
              "<td> (551.0/4868.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>307.0</td>\n",
              "<td>2825.0</td>\n",
              "<td>0.098</td>\n",
              "<td> (307.0/3132.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>4624.0</td>\n",
              "<td>3376.0</td>\n",
              "<td>0.1072</td>\n",
              "<td> (858.0/8000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-4.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-4 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-4 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-4 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table th,\n",
              "#h2o-table-4 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-4 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-4\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4077130</td>\n",
              "<td>0.8681623</td>\n",
              "<td>213.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.2910308</td>\n",
              "<td>0.9007375</td>\n",
              "<td>254.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6084025</td>\n",
              "<td>0.8859932</td>\n",
              "<td>148.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.4430403</td>\n",
              "<td>0.894625</td>\n",
              "<td>201.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0188069</td>\n",
              "<td>1.0</td>\n",
              "<td>390.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4260813</td>\n",
              "<td>0.7801245</td>\n",
              "<td>207.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.4194713</td>\n",
              "<td>0.8927203</td>\n",
              "<td>209.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3831034</td>\n",
              "<td>0.8948353</td>\n",
              "<td>222.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9931994</td>\n",
              "<td>4868.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9931994</td>\n",
              "<td>3120.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0052119</td>\n",
              "<td>4868.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0188069</td>\n",
              "<td>3132.0</td>\n",
              "<td>390.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9931994</td>\n",
              "<td>0.9961686</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0052119</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0188069</td>\n",
              "<td>1.0</td>\n",
              "<td>390.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-5.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-5 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-5 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-5 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table th,\n",
              "#h2o-table-5 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 39.32 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9832729</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9881432</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9881432</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0255428</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0255428</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9740088</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9789083</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9835258</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0510856</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0510856</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9650004</td>\n",
              "<td>2.5223499</td>\n",
              "<td>2.5436356</td>\n",
              "<td>0.9875</td>\n",
              "<td>0.9697575</td>\n",
              "<td>0.9958333</td>\n",
              "<td>0.9789364</td>\n",
              "<td>0.0252235</td>\n",
              "<td>0.0763091</td>\n",
              "<td>152.2349936</td>\n",
              "<td>154.3635590</td>\n",
              "<td>0.0761036</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9566181</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5462963</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9608343</td>\n",
              "<td>0.996875</td>\n",
              "<td>0.9744108</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.1018519</td>\n",
              "<td>155.4278416</td>\n",
              "<td>154.6296296</td>\n",
              "<td>0.1016464</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9482571</td>\n",
              "<td>2.4904215</td>\n",
              "<td>2.5351213</td>\n",
              "<td>0.975</td>\n",
              "<td>0.9524639</td>\n",
              "<td>0.9925</td>\n",
              "<td>0.9700214</td>\n",
              "<td>0.0249042</td>\n",
              "<td>0.1267561</td>\n",
              "<td>149.0421456</td>\n",
              "<td>153.5121328</td>\n",
              "<td>0.1261398</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8918075</td>\n",
              "<td>2.5223499</td>\n",
              "<td>2.5287356</td>\n",
              "<td>0.9875</td>\n",
              "<td>0.9201499</td>\n",
              "<td>0.99</td>\n",
              "<td>0.9450857</td>\n",
              "<td>0.1261175</td>\n",
              "<td>0.2528736</td>\n",
              "<td>152.2349936</td>\n",
              "<td>152.8735632</td>\n",
              "<td>0.2512302</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8343306</td>\n",
              "<td>2.4904215</td>\n",
              "<td>2.5159642</td>\n",
              "<td>0.975</td>\n",
              "<td>0.8645949</td>\n",
              "<td>0.985</td>\n",
              "<td>0.9182554</td>\n",
              "<td>0.1245211</td>\n",
              "<td>0.3773946</td>\n",
              "<td>149.0421456</td>\n",
              "<td>151.5964240</td>\n",
              "<td>0.3736970</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7789860</td>\n",
              "<td>2.4074074</td>\n",
              "<td>2.4888250</td>\n",
              "<td>0.9425</td>\n",
              "<td>0.8073055</td>\n",
              "<td>0.974375</td>\n",
              "<td>0.8905179</td>\n",
              "<td>0.1203704</td>\n",
              "<td>0.4977650</td>\n",
              "<td>140.7407407</td>\n",
              "<td>148.8825032</td>\n",
              "<td>0.4893427</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.6264191</td>\n",
              "<td>2.2062580</td>\n",
              "<td>2.3946360</td>\n",
              "<td>0.86375</td>\n",
              "<td>0.7049811</td>\n",
              "<td>0.9375</td>\n",
              "<td>0.8286723</td>\n",
              "<td>0.2206258</td>\n",
              "<td>0.7183908</td>\n",
              "<td>120.6257982</td>\n",
              "<td>139.4636015</td>\n",
              "<td>0.6875773</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4393838</td>\n",
              "<td>1.5772669</td>\n",
              "<td>2.1902937</td>\n",
              "<td>0.6175</td>\n",
              "<td>0.5325101</td>\n",
              "<td>0.8575</td>\n",
              "<td>0.7546318</td>\n",
              "<td>0.1577267</td>\n",
              "<td>0.8761175</td>\n",
              "<td>57.7266922</td>\n",
              "<td>119.0293742</td>\n",
              "<td>0.7824445</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.2917330</td>\n",
              "<td>0.7407407</td>\n",
              "<td>1.9003831</td>\n",
              "<td>0.29</td>\n",
              "<td>0.3620169</td>\n",
              "<td>0.744</td>\n",
              "<td>0.6761088</td>\n",
              "<td>0.0740741</td>\n",
              "<td>0.9501916</td>\n",
              "<td>-25.9259259</td>\n",
              "<td>90.0383142</td>\n",
              "<td>0.7398382</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.1867268</td>\n",
              "<td>0.3224777</td>\n",
              "<td>1.6373989</td>\n",
              "<td>0.12625</td>\n",
              "<td>0.2355222</td>\n",
              "<td>0.6410417</td>\n",
              "<td>0.6026777</td>\n",
              "<td>0.0322478</td>\n",
              "<td>0.9824393</td>\n",
              "<td>-67.7522350</td>\n",
              "<td>63.7398893</td>\n",
              "<td>0.6284952</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1182911</td>\n",
              "<td>0.0957854</td>\n",
              "<td>1.4171684</td>\n",
              "<td>0.0375</td>\n",
              "<td>0.1498300</td>\n",
              "<td>0.5548214</td>\n",
              "<td>0.5379852</td>\n",
              "<td>0.0095785</td>\n",
              "<td>0.9920179</td>\n",
              "<td>-90.4214559</td>\n",
              "<td>41.7168400</td>\n",
              "<td>0.4798979</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0704665</td>\n",
              "<td>0.0510856</td>\n",
              "<td>1.2464080</td>\n",
              "<td>0.02</td>\n",
              "<td>0.0932805</td>\n",
              "<td>0.4879688</td>\n",
              "<td>0.4823971</td>\n",
              "<td>0.0051086</td>\n",
              "<td>0.9971264</td>\n",
              "<td>-94.8914432</td>\n",
              "<td>24.6408046</td>\n",
              "<td>0.3239547</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0350014</td>\n",
              "<td>0.0159642</td>\n",
              "<td>1.1096921</td>\n",
              "<td>0.00625</td>\n",
              "<td>0.0520768</td>\n",
              "<td>0.4344444</td>\n",
              "<td>0.4345837</td>\n",
              "<td>0.0015964</td>\n",
              "<td>0.9987229</td>\n",
              "<td>-98.4035760</td>\n",
              "<td>10.9692068</td>\n",
              "<td>0.1622397</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0037830</td>\n",
              "<td>0.0127714</td>\n",
              "<td>1.0</td>\n",
              "<td>0.005</td>\n",
              "<td>0.0206824</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3931936</td>\n",
              "<td>0.0012771</td>\n",
              "<td>1.0</td>\n",
              "<td>-98.7228608</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.18840019637319533\n",
              "RMSE: 0.4340509144941355\n",
              "LogLoss: 0.5694655292500879\n",
              "Mean Per-Class Error: 0.29928818116277384\n",
              "AUC: 0.7805786689418005\n",
              "AUCPR: 0.7132335417937792\n",
              "Gini: 0.561157337883601</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-6.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-6 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-6 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-6 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table th,\n",
              "#h2o-table-6 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24615386625130972</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>731.0</td>\n",
              "<td>486.0</td>\n",
              "<td>0.3993</td>\n",
              "<td> (486.0/1217.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>156.0</td>\n",
              "<td>627.0</td>\n",
              "<td>0.1992</td>\n",
              "<td> (156.0/783.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>887.0</td>\n",
              "<td>1113.0</td>\n",
              "<td>0.321</td>\n",
              "<td> (642.0/2000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-7.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-7 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-7 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-7 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table th,\n",
              "#h2o-table-7 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-7 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-7\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.2461539</td>\n",
              "<td>0.6613924</td>\n",
              "<td>271.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.0822697</td>\n",
              "<td>0.7831708</td>\n",
              "<td>350.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.6801786</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.7385</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0091446</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.4352113</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3541214</td>\n",
              "<td>0.6998723</td>\n",
              "<td>226.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.5526123</td>\n",
              "<td>0.7065135</td>\n",
              "<td>157.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1217.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9962388</td>\n",
              "<td>782.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0048772</td>\n",
              "<td>1217.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0091446</td>\n",
              "<td>783.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9962388</td>\n",
              "<td>0.9987229</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0048772</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0091446</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-8.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-8 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-8 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-8 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table th,\n",
              "#h2o-table-8 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-8 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-8\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.46 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9777978</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9852836</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9852836</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0255428</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0255428</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9645726</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9728781</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9790808</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0510856</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0510856</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9547869</td>\n",
              "<td>2.2988506</td>\n",
              "<td>2.4691358</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9583625</td>\n",
              "<td>0.9666667</td>\n",
              "<td>0.9721747</td>\n",
              "<td>0.0229885</td>\n",
              "<td>0.0740741</td>\n",
              "<td>129.8850575</td>\n",
              "<td>146.9135802</td>\n",
              "<td>0.0724307</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9437785</td>\n",
              "<td>2.1711367</td>\n",
              "<td>2.3946360</td>\n",
              "<td>0.85</td>\n",
              "<td>0.9505307</td>\n",
              "<td>0.9375</td>\n",
              "<td>0.9667637</td>\n",
              "<td>0.0217114</td>\n",
              "<td>0.0957854</td>\n",
              "<td>117.1136654</td>\n",
              "<td>139.4636015</td>\n",
              "<td>0.0916770</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9288514</td>\n",
              "<td>2.0434227</td>\n",
              "<td>2.3243934</td>\n",
              "<td>0.8</td>\n",
              "<td>0.9355922</td>\n",
              "<td>0.91</td>\n",
              "<td>0.9605294</td>\n",
              "<td>0.0204342</td>\n",
              "<td>0.1162197</td>\n",
              "<td>104.3422733</td>\n",
              "<td>132.4393359</td>\n",
              "<td>0.1088244</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8749162</td>\n",
              "<td>1.9412516</td>\n",
              "<td>2.1328225</td>\n",
              "<td>0.76</td>\n",
              "<td>0.9020278</td>\n",
              "<td>0.835</td>\n",
              "<td>0.9312786</td>\n",
              "<td>0.0970626</td>\n",
              "<td>0.2132822</td>\n",
              "<td>94.1251596</td>\n",
              "<td>113.2822478</td>\n",
              "<td>0.1861664</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8005845</td>\n",
              "<td>1.9667944</td>\n",
              "<td>2.0774798</td>\n",
              "<td>0.77</td>\n",
              "<td>0.8378969</td>\n",
              "<td>0.8133333</td>\n",
              "<td>0.9001514</td>\n",
              "<td>0.0983397</td>\n",
              "<td>0.3116220</td>\n",
              "<td>96.6794381</td>\n",
              "<td>107.7479779</td>\n",
              "<td>0.2656072</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7278127</td>\n",
              "<td>1.7624521</td>\n",
              "<td>1.9987229</td>\n",
              "<td>0.69</td>\n",
              "<td>0.7695403</td>\n",
              "<td>0.7825</td>\n",
              "<td>0.8674986</td>\n",
              "<td>0.0881226</td>\n",
              "<td>0.3997446</td>\n",
              "<td>76.2452107</td>\n",
              "<td>99.8722861</td>\n",
              "<td>0.3282573</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.5725785</td>\n",
              "<td>1.4559387</td>\n",
              "<td>1.8177948</td>\n",
              "<td>0.57</td>\n",
              "<td>0.6497156</td>\n",
              "<td>0.7116667</td>\n",
              "<td>0.7949043</td>\n",
              "<td>0.1455939</td>\n",
              "<td>0.5453384</td>\n",
              "<td>45.5938697</td>\n",
              "<td>81.7794806</td>\n",
              "<td>0.4031856</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4279322</td>\n",
              "<td>0.9706258</td>\n",
              "<td>1.6060026</td>\n",
              "<td>0.38</td>\n",
              "<td>0.5013098</td>\n",
              "<td>0.62875</td>\n",
              "<td>0.7215057</td>\n",
              "<td>0.0970626</td>\n",
              "<td>0.6424010</td>\n",
              "<td>-2.9374202</td>\n",
              "<td>60.6002554</td>\n",
              "<td>0.3983583</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3032194</td>\n",
              "<td>0.9195402</td>\n",
              "<td>1.4687101</td>\n",
              "<td>0.36</td>\n",
              "<td>0.3639555</td>\n",
              "<td>0.575</td>\n",
              "<td>0.6499956</td>\n",
              "<td>0.0919540</td>\n",
              "<td>0.7343550</td>\n",
              "<td>-8.0459770</td>\n",
              "<td>46.8710089</td>\n",
              "<td>0.3851357</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.2088440</td>\n",
              "<td>0.8812261</td>\n",
              "<td>1.3707961</td>\n",
              "<td>0.345</td>\n",
              "<td>0.2523442</td>\n",
              "<td>0.5366667</td>\n",
              "<td>0.5837204</td>\n",
              "<td>0.0881226</td>\n",
              "<td>0.8224777</td>\n",
              "<td>-11.8773946</td>\n",
              "<td>37.0796083</td>\n",
              "<td>0.3656165</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1300754</td>\n",
              "<td>0.7024266</td>\n",
              "<td>1.2753147</td>\n",
              "<td>0.275</td>\n",
              "<td>0.1654164</td>\n",
              "<td>0.4992857</td>\n",
              "<td>0.5239627</td>\n",
              "<td>0.0702427</td>\n",
              "<td>0.8927203</td>\n",
              "<td>-29.7573436</td>\n",
              "<td>27.5314724</td>\n",
              "<td>0.3167137</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0777253</td>\n",
              "<td>0.5236271</td>\n",
              "<td>1.1813538</td>\n",
              "<td>0.205</td>\n",
              "<td>0.1021104</td>\n",
              "<td>0.4625</td>\n",
              "<td>0.4712311</td>\n",
              "<td>0.0523627</td>\n",
              "<td>0.9450830</td>\n",
              "<td>-47.6372925</td>\n",
              "<td>18.1353768</td>\n",
              "<td>0.2384273</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0354974</td>\n",
              "<td>0.3320562</td>\n",
              "<td>1.0869874</td>\n",
              "<td>0.13</td>\n",
              "<td>0.0543392</td>\n",
              "<td>0.4255556</td>\n",
              "<td>0.4249098</td>\n",
              "<td>0.0332056</td>\n",
              "<td>0.9782886</td>\n",
              "<td>-66.7943806</td>\n",
              "<td>8.6987371</td>\n",
              "<td>0.1286584</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0041094</td>\n",
              "<td>0.2171137</td>\n",
              "<td>1.0</td>\n",
              "<td>0.085</td>\n",
              "<td>0.0213709</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3845559</td>\n",
              "<td>0.0217114</td>\n",
              "<td>1.0</td>\n",
              "<td>-78.2886335</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.18615605059426837\n",
              "RMSE: 0.43145805195206216\n",
              "LogLoss: 0.5613540167599764\n",
              "Mean Per-Class Error: 0.28157062936622623\n",
              "AUC: 0.7899126007045779\n",
              "AUCPR: 0.704715036081932\n",
              "Gini: 0.5798252014091558</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-9.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-9 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-9 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-9 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table th,\n",
              "#h2o-table-9 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-9 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-9\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3172192946076393</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>3297.0</td>\n",
              "<td>1571.0</td>\n",
              "<td>0.3227</td>\n",
              "<td> (1571.0/4868.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>753.0</td>\n",
              "<td>2379.0</td>\n",
              "<td>0.2404</td>\n",
              "<td> (753.0/3132.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>4050.0</td>\n",
              "<td>3950.0</td>\n",
              "<td>0.2905</td>\n",
              "<td> (2324.0/8000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-10.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-10 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-10 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-10 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table th,\n",
              "#h2o-table-10 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-10 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-10\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3172193</td>\n",
              "<td>0.6718441</td>\n",
              "<td>248.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.0903366</td>\n",
              "<td>0.7906840</td>\n",
              "<td>347.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.5626134</td>\n",
              "<td>0.6619421</td>\n",
              "<td>158.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.5485779</td>\n",
              "<td>0.731375</td>\n",
              "<td>163.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9846833</td>\n",
              "<td>0.9411765</td>\n",
              "<td>2.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0067657</td>\n",
              "<td>1.0</td>\n",
              "<td>398.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9997946</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.3951432</td>\n",
              "<td>0.4347693</td>\n",
              "<td>217.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3693605</td>\n",
              "<td>0.7187101</td>\n",
              "<td>227.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3951432</td>\n",
              "<td>0.7207126</td>\n",
              "<td>217.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9915547</td>\n",
              "<td>4867.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9915547</td>\n",
              "<td>3120.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0036070</td>\n",
              "<td>4868.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0067657</td>\n",
              "<td>3132.0</td>\n",
              "<td>398.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9997946</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9961686</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0036070</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0067657</td>\n",
              "<td>1.0</td>\n",
              "<td>398.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-11.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-11 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-11 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-11 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table th,\n",
              "#h2o-table-11 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-11 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-11\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.96 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9767816</td>\n",
              "<td>2.2988506</td>\n",
              "<td>2.2988506</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9835398</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9835398</td>\n",
              "<td>0.0229885</td>\n",
              "<td>0.0229885</td>\n",
              "<td>129.8850575</td>\n",
              "<td>129.8850575</td>\n",
              "<td>0.0213451</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9687700</td>\n",
              "<td>2.3627075</td>\n",
              "<td>2.3307791</td>\n",
              "<td>0.925</td>\n",
              "<td>0.9723946</td>\n",
              "<td>0.9125</td>\n",
              "<td>0.9779672</td>\n",
              "<td>0.0236271</td>\n",
              "<td>0.0466156</td>\n",
              "<td>136.2707535</td>\n",
              "<td>133.0779055</td>\n",
              "<td>0.0437397</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9583953</td>\n",
              "<td>2.2669221</td>\n",
              "<td>2.3094934</td>\n",
              "<td>0.8875</td>\n",
              "<td>0.9638492</td>\n",
              "<td>0.9041667</td>\n",
              "<td>0.9732612</td>\n",
              "<td>0.0226692</td>\n",
              "<td>0.0692848</td>\n",
              "<td>126.6922095</td>\n",
              "<td>130.9493401</td>\n",
              "<td>0.0645601</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9470655</td>\n",
              "<td>2.2669221</td>\n",
              "<td>2.2988506</td>\n",
              "<td>0.8875</td>\n",
              "<td>0.9524081</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9680479</td>\n",
              "<td>0.0226692</td>\n",
              "<td>0.0919540</td>\n",
              "<td>126.6922095</td>\n",
              "<td>129.8850575</td>\n",
              "<td>0.0853805</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9362398</td>\n",
              "<td>2.2349936</td>\n",
              "<td>2.2860792</td>\n",
              "<td>0.875</td>\n",
              "<td>0.9417836</td>\n",
              "<td>0.895</td>\n",
              "<td>0.9627951</td>\n",
              "<td>0.0223499</td>\n",
              "<td>0.1143040</td>\n",
              "<td>123.4993614</td>\n",
              "<td>128.6079183</td>\n",
              "<td>0.1056762</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8791068</td>\n",
              "<td>1.9476373</td>\n",
              "<td>2.1168582</td>\n",
              "<td>0.7625</td>\n",
              "<td>0.9086671</td>\n",
              "<td>0.82875</td>\n",
              "<td>0.9357311</td>\n",
              "<td>0.0973819</td>\n",
              "<td>0.2116858</td>\n",
              "<td>94.7637292</td>\n",
              "<td>111.6858238</td>\n",
              "<td>0.1835428</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8148719</td>\n",
              "<td>1.8135377</td>\n",
              "<td>2.0157514</td>\n",
              "<td>0.71</td>\n",
              "<td>0.8465859</td>\n",
              "<td>0.7891667</td>\n",
              "<td>0.9060160</td>\n",
              "<td>0.0906769</td>\n",
              "<td>0.3023627</td>\n",
              "<td>81.3537676</td>\n",
              "<td>101.5751384</td>\n",
              "<td>0.2503906</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7436068</td>\n",
              "<td>1.6730524</td>\n",
              "<td>1.9300766</td>\n",
              "<td>0.655</td>\n",
              "<td>0.7813334</td>\n",
              "<td>0.755625</td>\n",
              "<td>0.8748454</td>\n",
              "<td>0.0836526</td>\n",
              "<td>0.3860153</td>\n",
              "<td>67.3052363</td>\n",
              "<td>93.0076628</td>\n",
              "<td>0.3056949</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.5880806</td>\n",
              "<td>1.4782886</td>\n",
              "<td>1.7794806</td>\n",
              "<td>0.57875</td>\n",
              "<td>0.6664730</td>\n",
              "<td>0.6966667</td>\n",
              "<td>0.8053879</td>\n",
              "<td>0.1478289</td>\n",
              "<td>0.5338442</td>\n",
              "<td>47.8288633</td>\n",
              "<td>77.9480630</td>\n",
              "<td>0.3842961</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4437644</td>\n",
              "<td>1.2803321</td>\n",
              "<td>1.6546935</td>\n",
              "<td>0.50125</td>\n",
              "<td>0.5125842</td>\n",
              "<td>0.6478125</td>\n",
              "<td>0.7321870</td>\n",
              "<td>0.1280332</td>\n",
              "<td>0.6618774</td>\n",
              "<td>28.0332056</td>\n",
              "<td>65.4693487</td>\n",
              "<td>0.4303655</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3069417</td>\n",
              "<td>1.0153257</td>\n",
              "<td>1.5268199</td>\n",
              "<td>0.3975</td>\n",
              "<td>0.3721498</td>\n",
              "<td>0.59775</td>\n",
              "<td>0.6601796</td>\n",
              "<td>0.1015326</td>\n",
              "<td>0.7634100</td>\n",
              "<td>1.5325670</td>\n",
              "<td>52.6819923</td>\n",
              "<td>0.4328841</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.2028235</td>\n",
              "<td>0.7662835</td>\n",
              "<td>1.4000639</td>\n",
              "<td>0.3</td>\n",
              "<td>0.2519894</td>\n",
              "<td>0.548125</td>\n",
              "<td>0.5921479</td>\n",
              "<td>0.0766284</td>\n",
              "<td>0.8400383</td>\n",
              "<td>-23.3716475</td>\n",
              "<td>40.0063857</td>\n",
              "<td>0.3944755</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1303173</td>\n",
              "<td>0.6641124</td>\n",
              "<td>1.2949279</td>\n",
              "<td>0.26</td>\n",
              "<td>0.1637290</td>\n",
              "<td>0.5069643</td>\n",
              "<td>0.5309452</td>\n",
              "<td>0.0664112</td>\n",
              "<td>0.9064496</td>\n",
              "<td>-33.5887612</td>\n",
              "<td>29.4927933</td>\n",
              "<td>0.3392762</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0758979</td>\n",
              "<td>0.4725415</td>\n",
              "<td>1.1921296</td>\n",
              "<td>0.185</td>\n",
              "<td>0.1017293</td>\n",
              "<td>0.4667188</td>\n",
              "<td>0.4772932</td>\n",
              "<td>0.0472542</td>\n",
              "<td>0.9537037</td>\n",
              "<td>-52.7458493</td>\n",
              "<td>19.2129630</td>\n",
              "<td>0.2525944</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0371696</td>\n",
              "<td>0.3224777</td>\n",
              "<td>1.0955016</td>\n",
              "<td>0.12625</td>\n",
              "<td>0.0557451</td>\n",
              "<td>0.4288889</td>\n",
              "<td>0.4304545</td>\n",
              "<td>0.0322478</td>\n",
              "<td>0.9859515</td>\n",
              "<td>-67.7522350</td>\n",
              "<td>9.5501632</td>\n",
              "<td>0.1412514</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0022882</td>\n",
              "<td>0.1404853</td>\n",
              "<td>1.0</td>\n",
              "<td>0.055</td>\n",
              "<td>0.0215713</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3895662</td>\n",
              "<td>0.0140485</td>\n",
              "<td>1.0</td>\n",
              "<td>-85.9514687</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-12.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-12 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-12 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-12 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table th,\n",
              "#h2o-table-12 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-12 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-12\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th>\n",
              "<th>cv_6_valid</th>\n",
              "<th>cv_7_valid</th>\n",
              "<th>cv_8_valid</th>\n",
              "<th>cv_9_valid</th>\n",
              "<th>cv_10_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.70825</td>\n",
              "<td>0.0371661</td>\n",
              "<td>0.7275</td>\n",
              "<td>0.74375</td>\n",
              "<td>0.74</td>\n",
              "<td>0.70375</td>\n",
              "<td>0.61875</td>\n",
              "<td>0.6975</td>\n",
              "<td>0.7375</td>\n",
              "<td>0.705</td>\n",
              "<td>0.72375</td>\n",
              "<td>0.685</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.7895642</td>\n",
              "<td>0.0191126</td>\n",
              "<td>0.7911474</td>\n",
              "<td>0.7978442</td>\n",
              "<td>0.7888304</td>\n",
              "<td>0.8012283</td>\n",
              "<td>0.7663443</td>\n",
              "<td>0.7767623</td>\n",
              "<td>0.8310178</td>\n",
              "<td>0.7896196</td>\n",
              "<td>0.7889371</td>\n",
              "<td>0.7639107</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.29175</td>\n",
              "<td>0.0371661</td>\n",
              "<td>0.2725</td>\n",
              "<td>0.25625</td>\n",
              "<td>0.26</td>\n",
              "<td>0.29625</td>\n",
              "<td>0.38125</td>\n",
              "<td>0.3025</td>\n",
              "<td>0.2625</td>\n",
              "<td>0.295</td>\n",
              "<td>0.27625</td>\n",
              "<td>0.315</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>233.4</td>\n",
              "<td>29.732885</td>\n",
              "<td>218.0</td>\n",
              "<td>205.0</td>\n",
              "<td>208.0</td>\n",
              "<td>237.0</td>\n",
              "<td>305.0</td>\n",
              "<td>242.0</td>\n",
              "<td>210.0</td>\n",
              "<td>236.0</td>\n",
              "<td>221.0</td>\n",
              "<td>252.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.6294831</td>\n",
              "<td>0.0312905</td>\n",
              "<td>0.6416957</td>\n",
              "<td>0.6482593</td>\n",
              "<td>0.6554878</td>\n",
              "<td>0.6024097</td>\n",
              "<td>0.5706727</td>\n",
              "<td>0.637743</td>\n",
              "<td>0.6752874</td>\n",
              "<td>0.6388088</td>\n",
              "<td>0.6294667</td>\n",
              "<td>0.595</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.6795281</td>\n",
              "<td>0.0206461</td>\n",
              "<td>0.669697</td>\n",
              "<td>0.6781790</td>\n",
              "<td>0.6739812</td>\n",
              "<td>0.6694561</td>\n",
              "<td>0.6644664</td>\n",
              "<td>0.6897436</td>\n",
              "<td>0.7286822</td>\n",
              "<td>0.6927083</td>\n",
              "<td>0.6745213</td>\n",
              "<td>0.6538461</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.7404144</td>\n",
              "<td>0.0352846</td>\n",
              "<td>0.7002535</td>\n",
              "<td>0.7109941</td>\n",
              "<td>0.6935484</td>\n",
              "<td>0.7532957</td>\n",
              "<td>0.7951554</td>\n",
              "<td>0.7509771</td>\n",
              "<td>0.7912458</td>\n",
              "<td>0.7565416</td>\n",
              "<td>0.7265229</td>\n",
              "<td>0.7256098</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.3429408</td>\n",
              "<td>0.3369483</td>\n",
              "<td>1.9607843</td>\n",
              "<td>2.3809524</td>\n",
              "<td>2.631579</td>\n",
              "<td>2.739726</td>\n",
              "<td>2.4242425</td>\n",
              "<td>1.7804154</td>\n",
              "<td>2.0833333</td>\n",
              "<td>2.121212</td>\n",
              "<td>2.6755853</td>\n",
              "<td>2.631579</td></tr>\n",
              "<tr><td>logloss</td>\n",
              "<td>0.5613540</td>\n",
              "<td>0.0306372</td>\n",
              "<td>0.5638731</td>\n",
              "<td>0.5380028</td>\n",
              "<td>0.5547891</td>\n",
              "<td>0.5330562</td>\n",
              "<td>0.5962847</td>\n",
              "<td>0.5928004</td>\n",
              "<td>0.5062535</td>\n",
              "<td>0.576416</td>\n",
              "<td>0.5520594</td>\n",
              "<td>0.600005</td></tr>\n",
              "<tr><td>max_per_class_error</td>\n",
              "<td>0.3543753</td>\n",
              "<td>0.0925364</td>\n",
              "<td>0.2777778</td>\n",
              "<td>0.2653061</td>\n",
              "<td>0.2927631</td>\n",
              "<td>0.3641732</td>\n",
              "<td>0.5893617</td>\n",
              "<td>0.3758099</td>\n",
              "<td>0.3362069</td>\n",
              "<td>0.3659574</td>\n",
              "<td>0.3013972</td>\n",
              "<td>0.375</td></tr>\n",
              "<tr><td>mcc</td>\n",
              "<td>0.4375112</td>\n",
              "<td>0.0391816</td>\n",
              "<td>0.4432463</td>\n",
              "<td>0.4712214</td>\n",
              "<td>0.4599636</td>\n",
              "<td>0.4416075</td>\n",
              "<td>0.3586807</td>\n",
              "<td>0.4195689</td>\n",
              "<td>0.4988538</td>\n",
              "<td>0.4352803</td>\n",
              "<td>0.4500011</td>\n",
              "<td>0.3966878</td></tr>\n",
              "<tr><td>mean_per_class_accuracy</td>\n",
              "<td>0.7212762</td>\n",
              "<td>0.0247419</td>\n",
              "<td>0.7264957</td>\n",
              "<td>0.7418529</td>\n",
              "<td>0.7336587</td>\n",
              "<td>0.7288723</td>\n",
              "<td>0.6628949</td>\n",
              "<td>0.7112048</td>\n",
              "<td>0.7515394</td>\n",
              "<td>0.7200516</td>\n",
              "<td>0.7322445</td>\n",
              "<td>0.7039474</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.2787238</td>\n",
              "<td>0.0247419</td>\n",
              "<td>0.2735043</td>\n",
              "<td>0.2581471</td>\n",
              "<td>0.2663413</td>\n",
              "<td>0.2711277</td>\n",
              "<td>0.3371051</td>\n",
              "<td>0.2887952</td>\n",
              "<td>0.2484606</td>\n",
              "<td>0.2799484</td>\n",
              "<td>0.2677555</td>\n",
              "<td>0.2960526</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.1861560</td>\n",
              "<td>0.0112054</td>\n",
              "<td>0.1855874</td>\n",
              "<td>0.1753139</td>\n",
              "<td>0.1817776</td>\n",
              "<td>0.1777394</td>\n",
              "<td>0.2000736</td>\n",
              "<td>0.1988681</td>\n",
              "<td>0.1670406</td>\n",
              "<td>0.1912379</td>\n",
              "<td>0.1846039</td>\n",
              "<td>0.1993182</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.706007</td>\n",
              "<td>0.0296863</td>\n",
              "<td>0.6841726</td>\n",
              "<td>0.7025523</td>\n",
              "<td>0.7051897</td>\n",
              "<td>0.7070935</td>\n",
              "<td>0.7065780</td>\n",
              "<td>0.6877939</td>\n",
              "<td>0.7829888</td>\n",
              "<td>0.7020859</td>\n",
              "<td>0.7095867</td>\n",
              "<td>0.6720285</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.6006356</td>\n",
              "<td>0.0398683</td>\n",
              "<td>0.6242938</td>\n",
              "<td>0.6297376</td>\n",
              "<td>0.6437126</td>\n",
              "<td>0.5647059</td>\n",
              "<td>0.5215889</td>\n",
              "<td>0.6072234</td>\n",
              "<td>0.6438356</td>\n",
              "<td>0.6073059</td>\n",
              "<td>0.6026316</td>\n",
              "<td>0.5613208</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.2170801</td>\n",
              "<td>0.0442381</td>\n",
              "<td>0.2142580</td>\n",
              "<td>0.2457793</td>\n",
              "<td>0.2284481</td>\n",
              "<td>0.2331382</td>\n",
              "<td>0.1744221</td>\n",
              "<td>0.1842931</td>\n",
              "<td>0.3142833</td>\n",
              "<td>0.2108816</td>\n",
              "<td>0.2113000</td>\n",
              "<td>0.1539976</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.7893569</td>\n",
              "<td>0.0619735</td>\n",
              "<td>0.7222222</td>\n",
              "<td>0.7346939</td>\n",
              "<td>0.7072368</td>\n",
              "<td>0.8219178</td>\n",
              "<td>0.9151515</td>\n",
              "<td>0.7982196</td>\n",
              "<td>0.8392857</td>\n",
              "<td>0.8060606</td>\n",
              "<td>0.7658863</td>\n",
              "<td>0.7828947</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.4312811</td>\n",
              "<td>0.0130248</td>\n",
              "<td>0.4307985</td>\n",
              "<td>0.4187050</td>\n",
              "<td>0.4263539</td>\n",
              "<td>0.4215915</td>\n",
              "<td>0.4472959</td>\n",
              "<td>0.4459462</td>\n",
              "<td>0.408706</td>\n",
              "<td>0.4373076</td>\n",
              "<td>0.4296555</td>\n",
              "<td>0.4464506</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.6531956</td>\n",
              "<td>0.0999643</td>\n",
              "<td>0.7307692</td>\n",
              "<td>0.7490119</td>\n",
              "<td>0.7600806</td>\n",
              "<td>0.6358268</td>\n",
              "<td>0.4106383</td>\n",
              "<td>0.6241901</td>\n",
              "<td>0.6637931</td>\n",
              "<td>0.6340426</td>\n",
              "<td>0.6986028</td>\n",
              "<td>0.625</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-13.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-13 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-13 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-13 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table th,\n",
              "#h2o-table-13 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-13 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-13\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Scoring History: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>timestamp</th>\n",
              "<th>duration</th>\n",
              "<th>number_of_trees</th>\n",
              "<th>training_rmse</th>\n",
              "<th>training_logloss</th>\n",
              "<th>training_auc</th>\n",
              "<th>training_pr_auc</th>\n",
              "<th>training_lift</th>\n",
              "<th>training_classification_error</th>\n",
              "<th>validation_rmse</th>\n",
              "<th>validation_logloss</th>\n",
              "<th>validation_auc</th>\n",
              "<th>validation_pr_auc</th>\n",
              "<th>validation_lift</th>\n",
              "<th>validation_classification_error</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>2023-07-05 16:26:33</td>\n",
              "<td>23.379 sec</td>\n",
              "<td>0.0</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3915000</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6085</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3915000</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6085</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-05 16:26:34</td>\n",
              "<td>23.694 sec</td>\n",
              "<td>5.0</td>\n",
              "<td>0.3871426</td>\n",
              "<td>0.4694493</td>\n",
              "<td>0.8686003</td>\n",
              "<td>0.8157606</td>\n",
              "<td>2.5223499</td>\n",
              "<td>0.211</td>\n",
              "<td>0.4205830</td>\n",
              "<td>0.5323082</td>\n",
              "<td>0.7970855</td>\n",
              "<td>0.7189038</td>\n",
              "<td>2.1711367</td>\n",
              "<td>0.2715</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-05 16:26:34</td>\n",
              "<td>23.931 sec</td>\n",
              "<td>10.0</td>\n",
              "<td>0.3599499</td>\n",
              "<td>0.4107030</td>\n",
              "<td>0.8967256</td>\n",
              "<td>0.8539203</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.182</td>\n",
              "<td>0.4204562</td>\n",
              "<td>0.5315744</td>\n",
              "<td>0.7955124</td>\n",
              "<td>0.7229683</td>\n",
              "<td>2.4265645</td>\n",
              "<td>0.2735</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-05 16:26:34</td>\n",
              "<td>24.226 sec</td>\n",
              "<td>15.0</td>\n",
              "<td>0.3408629</td>\n",
              "<td>0.3748717</td>\n",
              "<td>0.9173308</td>\n",
              "<td>0.8814513</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.15525</td>\n",
              "<td>0.4256850</td>\n",
              "<td>0.5460932</td>\n",
              "<td>0.7883968</td>\n",
              "<td>0.7171523</td>\n",
              "<td>2.4265645</td>\n",
              "<td>0.3105</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-05 16:26:34</td>\n",
              "<td>24.533 sec</td>\n",
              "<td>20.0</td>\n",
              "<td>0.3228023</td>\n",
              "<td>0.3436028</td>\n",
              "<td>0.9346762</td>\n",
              "<td>0.9068350</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.135625</td>\n",
              "<td>0.4289042</td>\n",
              "<td>0.5560917</td>\n",
              "<td>0.7849017</td>\n",
              "<td>0.7144570</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.2845</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-05 16:26:35</td>\n",
              "<td>24.855 sec</td>\n",
              "<td>25.0</td>\n",
              "<td>0.3070198</td>\n",
              "<td>0.3181104</td>\n",
              "<td>0.9478527</td>\n",
              "<td>0.9247560</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.118625</td>\n",
              "<td>0.4319605</td>\n",
              "<td>0.5645589</td>\n",
              "<td>0.7817372</td>\n",
              "<td>0.7128463</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.2935</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-05 16:26:35</td>\n",
              "<td>25.134 sec</td>\n",
              "<td>30.0</td>\n",
              "<td>0.2937282</td>\n",
              "<td>0.2961196</td>\n",
              "<td>0.9574856</td>\n",
              "<td>0.9379441</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.10725</td>\n",
              "<td>0.4340509</td>\n",
              "<td>0.5694655</td>\n",
              "<td>0.7805787</td>\n",
              "<td>0.7132335</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.321</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-14.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-14 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-14 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-14 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table th,\n",
              "#h2o-table-14 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-14 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-14\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Variable Importances: </caption>\n",
              "    <thead><tr><th>variable</th>\n",
              "<th>relative_importance</th>\n",
              "<th>scaled_importance</th>\n",
              "<th>percentage</th></tr></thead>\n",
              "    <tbody><tr><td>area_se</td>\n",
              "<td>1904.8582764</td>\n",
              "<td>1.0</td>\n",
              "<td>0.1771016</td></tr>\n",
              "<tr><td>perimeter_worst</td>\n",
              "<td>670.3985596</td>\n",
              "<td>0.3519414</td>\n",
              "<td>0.0623294</td></tr>\n",
              "<tr><td>smoothness_worst</td>\n",
              "<td>490.8770752</td>\n",
              "<td>0.2576974</td>\n",
              "<td>0.0456386</td></tr>\n",
              "<tr><td>texture_worst</td>\n",
              "<td>471.0637817</td>\n",
              "<td>0.2472960</td>\n",
              "<td>0.0437965</td></tr>\n",
              "<tr><td>radius_se</td>\n",
              "<td>406.7816772</td>\n",
              "<td>0.2135496</td>\n",
              "<td>0.0378200</td></tr>\n",
              "<tr><td>symmetry_se</td>\n",
              "<td>383.1606445</td>\n",
              "<td>0.2011492</td>\n",
              "<td>0.0356238</td></tr>\n",
              "<tr><td>area_worst</td>\n",
              "<td>379.5228882</td>\n",
              "<td>0.1992394</td>\n",
              "<td>0.0352856</td></tr>\n",
              "<tr><td>texture_se</td>\n",
              "<td>376.7027893</td>\n",
              "<td>0.1977590</td>\n",
              "<td>0.0350234</td></tr>\n",
              "<tr><td>radius_worst</td>\n",
              "<td>356.6051025</td>\n",
              "<td>0.1872082</td>\n",
              "<td>0.0331549</td></tr>\n",
              "<tr><td>symmetry_mean</td>\n",
              "<td>341.8689880</td>\n",
              "<td>0.1794721</td>\n",
              "<td>0.0317848</td></tr>\n",
              "<tr><td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td></tr>\n",
              "<tr><td>compactness_mean</td>\n",
              "<td>257.6020813</td>\n",
              "<td>0.1352343</td>\n",
              "<td>0.0239502</td></tr>\n",
              "<tr><td>smoothness_se</td>\n",
              "<td>247.9092102</td>\n",
              "<td>0.1301458</td>\n",
              "<td>0.0230490</td></tr>\n",
              "<tr><td>fractal_dimension_worst</td>\n",
              "<td>239.3760834</td>\n",
              "<td>0.1256661</td>\n",
              "<td>0.0222557</td></tr>\n",
              "<tr><td>perimeter_mean</td>\n",
              "<td>235.8315887</td>\n",
              "<td>0.1238053</td>\n",
              "<td>0.0219261</td></tr>\n",
              "<tr><td>fractal_dimension_mean</td>\n",
              "<td>229.6742096</td>\n",
              "<td>0.1205729</td>\n",
              "<td>0.0213537</td></tr>\n",
              "<tr><td>compactness_worst</td>\n",
              "<td>214.6832581</td>\n",
              "<td>0.1127030</td>\n",
              "<td>0.0199599</td></tr>\n",
              "<tr><td>concave points_mean</td>\n",
              "<td>202.2340851</td>\n",
              "<td>0.1061675</td>\n",
              "<td>0.0188024</td></tr>\n",
              "<tr><td>radius_mean</td>\n",
              "<td>184.1133881</td>\n",
              "<td>0.0966546</td>\n",
              "<td>0.0171177</td></tr>\n",
              "<tr><td>area_mean</td>\n",
              "<td>175.9438477</td>\n",
              "<td>0.0923658</td>\n",
              "<td>0.0163581</td></tr>\n",
              "<tr><td>compactness_se</td>\n",
              "<td>169.7982025</td>\n",
              "<td>0.0891395</td>\n",
              "<td>0.0157868</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "<pre style='font-size: smaller; margin-bottom: 1em;'>[30 rows x 4 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.model_performance(train).accuracy()"
      ],
      "metadata": {
        "id": "G_PyYFNMs97_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71452787-e653-45f2-f81c-a2329b9c9dcd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.44304026911656064, 0.894625]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "HATr  = best_model.model_performance(train)\n",
        "HATe  = best_model.model_performance(valid)"
      ],
      "metadata": {
        "id": "jKquKjVVJBL9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_h2o = pd.DataFrame(h2o.as_list(best_model.predict(valid)))['predict']\n",
        "y_test_h2o = np.array(valid1['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-caBWGPlp8P",
        "outputId": "447bfbe5-33fa-445e-909e-42fe7e6f785c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SFOLD DATA AUTOML\n",
        "#strain, svalid = shdf.split_frame(ratios=[.8], seed=123)\n",
        "shdf  = newdata.copy()\n",
        "#shdf['y_test'] = shdf['y_test'].replace(0,\"B\")\n",
        "#shdf['y_test'] = shdf['y_test'].replace(1,\"M\")\n",
        "shy = \"y_test\"\n",
        "shx = list(shdf.columns)\n",
        "shx.remove(shy)\n",
        "\n",
        "shdf.iloc[:,0:6] = StandardScaler().fit_transform(shdf.iloc[:,0:6])\n",
        "#shdf.iloc[:,-1] = LabelEncoder().fit_transform(shdf.iloc[:,-1])\n",
        "strain1, svalid1 = train_test_split(shdf, test_size=0.2,random_state=123)\n",
        "strain = h2o.H2OFrame(strain1)\n",
        "svalid = h2o.H2OFrame(svalid1)\n",
        "strain[\"y_test\"] = strain[\"y_test\"].asfactor()\n",
        "svalid[\"y_test\"] = svalid[\"y_test\"].asfactor()\n",
        "\n",
        "st = time.time()\n",
        "\n",
        "saml = H2OAutoML(include_algos = ['DeepLearning'], max_models = 10, seed = 123, verbosity=\"info\", nfolds=10, sort_metric='accuracy')\n",
        "\n",
        "saml.train(x = shx, y = shy, training_frame = strain, validation_frame = svalid)\n",
        "sautoend = time.time() - st\n",
        "sbest_model = saml.get_best_model()\n",
        "sHATr  = sbest_model.model_performance(strain)\n",
        "sHATe  = sbest_model.model_performance(svalid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMDqylVK7svS",
        "outputId": "ab4af6b1-27e6-4c65-bde6-b7a4a8d753c7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "AutoML progress: |\n",
            "16:30:55.416: Project: AutoML_2_20230705_163055\n",
            "16:30:55.416: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "16:30:55.417: Setting stopping tolerance adaptively based on the training frame: 0.011180339887498949\n",
            "16:30:55.417: Build control seed: 123\n",
            "16:30:55.417: training frame: Frame key: AutoML_2_20230705_163055_training_py_11_sid_956a    cols: 7    rows: 8000  chunks: 1    size: 98174  checksum: -2054600783660545056\n",
            "16:30:55.417: validation frame: Frame key: py_12_sid_956a    cols: 7    rows: 2000  chunks: 1    size: 25424  checksum: 8020978961634356592\n",
            "16:30:55.417: leaderboard frame: NULL\n",
            "16:30:55.417: blending frame: NULL\n",
            "16:30:55.417: response column: y_test\n",
            "16:30:55.417: fold column: null\n",
            "16:30:55.417: weights column: null\n",
            "16:30:55.418: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "16:30:55.419: Disabling Algo: XGBoost as requested by the user.\n",
            "16:30:55.419: Disabling Algo: DRF as requested by the user.\n",
            "16:30:55.419: Disabling Algo: GBM as requested by the user.\n",
            "16:30:55.419: Disabling Algo: GLM as requested by the user.\n",
            "16:30:55.419: Disabling Algo: StackedEnsemble as requested by the user.\n",
            "16:30:55.419: AutoML job created: 2023.07.05 16:30:55.415\n",
            "16:30:55.420: AutoML build started: 2023.07.05 16:30:55.420\n",
            "16:30:55.438: AutoML: starting DeepLearning_1_AutoML_2_20230705_163055 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆ\n",
            "16:31:07.278: New leader: DeepLearning_1_AutoML_2_20230705_163055, accuracy: 0.754\n",
            "16:31:07.282: AutoML: starting DeepLearning_grid_1_AutoML_2_20230705_163055 hyperparameter search\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "16:42:01.864: AutoML: starting DeepLearning_grid_2_AutoML_2_20230705_163055 hyperparameter search\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "17:03:15.463: AutoML: starting DeepLearning_grid_3_AutoML_2_20230705_163055 hyperparameter search\n",
            "\n",
            "â–ˆ| (done) 100%\n",
            "\n",
            "17:35:35.579: Skipping StackedEnsemble 'monotonic' due to the exclude_algos option or it is already trained.\n",
            "17:35:35.580: Skipping StackedEnsemble 'best_of_family_xglm' due to the exclude_algos option or it is already trained.\n",
            "17:35:35.585: Skipping StackedEnsemble 'all_xglm' due to the exclude_algos option or it is already trained.\n",
            "17:35:35.585: Actual modeling steps: [{DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}]\n",
            "17:35:35.585: AutoML build stopped: 2023.07.05 17:35:35.585\n",
            "17:35:35.585: AutoML build done: built 10 models\n",
            "17:35:35.585: AutoML duration:  1:04:40.165\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_sh2o = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))['predict']\n",
        "y_test_sh2o = np.array(svalid1['y_test']).copy()"
      ],
      "metadata": {
        "id": "ivMaaDNNmYY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "id": "nJrcI3Gn8we2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]\n",
        "\n",
        "label = [\"ArtificialNeuralNetwork\", 'DeepNeuralNetwork',\n",
        "         'KNearestNeighborsClassifier', 'LogisticRegression',\n",
        "         'RandomForestClassifier', 'SupportVectorClassifier',\n",
        "         'XGBoost', type(best_model).__name__, type(sbest_model).__name__ ]\n",
        "\n",
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"DNN\":[DTr,DTe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"H_OD\":[HATr.accuracy()[0][1],HATe.accuracy()[0][1]],\n",
        "    \"H_SOD\":[sHATr.accuracy()[0][1],sHATe.accuracy()[0][1]]\n",
        "    })\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc['Model'] = label\n",
        "\n",
        "acc = acc[['Model', 'train', 'test']]\n",
        "acc['avg'] = round((acc['train'] + acc['test'])/2, 6)\n",
        "acc[acc[\"avg\"] == acc[\"avg\"].max()]\n",
        "acc['BestModel'] = 0\n",
        "for i in range(len(acc)):\n",
        "  if acc['avg'][i] >= 90 and acc['avg'][i] < acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"good\"\n",
        "  elif acc['avg'][i] == acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"best\"\n",
        "  else:\n",
        "    acc.iloc[i,-1] = \"not good\"\n",
        "\n",
        "acc[\"Precision\"] = np.zeros(len(acc))\n",
        "acc[\"Recall\"]    = np.zeros(len(acc))\n",
        "acc[\"F1_Score\"]  = np.zeros(len(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ba6kRO-eI60S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ANNn = []\n",
        "y_pred_DNNn = []\n",
        "for i in range(len(y_pred_ANN)):\n",
        "  y_pred_ANNn.append(y_pred_ANN[i][0])\n",
        "  y_pred_DNNn.append(y_pred_DNN[i][0])"
      ],
      "metadata": {
        "id": "S7pf8G6ao4oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [np.array(y_pred_ANNn), np.array(y_pred_DNNn), y_pred_knn,\n",
        "        y_pred_lr, y_pred_rf,\n",
        "        y_pred_svm, y_pred_xgb, y_pred_h2o, y_pred_sh2o]\n",
        "\n",
        "tes  = [y_test_indi_ML, np.array(Dy_test), y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_indi_ML, y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_h2o.copy(), y_test_sh2o.copy()]"
      ],
      "metadata": {
        "id": "D2n2VnW1jKp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pred)):\n",
        "  p,r,f,_ = precision_recall_fscore_support(tes[i], pred[i],\n",
        "                                            average='macro')\n",
        "  acc.iloc[i,5]= p\n",
        "  acc.iloc[i,6]= r\n",
        "  acc.iloc[i,7]= f\n",
        "  p = 0\n",
        "  r = 0\n",
        "  f = 0\n",
        "acc"
      ],
      "metadata": {
        "id": "2DlSQ29moN9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "id": "ru4oH6-YzGPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve"
      ],
      "metadata": {
        "id": "Qma6hWrmDBvf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]"
      ],
      "metadata": {
        "id": "03rwpiYpoAGs"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()"
      ],
      "metadata": {
        "id": "x5wrzn-qPaXw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "C80ESe__xB5x"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
        "\n",
        "y_pred = ANNmodel.predict(X_test).ravel()\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[0]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = model.predict(DX_test).ravel()\n",
        "y_test = Dy_test.copy()\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[1]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = knn.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[2]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = lr.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[3]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = rf.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[4]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = svm.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[5]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = xgb.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[6]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(best_model.predict(valid)))\n",
        "y_test = h2o.as_list(valid['diagnosis'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(best_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))\n",
        "y_test = h2o.as_list(svalid['y_test'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(sbest_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('AUC ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "4Bi4CMUJm6yH",
        "outputId": "bee1f9af-c681-4e78-8dae-93d59c319f01"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 2ms/step\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "xgboost prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "deeplearning prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tm = [aend,dend,kend,lend,rend,send,xend,autoend,sautoend]"
      ],
      "metadata": {
        "id": "6vUubKKMxLIP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['time'] = 0"
      ],
      "metadata": {
        "id": "_1wKoAe9wIPn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['time'] = tm"
      ],
      "metadata": {
        "id": "P-K86caJxhvX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Kl5SnYY3R1S4",
        "outputId": "1227eec0-cc8d-4544-b9ae-eca124737281"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Model     train    test       avg BestModel  \\\n",
              "ANN        ArtificialNeuralNetwork  0.767500  0.7525  0.760000  not good   \n",
              "DNN              DeepNeuralNetwork  0.760750  0.7530  0.756875  not good   \n",
              "KNN    KNearestNeighborsClassifier  0.772250  0.7340  0.753125  not good   \n",
              "LR              LogisticRegression  0.761500  0.7565  0.759000  not good   \n",
              "RF          RandomForestClassifier  0.751750  0.7565  0.754125  not good   \n",
              "SVM        SupportVectorClassifier  0.755750  0.7555  0.755625  not good   \n",
              "XGB                        XGBoost  0.775375  0.7555  0.765438  not good   \n",
              "H_OD           H2OXGBoostEstimator  0.894625  0.7385  0.816563      best   \n",
              "H_SOD     H2ODeepLearningEstimator  0.759500  0.7570  0.758250  not good   \n",
              "\n",
              "       Precision    Recall  F1_Score         time  \n",
              "ANN     0.743177  0.727403  0.732490     6.263754  \n",
              "DNN     0.747604  0.722121  0.728750   203.432534  \n",
              "KNN     0.723671  0.705142  0.710304     0.001664  \n",
              "LR      0.752195  0.725225  0.732152     0.189482  \n",
              "RF      0.748737  0.729779  0.735544     1.145092  \n",
              "SVM     0.752952  0.722126  0.729413    18.329859  \n",
              "XGB     0.747051  0.729641  0.735101    12.569428  \n",
              "H_OD    0.691755  0.698796  0.676574   282.127125  \n",
              "H_SOD   0.724541  0.734014  0.725831  3881.554655  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ada9d7f-87de-4f03-b183-e4fdfff223f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>avg</th>\n",
              "      <th>BestModel</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_Score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>ArtificialNeuralNetwork</td>\n",
              "      <td>0.767500</td>\n",
              "      <td>0.7525</td>\n",
              "      <td>0.760000</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.743177</td>\n",
              "      <td>0.727403</td>\n",
              "      <td>0.732490</td>\n",
              "      <td>6.263754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>DeepNeuralNetwork</td>\n",
              "      <td>0.760750</td>\n",
              "      <td>0.7530</td>\n",
              "      <td>0.756875</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.747604</td>\n",
              "      <td>0.722121</td>\n",
              "      <td>0.728750</td>\n",
              "      <td>203.432534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>KNearestNeighborsClassifier</td>\n",
              "      <td>0.772250</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.753125</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.723671</td>\n",
              "      <td>0.705142</td>\n",
              "      <td>0.710304</td>\n",
              "      <td>0.001664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.7565</td>\n",
              "      <td>0.759000</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.752195</td>\n",
              "      <td>0.725225</td>\n",
              "      <td>0.732152</td>\n",
              "      <td>0.189482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.751750</td>\n",
              "      <td>0.7565</td>\n",
              "      <td>0.754125</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.748737</td>\n",
              "      <td>0.729779</td>\n",
              "      <td>0.735544</td>\n",
              "      <td>1.145092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>SupportVectorClassifier</td>\n",
              "      <td>0.755750</td>\n",
              "      <td>0.7555</td>\n",
              "      <td>0.755625</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.752952</td>\n",
              "      <td>0.722126</td>\n",
              "      <td>0.729413</td>\n",
              "      <td>18.329859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.775375</td>\n",
              "      <td>0.7555</td>\n",
              "      <td>0.765438</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.747051</td>\n",
              "      <td>0.729641</td>\n",
              "      <td>0.735101</td>\n",
              "      <td>12.569428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_OD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.7385</td>\n",
              "      <td>0.816563</td>\n",
              "      <td>best</td>\n",
              "      <td>0.691755</td>\n",
              "      <td>0.698796</td>\n",
              "      <td>0.676574</td>\n",
              "      <td>282.127125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_SOD</th>\n",
              "      <td>H2ODeepLearningEstimator</td>\n",
              "      <td>0.759500</td>\n",
              "      <td>0.7570</td>\n",
              "      <td>0.758250</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.724541</td>\n",
              "      <td>0.734014</td>\n",
              "      <td>0.725831</td>\n",
              "      <td>3881.554655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ada9d7f-87de-4f03-b183-e4fdfff223f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ada9d7f-87de-4f03-b183-e4fdfff223f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ada9d7f-87de-4f03-b183-e4fdfff223f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}