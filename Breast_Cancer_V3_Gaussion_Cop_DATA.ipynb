{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashiqurrahmankhan21st/BreastCancer/blob/main/Breast_Cancer_V3_Gaussion_Cop_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EUMYU_d6_J-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "#import keras_metrics\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#original data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/GC_SDV_BreastCancer.csv\")\n",
        "del df['Unnamed: 0']\n",
        "df"
      ],
      "metadata": {
        "id": "Ntdqq4T_aGXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "fede362e-4292-45ae-c4a3-2a41c5c4627b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0            B       12.759         17.02           80.93      509.4   \n",
              "1            M       11.306         19.79           74.48      390.1   \n",
              "2            M       12.325         24.63           81.38      484.2   \n",
              "3            B       10.635         24.89           68.67      341.9   \n",
              "4            M       28.110         29.74          188.50     2286.8   \n",
              "...        ...          ...           ...             ...        ...   \n",
              "9995         M       13.590         25.97           88.89      597.2   \n",
              "9996         B       13.292         19.63           86.72      554.2   \n",
              "9997         B        9.346         22.18           60.79      275.9   \n",
              "9998         B       10.727         19.67           68.48      352.5   \n",
              "9999         B       11.389         15.72           71.71      418.7   \n",
              "\n",
              "      smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0             0.06687           0.03913        0.025046             0.016961   \n",
              "1             0.10894           0.10373        0.114211             0.053879   \n",
              "2             0.11996           0.16272        0.067551             0.047048   \n",
              "3             0.08418           0.08830        0.027259             0.006019   \n",
              "4             0.06442           0.06746        0.191878             0.115942   \n",
              "...               ...               ...             ...                  ...   \n",
              "9995          0.09826           0.10244        0.048636             0.036183   \n",
              "9996          0.09190           0.13122        0.059075             0.031142   \n",
              "9997          0.10120           0.09687        0.040308             0.022831   \n",
              "9998          0.08418           0.07468        0.078046             0.030488   \n",
              "9999          0.08828           0.04033        0.008568             0.006621   \n",
              "\n",
              "      symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
              "0            0.1493  ...        13.858          23.49            88.06   \n",
              "1            0.2135  ...        13.389          27.58            90.55   \n",
              "2            0.2181  ...        13.615          31.28            92.97   \n",
              "3            0.1938  ...        11.340          29.58            75.24   \n",
              "4            0.1850  ...        36.040          36.01           232.15   \n",
              "...             ...  ...           ...            ...              ...   \n",
              "9995         0.1586  ...        14.961          34.97            96.72   \n",
              "9996         0.1773  ...        16.561          27.23           109.80   \n",
              "9997         0.1485  ...        10.957          34.94            71.87   \n",
              "9998         0.1610  ...        11.669          30.02            75.03   \n",
              "9999         0.1473  ...        12.433          17.77            78.42   \n",
              "\n",
              "      area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "0          590.4           0.12370            0.13034         0.145731   \n",
              "1          548.5           0.14599            0.26635         0.364920   \n",
              "2          611.2           0.14752            0.37139         0.172141   \n",
              "3          397.9           0.13029            0.15898         0.044588   \n",
              "4         3615.1           0.11879            0.19572         0.576466   \n",
              "...          ...               ...                ...              ...   \n",
              "9995       719.5           0.14368            0.23676         0.144657   \n",
              "9996       875.1           0.13810            0.30831         0.222822   \n",
              "9997       362.1           0.14212            0.20370         0.096103   \n",
              "9998       408.7           0.13676            0.18993         0.211117   \n",
              "9999       486.1           0.12812            0.06561         0.037096   \n",
              "\n",
              "      concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
              "0                 0.075146          0.2831                  0.07432  \n",
              "1                 0.154583          0.4364                  0.10487  \n",
              "2                 0.110892          0.3186                  0.10515  \n",
              "3                 0.052535          0.2564                  0.08163  \n",
              "4                 0.216770          0.4201                  0.06176  \n",
              "...                    ...             ...                      ...  \n",
              "9995              0.096611          0.2627                  0.08677  \n",
              "9996              0.105450          0.3257                  0.08708  \n",
              "9997              0.080021          0.2338                  0.10824  \n",
              "9998              0.100522          0.2035                  0.08088  \n",
              "9999              0.028837          0.1847                  0.05774  \n",
              "\n",
              "[10000 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e935c780-5ba4-4aff-a85f-0e5c99ca5c65\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B</td>\n",
              "      <td>12.759</td>\n",
              "      <td>17.02</td>\n",
              "      <td>80.93</td>\n",
              "      <td>509.4</td>\n",
              "      <td>0.06687</td>\n",
              "      <td>0.03913</td>\n",
              "      <td>0.025046</td>\n",
              "      <td>0.016961</td>\n",
              "      <td>0.1493</td>\n",
              "      <td>...</td>\n",
              "      <td>13.858</td>\n",
              "      <td>23.49</td>\n",
              "      <td>88.06</td>\n",
              "      <td>590.4</td>\n",
              "      <td>0.12370</td>\n",
              "      <td>0.13034</td>\n",
              "      <td>0.145731</td>\n",
              "      <td>0.075146</td>\n",
              "      <td>0.2831</td>\n",
              "      <td>0.07432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>11.306</td>\n",
              "      <td>19.79</td>\n",
              "      <td>74.48</td>\n",
              "      <td>390.1</td>\n",
              "      <td>0.10894</td>\n",
              "      <td>0.10373</td>\n",
              "      <td>0.114211</td>\n",
              "      <td>0.053879</td>\n",
              "      <td>0.2135</td>\n",
              "      <td>...</td>\n",
              "      <td>13.389</td>\n",
              "      <td>27.58</td>\n",
              "      <td>90.55</td>\n",
              "      <td>548.5</td>\n",
              "      <td>0.14599</td>\n",
              "      <td>0.26635</td>\n",
              "      <td>0.364920</td>\n",
              "      <td>0.154583</td>\n",
              "      <td>0.4364</td>\n",
              "      <td>0.10487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>12.325</td>\n",
              "      <td>24.63</td>\n",
              "      <td>81.38</td>\n",
              "      <td>484.2</td>\n",
              "      <td>0.11996</td>\n",
              "      <td>0.16272</td>\n",
              "      <td>0.067551</td>\n",
              "      <td>0.047048</td>\n",
              "      <td>0.2181</td>\n",
              "      <td>...</td>\n",
              "      <td>13.615</td>\n",
              "      <td>31.28</td>\n",
              "      <td>92.97</td>\n",
              "      <td>611.2</td>\n",
              "      <td>0.14752</td>\n",
              "      <td>0.37139</td>\n",
              "      <td>0.172141</td>\n",
              "      <td>0.110892</td>\n",
              "      <td>0.3186</td>\n",
              "      <td>0.10515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B</td>\n",
              "      <td>10.635</td>\n",
              "      <td>24.89</td>\n",
              "      <td>68.67</td>\n",
              "      <td>341.9</td>\n",
              "      <td>0.08418</td>\n",
              "      <td>0.08830</td>\n",
              "      <td>0.027259</td>\n",
              "      <td>0.006019</td>\n",
              "      <td>0.1938</td>\n",
              "      <td>...</td>\n",
              "      <td>11.340</td>\n",
              "      <td>29.58</td>\n",
              "      <td>75.24</td>\n",
              "      <td>397.9</td>\n",
              "      <td>0.13029</td>\n",
              "      <td>0.15898</td>\n",
              "      <td>0.044588</td>\n",
              "      <td>0.052535</td>\n",
              "      <td>0.2564</td>\n",
              "      <td>0.08163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>28.110</td>\n",
              "      <td>29.74</td>\n",
              "      <td>188.50</td>\n",
              "      <td>2286.8</td>\n",
              "      <td>0.06442</td>\n",
              "      <td>0.06746</td>\n",
              "      <td>0.191878</td>\n",
              "      <td>0.115942</td>\n",
              "      <td>0.1850</td>\n",
              "      <td>...</td>\n",
              "      <td>36.040</td>\n",
              "      <td>36.01</td>\n",
              "      <td>232.15</td>\n",
              "      <td>3615.1</td>\n",
              "      <td>0.11879</td>\n",
              "      <td>0.19572</td>\n",
              "      <td>0.576466</td>\n",
              "      <td>0.216770</td>\n",
              "      <td>0.4201</td>\n",
              "      <td>0.06176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>M</td>\n",
              "      <td>13.590</td>\n",
              "      <td>25.97</td>\n",
              "      <td>88.89</td>\n",
              "      <td>597.2</td>\n",
              "      <td>0.09826</td>\n",
              "      <td>0.10244</td>\n",
              "      <td>0.048636</td>\n",
              "      <td>0.036183</td>\n",
              "      <td>0.1586</td>\n",
              "      <td>...</td>\n",
              "      <td>14.961</td>\n",
              "      <td>34.97</td>\n",
              "      <td>96.72</td>\n",
              "      <td>719.5</td>\n",
              "      <td>0.14368</td>\n",
              "      <td>0.23676</td>\n",
              "      <td>0.144657</td>\n",
              "      <td>0.096611</td>\n",
              "      <td>0.2627</td>\n",
              "      <td>0.08677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>B</td>\n",
              "      <td>13.292</td>\n",
              "      <td>19.63</td>\n",
              "      <td>86.72</td>\n",
              "      <td>554.2</td>\n",
              "      <td>0.09190</td>\n",
              "      <td>0.13122</td>\n",
              "      <td>0.059075</td>\n",
              "      <td>0.031142</td>\n",
              "      <td>0.1773</td>\n",
              "      <td>...</td>\n",
              "      <td>16.561</td>\n",
              "      <td>27.23</td>\n",
              "      <td>109.80</td>\n",
              "      <td>875.1</td>\n",
              "      <td>0.13810</td>\n",
              "      <td>0.30831</td>\n",
              "      <td>0.222822</td>\n",
              "      <td>0.105450</td>\n",
              "      <td>0.3257</td>\n",
              "      <td>0.08708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>B</td>\n",
              "      <td>9.346</td>\n",
              "      <td>22.18</td>\n",
              "      <td>60.79</td>\n",
              "      <td>275.9</td>\n",
              "      <td>0.10120</td>\n",
              "      <td>0.09687</td>\n",
              "      <td>0.040308</td>\n",
              "      <td>0.022831</td>\n",
              "      <td>0.1485</td>\n",
              "      <td>...</td>\n",
              "      <td>10.957</td>\n",
              "      <td>34.94</td>\n",
              "      <td>71.87</td>\n",
              "      <td>362.1</td>\n",
              "      <td>0.14212</td>\n",
              "      <td>0.20370</td>\n",
              "      <td>0.096103</td>\n",
              "      <td>0.080021</td>\n",
              "      <td>0.2338</td>\n",
              "      <td>0.10824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>B</td>\n",
              "      <td>10.727</td>\n",
              "      <td>19.67</td>\n",
              "      <td>68.48</td>\n",
              "      <td>352.5</td>\n",
              "      <td>0.08418</td>\n",
              "      <td>0.07468</td>\n",
              "      <td>0.078046</td>\n",
              "      <td>0.030488</td>\n",
              "      <td>0.1610</td>\n",
              "      <td>...</td>\n",
              "      <td>11.669</td>\n",
              "      <td>30.02</td>\n",
              "      <td>75.03</td>\n",
              "      <td>408.7</td>\n",
              "      <td>0.13676</td>\n",
              "      <td>0.18993</td>\n",
              "      <td>0.211117</td>\n",
              "      <td>0.100522</td>\n",
              "      <td>0.2035</td>\n",
              "      <td>0.08088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>B</td>\n",
              "      <td>11.389</td>\n",
              "      <td>15.72</td>\n",
              "      <td>71.71</td>\n",
              "      <td>418.7</td>\n",
              "      <td>0.08828</td>\n",
              "      <td>0.04033</td>\n",
              "      <td>0.008568</td>\n",
              "      <td>0.006621</td>\n",
              "      <td>0.1473</td>\n",
              "      <td>...</td>\n",
              "      <td>12.433</td>\n",
              "      <td>17.77</td>\n",
              "      <td>78.42</td>\n",
              "      <td>486.1</td>\n",
              "      <td>0.12812</td>\n",
              "      <td>0.06561</td>\n",
              "      <td>0.037096</td>\n",
              "      <td>0.028837</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.05774</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e935c780-5ba4-4aff-a85f-0e5c99ca5c65')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e935c780-5ba4-4aff-a85f-0e5c99ca5c65 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e935c780-5ba4-4aff-a85f-0e5c99ca5c65');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()"
      ],
      "metadata": {
        "id": "4cimS259ti6F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diagnosis'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv9sG-P8M1Fe",
        "outputId": "df6ddf45-f706-468d-95a2-40404d650535"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    6085\n",
              "M    3915\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()\n",
        "print(\"Original data : \",df.shape)\n",
        "print(\"tarin         : \",X_train.shape)\n",
        "print(\"test          : \",X_test.shape[0])\n",
        "#print(\"validation    : \",X_val.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpyVwv13CTut",
        "outputId": "b0661b2a-32c0-44d5-e664-2c81ff22e48e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data :  (10000, 31)\n",
            "tarin         :  (8000, 30)\n",
            "test          :  2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "svm = SVC(C=0.1, gamma='auto', kernel = 'rbf',probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "STr = svm.score(X_train, y_train)\n",
        "STe = svm.score(X_test, y_test)\n",
        "y_pred_svm = svm.predict(X_test)"
      ],
      "metadata": {
        "id": "wnRYFOkyEEZ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANN\n",
        "tf.random.set_seed(123)\n",
        "ANNmodel = Sequential()\n",
        "ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "ANNmodel.add(Dense(10, activation='relu'))\n",
        "ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "ATr = ANNmodel.evaluate(X_train,y_train,verbose=0)[1]\n",
        "ATe = ANNmodel.evaluate(X_test,y_test,verbose=0)[1]\n",
        "y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZX7DbN-OIyu",
        "outputId": "3ccbb33d-0500-4d0a-fc7c-f6506e25cb57"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 8ms/step - loss: 0.6320 - accuracy: 0.6399 - val_loss: 0.5355 - val_accuracy: 0.7638\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5231 - accuracy: 0.7436 - val_loss: 0.4975 - val_accuracy: 0.7650\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7461 - val_loss: 0.4933 - val_accuracy: 0.7650\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7485 - val_loss: 0.4916 - val_accuracy: 0.7613\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7508 - val_loss: 0.4900 - val_accuracy: 0.7563\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7551 - val_loss: 0.4888 - val_accuracy: 0.7550\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7553 - val_loss: 0.4891 - val_accuracy: 0.7563\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7576 - val_loss: 0.4883 - val_accuracy: 0.7525\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.7572 - val_loss: 0.4874 - val_accuracy: 0.7588\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7585 - val_loss: 0.4869 - val_accuracy: 0.7613\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.7582 - val_loss: 0.4868 - val_accuracy: 0.7600\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7600 - val_loss: 0.4861 - val_accuracy: 0.7600\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.7615 - val_loss: 0.4853 - val_accuracy: 0.7625\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7607 - val_loss: 0.4847 - val_accuracy: 0.7613\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.7608 - val_loss: 0.4841 - val_accuracy: 0.7600\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7636 - val_loss: 0.4843 - val_accuracy: 0.7638\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.7613 - val_loss: 0.4842 - val_accuracy: 0.7713\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.7626 - val_loss: 0.4830 - val_accuracy: 0.7613\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.7632 - val_loss: 0.4834 - val_accuracy: 0.7675\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.7629 - val_loss: 0.4829 - val_accuracy: 0.7663\n",
            "63/63 [==============================] - 0s 743us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "params = {\n",
        "            'objective':'binary:logistic',\n",
        "            'max_depth': 7,\n",
        "            'alpha': 10,\n",
        "            'learning_rate': 1,\n",
        "            'n_estimators':100\n",
        "        }\n",
        "xgb = XGBClassifier(objective='binary:logistic',max_depth= 7,alpha= 10,learning_rate= 1,n_estimators=100)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "XTr = accuracy_score(y_train, xgb.predict(X_train))\n",
        "XTe = accuracy_score(y_test, xgb.predict(X_test))\n",
        "XTr,XTe"
      ],
      "metadata": {
        "id": "PCP82YZ9RjgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7fa8bf-3280-4311-d32f-c2e3e659c28d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.983125, 0.724)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "# Define the range of n_neighbors values to test\n",
        "n_neighbors_values = [1,3, 5, 7, 9, 11]\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_n_neighbors = None\n",
        "\n",
        "for n_neighbors in n_neighbors_values:\n",
        "    print(\"Number of Neighbors:\", n_neighbors)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, knn.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "    print('KNN model train accuracy score: {0:0.4f}'.format(train_accuracy))\n",
        "    print('KNN model test accuracy score: {0:0.4f}'.format(test_accuracy))\n",
        "    print()\n",
        "\n",
        "    # Check if the current test accuracy is better than the previous best\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_n_neighbors = n_neighbors\n",
        "print(\"best neighbours: \", best_n_neighbors)\n",
        "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "knn.fit(X_train, y_train)\n",
        "KTr = accuracy_score(y_train, knn.predict(X_train))\n",
        "KTe = accuracy_score(y_test, knn.predict(X_test))\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "KTr,KTe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y_6b33A1imy",
        "outputId": "ec26126c-c292-4ac8-9a35-b3391f12d185"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Neighbors: 1\n",
            "KNN model train accuracy score: 1.0000\n",
            "KNN model test accuracy score: 0.6770\n",
            "\n",
            "Number of Neighbors: 3\n",
            "KNN model train accuracy score: 0.8323\n",
            "KNN model test accuracy score: 0.6955\n",
            "\n",
            "Number of Neighbors: 5\n",
            "KNN model train accuracy score: 0.7936\n",
            "KNN model test accuracy score: 0.7090\n",
            "\n",
            "Number of Neighbors: 7\n",
            "KNN model train accuracy score: 0.7846\n",
            "KNN model test accuracy score: 0.7160\n",
            "\n",
            "Number of Neighbors: 9\n",
            "KNN model train accuracy score: 0.7774\n",
            "KNN model test accuracy score: 0.7275\n",
            "\n",
            "Number of Neighbors: 11\n",
            "KNN model train accuracy score: 0.7722\n",
            "KNN model test accuracy score: 0.7340\n",
            "\n",
            "best neighbours:  11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.77225, 0.734)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RF\n",
        "rf = RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123)\n",
        "rf.fit(X_train, y_train)\n",
        "RTr = accuracy_score(y_train, rf.predict(X_train))\n",
        "RTe = accuracy_score(y_test, rf.predict(X_test))\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "RTr,RTe"
      ],
      "metadata": {
        "id": "k_kyk_E92bSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ec2813-9007-4471-fbf2-c7a73ce20926"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.75175, 0.7565)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LR\n",
        "lr = LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123)\n",
        "lr.fit(X_train, y_train)\n",
        "LTr = accuracy_score(y_train, lr.predict(X_train))\n",
        "LTe = accuracy_score(y_test, lr.predict(X_test))\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "LTr,LTe"
      ],
      "metadata": {
        "id": "OWkZmwL23Fm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99a9ac6-85d6-4cad-d15c-0facb61bff72"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7615, 0.7565)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E9_l6r5f0w5n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CVal(ML):\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/GC_SDV_BreastCancer.csv\")\n",
        "  del df['Unnamed: 0']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "    ML.fit(X_train, y_train)\n",
        "    y_pred_ML = ML.predict(X_test)\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ML)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]+1\n",
        "\n",
        "  y_pred_final = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_final.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "XqaJZ1Mb0xAe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CValANN():\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/GC_SDV_BreastCancer.csv\")\n",
        "  del df['Unnamed: 0']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "    tf.random.set_seed(123)\n",
        "    ANNmodel = Sequential()\n",
        "    ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    ANNmodel.add(Dense(10, activation='relu'))\n",
        "    ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "    ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "    ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "    y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ANN)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]\n",
        "\n",
        "  y_pred_fina = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_fina.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  y_pred_final = []\n",
        "\n",
        "  for i in range(len(y_pred_fina)):\n",
        "    y_pred_final.append(y_pred_fina[i][0])\n",
        "\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "jRa_a7EY0y6B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = pd.DataFrame({\n",
        "    \"SVM\": CVal(SVC(C=0.1, gamma='auto', kernel = 'rbf'))\n",
        "})\n",
        "newdata[\"KNN\"] = CVal(KNeighborsClassifier(n_neighbors=3))\n",
        "newdata[\"RF\"]  = CVal(RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123))\n",
        "newdata['LR']  = CVal(LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123))\n",
        "newdata[\"ANN\"] = CValANN()\n",
        "newdata[\"XGB\"] = CVal(XGBClassifier(objective='binary:logistic',max_depth= 7,alpha= 10,learning_rate= 1,n_estimators=100))\n",
        "newdata['y_test'] = encoder.fit_transform(df['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CGgdO1V0zA9",
        "outputId": "3c1113be-e6b8-4243-f249-1e9a3fea4839"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 4ms/step - loss: 0.5958 - accuracy: 0.6832 - val_loss: 0.5193 - val_accuracy: 0.7425\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7456 - val_loss: 0.5016 - val_accuracy: 0.7437\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7496 - val_loss: 0.4942 - val_accuracy: 0.7550\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.7524 - val_loss: 0.4917 - val_accuracy: 0.7588\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.7554 - val_loss: 0.4893 - val_accuracy: 0.7613\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.7568 - val_loss: 0.4888 - val_accuracy: 0.7588\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7582 - val_loss: 0.4882 - val_accuracy: 0.7600\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7603 - val_loss: 0.4877 - val_accuracy: 0.7613\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.7619 - val_loss: 0.4869 - val_accuracy: 0.7613\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7624 - val_loss: 0.4873 - val_accuracy: 0.7575\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.7632 - val_loss: 0.4877 - val_accuracy: 0.7563\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7624 - val_loss: 0.4878 - val_accuracy: 0.7563\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.7640 - val_loss: 0.4867 - val_accuracy: 0.7563\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.7647 - val_loss: 0.4871 - val_accuracy: 0.7550\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.7644 - val_loss: 0.4865 - val_accuracy: 0.7538\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.7667 - val_loss: 0.4869 - val_accuracy: 0.7600\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4862 - accuracy: 0.7688 - val_loss: 0.4856 - val_accuracy: 0.7563\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7678 - val_loss: 0.4862 - val_accuracy: 0.7550\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.7696 - val_loss: 0.4850 - val_accuracy: 0.7600\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7683 - val_loss: 0.4856 - val_accuracy: 0.7563\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6640 - val_loss: 0.5391 - val_accuracy: 0.7437\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5230 - accuracy: 0.7479 - val_loss: 0.5013 - val_accuracy: 0.7550\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7515 - val_loss: 0.4957 - val_accuracy: 0.7613\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5045 - accuracy: 0.7532 - val_loss: 0.4933 - val_accuracy: 0.7563\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7546 - val_loss: 0.4923 - val_accuracy: 0.7575\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4998 - accuracy: 0.7560 - val_loss: 0.4922 - val_accuracy: 0.7625\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.7568 - val_loss: 0.4909 - val_accuracy: 0.7600\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.7597 - val_loss: 0.4909 - val_accuracy: 0.7625\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.7608 - val_loss: 0.4893 - val_accuracy: 0.7600\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.7597 - val_loss: 0.4890 - val_accuracy: 0.7575\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7585 - val_loss: 0.4896 - val_accuracy: 0.7550\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7622 - val_loss: 0.4892 - val_accuracy: 0.7588\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7610 - val_loss: 0.4895 - val_accuracy: 0.7613\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4905 - accuracy: 0.7604 - val_loss: 0.4887 - val_accuracy: 0.7613\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.7619 - val_loss: 0.4870 - val_accuracy: 0.7550\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.7600 - val_loss: 0.4873 - val_accuracy: 0.7563\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.7626 - val_loss: 0.4877 - val_accuracy: 0.7650\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7613 - val_loss: 0.4876 - val_accuracy: 0.7575\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7626 - val_loss: 0.4882 - val_accuracy: 0.7625\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.7626 - val_loss: 0.4874 - val_accuracy: 0.7600\n",
            "63/63 [==============================] - 0s 749us/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 4ms/step - loss: 0.5592 - accuracy: 0.7138 - val_loss: 0.5074 - val_accuracy: 0.7437\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7401 - val_loss: 0.4948 - val_accuracy: 0.7525\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.7465 - val_loss: 0.4913 - val_accuracy: 0.7500\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7461 - val_loss: 0.4899 - val_accuracy: 0.7462\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5077 - accuracy: 0.7496 - val_loss: 0.4883 - val_accuracy: 0.7613\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7501 - val_loss: 0.4888 - val_accuracy: 0.7513\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7515 - val_loss: 0.4876 - val_accuracy: 0.7487\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.7554 - val_loss: 0.4876 - val_accuracy: 0.7550\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7546 - val_loss: 0.4877 - val_accuracy: 0.7600\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7550 - val_loss: 0.4885 - val_accuracy: 0.7563\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7568 - val_loss: 0.4891 - val_accuracy: 0.7550\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7538 - val_loss: 0.4896 - val_accuracy: 0.7588\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7581 - val_loss: 0.4896 - val_accuracy: 0.7625\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.7549 - val_loss: 0.4881 - val_accuracy: 0.7575\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7617 - val_loss: 0.4865 - val_accuracy: 0.7588\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7608 - val_loss: 0.4882 - val_accuracy: 0.7538\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.7611 - val_loss: 0.4870 - val_accuracy: 0.7538\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.7589 - val_loss: 0.4887 - val_accuracy: 0.7575\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.7633 - val_loss: 0.4894 - val_accuracy: 0.7600\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.7621 - val_loss: 0.4869 - val_accuracy: 0.7588\n",
            "63/63 [==============================] - 0s 720us/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 3ms/step - loss: 0.5768 - accuracy: 0.7061 - val_loss: 0.5194 - val_accuracy: 0.7462\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7490 - val_loss: 0.4999 - val_accuracy: 0.7675\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.5099 - accuracy: 0.7524 - val_loss: 0.4941 - val_accuracy: 0.7625\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.7515 - val_loss: 0.4918 - val_accuracy: 0.7638\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7526 - val_loss: 0.4893 - val_accuracy: 0.7625\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7563 - val_loss: 0.4892 - val_accuracy: 0.7575\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4976 - accuracy: 0.7586 - val_loss: 0.4879 - val_accuracy: 0.7600\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4963 - accuracy: 0.7597 - val_loss: 0.4871 - val_accuracy: 0.7675\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7582 - val_loss: 0.4868 - val_accuracy: 0.7613\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.7588 - val_loss: 0.4855 - val_accuracy: 0.7650\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4927 - accuracy: 0.7589 - val_loss: 0.4859 - val_accuracy: 0.7675\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.7599 - val_loss: 0.4855 - val_accuracy: 0.7675\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.7625 - val_loss: 0.4862 - val_accuracy: 0.7625\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.7615 - val_loss: 0.4852 - val_accuracy: 0.7613\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.7632 - val_loss: 0.4847 - val_accuracy: 0.7663\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.7639 - val_loss: 0.4851 - val_accuracy: 0.7638\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7649 - val_loss: 0.4856 - val_accuracy: 0.7675\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4863 - accuracy: 0.7644 - val_loss: 0.4846 - val_accuracy: 0.7600\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.7650 - val_loss: 0.4869 - val_accuracy: 0.7663\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.7647 - val_loss: 0.4873 - val_accuracy: 0.7663\n",
            "63/63 [==============================] - 0s 800us/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 3ms/step - loss: 0.5783 - accuracy: 0.6922 - val_loss: 0.5393 - val_accuracy: 0.7212\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7382 - val_loss: 0.5178 - val_accuracy: 0.7462\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7496 - val_loss: 0.5100 - val_accuracy: 0.7487\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7536 - val_loss: 0.5048 - val_accuracy: 0.7487\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5002 - accuracy: 0.7554 - val_loss: 0.5016 - val_accuracy: 0.7525\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7569 - val_loss: 0.5012 - val_accuracy: 0.7575\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7582 - val_loss: 0.4991 - val_accuracy: 0.7563\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.7600 - val_loss: 0.4978 - val_accuracy: 0.7588\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7610 - val_loss: 0.4965 - val_accuracy: 0.7625\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.7624 - val_loss: 0.4954 - val_accuracy: 0.7675\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.7628 - val_loss: 0.4949 - val_accuracy: 0.7538\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7644 - val_loss: 0.4941 - val_accuracy: 0.7625\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7643 - val_loss: 0.4948 - val_accuracy: 0.7538\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.7626 - val_loss: 0.4951 - val_accuracy: 0.7613\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7650 - val_loss: 0.4928 - val_accuracy: 0.7538\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.7651 - val_loss: 0.4934 - val_accuracy: 0.7538\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4832 - accuracy: 0.7651 - val_loss: 0.4952 - val_accuracy: 0.7613\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7663 - val_loss: 0.4927 - val_accuracy: 0.7613\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4809 - accuracy: 0.7678 - val_loss: 0.4933 - val_accuracy: 0.7525\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7671 - val_loss: 0.4941 - val_accuracy: 0.7487\n",
            "63/63 [==============================] - 0s 755us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newdata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "05ryfvyLzFVK",
        "outputId": "470c7730-c6a0-44cd-c2e1-c90942955fc4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SVM  KNN  RF  LR  ANN  XGB  y_test\n",
              "0    0    0   0   0    0    0       0\n",
              "1    0    0   0   0    0    0       1\n",
              "2    1    1   1   0    1    1       1\n",
              "3    0    0   0   0    0    1       0\n",
              "4    1    1   1   1    1    1       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e4001e7-967c-49f7-9db3-6316fc621d21\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SVM</th>\n",
              "      <th>KNN</th>\n",
              "      <th>RF</th>\n",
              "      <th>LR</th>\n",
              "      <th>ANN</th>\n",
              "      <th>XGB</th>\n",
              "      <th>y_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e4001e7-967c-49f7-9db3-6316fc621d21')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e4001e7-967c-49f7-9db3-6316fc621d21 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e4001e7-967c-49f7-9db3-6316fc621d21');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DNN model\n",
        "DNNX = newdata.drop(columns=['y_test']).copy()\n",
        "DNNY = newdata.y_test.copy()\n",
        "DX_train, DX_test, Dy_train, Dy_test = train_test_split(\n",
        "    DNNX, DNNY, test_size=0.2, random_state=123)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(30, activation='relu', input_shape=DNNX.shape[1:]),\n",
        "    tf.keras.layers.Dense(25, activation='relu'),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dense(15, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(DX_train, Dy_train, epochs=500, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4kf97a3yX15",
        "outputId": "d7571b53-e8f6-41f1-bbf3-47480bb6c34a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "100/100 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.7241 - val_loss: 0.6320 - val_accuracy: 0.7606\n",
            "Epoch 2/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.6297 - accuracy: 0.7556 - val_loss: 0.6203 - val_accuracy: 0.7694\n",
            "Epoch 3/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.6149 - accuracy: 0.7556 - val_loss: 0.6047 - val_accuracy: 0.7681\n",
            "Epoch 4/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.6043 - accuracy: 0.7563 - val_loss: 0.5923 - val_accuracy: 0.7694\n",
            "Epoch 5/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5943 - accuracy: 0.7561 - val_loss: 0.5835 - val_accuracy: 0.7688\n",
            "Epoch 6/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5871 - accuracy: 0.7544 - val_loss: 0.5739 - val_accuracy: 0.7706\n",
            "Epoch 7/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.7572 - val_loss: 0.5666 - val_accuracy: 0.7725\n",
            "Epoch 8/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5726 - accuracy: 0.7548 - val_loss: 0.5603 - val_accuracy: 0.7706\n",
            "Epoch 9/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7572 - val_loss: 0.5548 - val_accuracy: 0.7713\n",
            "Epoch 10/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5622 - accuracy: 0.7578 - val_loss: 0.5493 - val_accuracy: 0.7675\n",
            "Epoch 11/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7548 - val_loss: 0.5462 - val_accuracy: 0.7681\n",
            "Epoch 12/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.7577 - val_loss: 0.5439 - val_accuracy: 0.7669\n",
            "Epoch 13/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.7589 - val_loss: 0.5413 - val_accuracy: 0.7663\n",
            "Epoch 14/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.7592 - val_loss: 0.5377 - val_accuracy: 0.7681\n",
            "Epoch 15/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.7573 - val_loss: 0.5376 - val_accuracy: 0.7625\n",
            "Epoch 16/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.7589 - val_loss: 0.5340 - val_accuracy: 0.7650\n",
            "Epoch 17/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5442 - accuracy: 0.7594 - val_loss: 0.5326 - val_accuracy: 0.7650\n",
            "Epoch 18/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5422 - accuracy: 0.7603 - val_loss: 0.5318 - val_accuracy: 0.7663\n",
            "Epoch 19/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5409 - accuracy: 0.7597 - val_loss: 0.5298 - val_accuracy: 0.7650\n",
            "Epoch 20/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.7597 - val_loss: 0.5296 - val_accuracy: 0.7650\n",
            "Epoch 21/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.7605 - val_loss: 0.5323 - val_accuracy: 0.7663\n",
            "Epoch 22/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7608 - val_loss: 0.5300 - val_accuracy: 0.7663\n",
            "Epoch 23/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5375 - accuracy: 0.7602 - val_loss: 0.5327 - val_accuracy: 0.7669\n",
            "Epoch 24/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5379 - accuracy: 0.7595 - val_loss: 0.5312 - val_accuracy: 0.7644\n",
            "Epoch 25/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7605 - val_loss: 0.5288 - val_accuracy: 0.7631\n",
            "Epoch 26/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5368 - accuracy: 0.7597 - val_loss: 0.5288 - val_accuracy: 0.7631\n",
            "Epoch 27/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5369 - accuracy: 0.7606 - val_loss: 0.5284 - val_accuracy: 0.7606\n",
            "Epoch 28/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7584 - val_loss: 0.5259 - val_accuracy: 0.7650\n",
            "Epoch 29/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5360 - accuracy: 0.7608 - val_loss: 0.5264 - val_accuracy: 0.7663\n",
            "Epoch 30/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5359 - accuracy: 0.7600 - val_loss: 0.5266 - val_accuracy: 0.7638\n",
            "Epoch 31/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5364 - accuracy: 0.7608 - val_loss: 0.5278 - val_accuracy: 0.7656\n",
            "Epoch 32/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7600 - val_loss: 0.5264 - val_accuracy: 0.7675\n",
            "Epoch 33/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.7613 - val_loss: 0.5275 - val_accuracy: 0.7669\n",
            "Epoch 34/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5359 - accuracy: 0.7608 - val_loss: 0.5277 - val_accuracy: 0.7631\n",
            "Epoch 35/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7609 - val_loss: 0.5265 - val_accuracy: 0.7625\n",
            "Epoch 36/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7606 - val_loss: 0.5267 - val_accuracy: 0.7675\n",
            "Epoch 37/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7605 - val_loss: 0.5259 - val_accuracy: 0.7638\n",
            "Epoch 38/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7589 - val_loss: 0.5276 - val_accuracy: 0.7669\n",
            "Epoch 39/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7589 - val_loss: 0.5267 - val_accuracy: 0.7669\n",
            "Epoch 40/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7606 - val_loss: 0.5258 - val_accuracy: 0.7656\n",
            "Epoch 41/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7600 - val_loss: 0.5267 - val_accuracy: 0.7656\n",
            "Epoch 42/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7616 - val_loss: 0.5264 - val_accuracy: 0.7631\n",
            "Epoch 43/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7598 - val_loss: 0.5272 - val_accuracy: 0.7638\n",
            "Epoch 44/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7608 - val_loss: 0.5269 - val_accuracy: 0.7631\n",
            "Epoch 45/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7597 - val_loss: 0.5260 - val_accuracy: 0.7644\n",
            "Epoch 46/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7597 - val_loss: 0.5296 - val_accuracy: 0.7669\n",
            "Epoch 47/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7598 - val_loss: 0.5272 - val_accuracy: 0.7638\n",
            "Epoch 48/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7602 - val_loss: 0.5262 - val_accuracy: 0.7625\n",
            "Epoch 49/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7619 - val_loss: 0.5271 - val_accuracy: 0.7631\n",
            "Epoch 50/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7606 - val_loss: 0.5293 - val_accuracy: 0.7625\n",
            "Epoch 51/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7606 - val_loss: 0.5294 - val_accuracy: 0.7644\n",
            "Epoch 52/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7603 - val_loss: 0.5258 - val_accuracy: 0.7656\n",
            "Epoch 53/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7600 - val_loss: 0.5267 - val_accuracy: 0.7638\n",
            "Epoch 54/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7608 - val_loss: 0.5275 - val_accuracy: 0.7619\n",
            "Epoch 55/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7606 - val_loss: 0.5265 - val_accuracy: 0.7619\n",
            "Epoch 56/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7591 - val_loss: 0.5280 - val_accuracy: 0.7656\n",
            "Epoch 57/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7602 - val_loss: 0.5269 - val_accuracy: 0.7619\n",
            "Epoch 58/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7600 - val_loss: 0.5273 - val_accuracy: 0.7619\n",
            "Epoch 59/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7603 - val_loss: 0.5271 - val_accuracy: 0.7644\n",
            "Epoch 60/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7602 - val_loss: 0.5268 - val_accuracy: 0.7650\n",
            "Epoch 61/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7606 - val_loss: 0.5267 - val_accuracy: 0.7625\n",
            "Epoch 62/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7613 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 63/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7600 - val_loss: 0.5278 - val_accuracy: 0.7625\n",
            "Epoch 64/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7611 - val_loss: 0.5263 - val_accuracy: 0.7638\n",
            "Epoch 65/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7598 - val_loss: 0.5276 - val_accuracy: 0.7625\n",
            "Epoch 66/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7591 - val_loss: 0.5267 - val_accuracy: 0.7625\n",
            "Epoch 67/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7613 - val_loss: 0.5273 - val_accuracy: 0.7638\n",
            "Epoch 68/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7606 - val_loss: 0.5276 - val_accuracy: 0.7650\n",
            "Epoch 69/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7597 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 70/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7602 - val_loss: 0.5268 - val_accuracy: 0.7644\n",
            "Epoch 71/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7594 - val_loss: 0.5268 - val_accuracy: 0.7650\n",
            "Epoch 72/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7613 - val_loss: 0.5268 - val_accuracy: 0.7625\n",
            "Epoch 73/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7595 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 74/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7598 - val_loss: 0.5293 - val_accuracy: 0.7681\n",
            "Epoch 75/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7603 - val_loss: 0.5276 - val_accuracy: 0.7631\n",
            "Epoch 76/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7598 - val_loss: 0.5277 - val_accuracy: 0.7619\n",
            "Epoch 77/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7603 - val_loss: 0.5287 - val_accuracy: 0.7656\n",
            "Epoch 78/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7595 - val_loss: 0.5274 - val_accuracy: 0.7638\n",
            "Epoch 79/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7611 - val_loss: 0.5291 - val_accuracy: 0.7638\n",
            "Epoch 80/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7609 - val_loss: 0.5280 - val_accuracy: 0.7631\n",
            "Epoch 81/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7600 - val_loss: 0.5279 - val_accuracy: 0.7625\n",
            "Epoch 82/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7609 - val_loss: 0.5276 - val_accuracy: 0.7625\n",
            "Epoch 83/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7608 - val_loss: 0.5264 - val_accuracy: 0.7656\n",
            "Epoch 84/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7605 - val_loss: 0.5270 - val_accuracy: 0.7656\n",
            "Epoch 85/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7602 - val_loss: 0.5271 - val_accuracy: 0.7656\n",
            "Epoch 86/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7589 - val_loss: 0.5266 - val_accuracy: 0.7644\n",
            "Epoch 87/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7595 - val_loss: 0.5274 - val_accuracy: 0.7631\n",
            "Epoch 88/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7602 - val_loss: 0.5282 - val_accuracy: 0.7631\n",
            "Epoch 89/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7589 - val_loss: 0.5264 - val_accuracy: 0.7656\n",
            "Epoch 90/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7605 - val_loss: 0.5287 - val_accuracy: 0.7631\n",
            "Epoch 91/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7605 - val_loss: 0.5272 - val_accuracy: 0.7631\n",
            "Epoch 92/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7611 - val_loss: 0.5279 - val_accuracy: 0.7631\n",
            "Epoch 93/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7614 - val_loss: 0.5276 - val_accuracy: 0.7631\n",
            "Epoch 94/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7597 - val_loss: 0.5275 - val_accuracy: 0.7625\n",
            "Epoch 95/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7625 - val_loss: 0.5276 - val_accuracy: 0.7606\n",
            "Epoch 96/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7586 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 97/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7609 - val_loss: 0.5287 - val_accuracy: 0.7631\n",
            "Epoch 98/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7597 - val_loss: 0.5289 - val_accuracy: 0.7631\n",
            "Epoch 99/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7605 - val_loss: 0.5270 - val_accuracy: 0.7656\n",
            "Epoch 100/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7606 - val_loss: 0.5276 - val_accuracy: 0.7650\n",
            "Epoch 101/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7598 - val_loss: 0.5267 - val_accuracy: 0.7650\n",
            "Epoch 102/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7597 - val_loss: 0.5301 - val_accuracy: 0.7694\n",
            "Epoch 103/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7594 - val_loss: 0.5281 - val_accuracy: 0.7656\n",
            "Epoch 104/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7602 - val_loss: 0.5278 - val_accuracy: 0.7619\n",
            "Epoch 105/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7606 - val_loss: 0.5275 - val_accuracy: 0.7644\n",
            "Epoch 106/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7606 - val_loss: 0.5276 - val_accuracy: 0.7650\n",
            "Epoch 107/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7603 - val_loss: 0.5283 - val_accuracy: 0.7631\n",
            "Epoch 108/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7606 - val_loss: 0.5284 - val_accuracy: 0.7631\n",
            "Epoch 109/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7588 - val_loss: 0.5280 - val_accuracy: 0.7619\n",
            "Epoch 110/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7595 - val_loss: 0.5271 - val_accuracy: 0.7625\n",
            "Epoch 111/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7602 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 112/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7619 - val_loss: 0.5292 - val_accuracy: 0.7656\n",
            "Epoch 113/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7605 - val_loss: 0.5277 - val_accuracy: 0.7638\n",
            "Epoch 114/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7606 - val_loss: 0.5290 - val_accuracy: 0.7631\n",
            "Epoch 115/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7609 - val_loss: 0.5289 - val_accuracy: 0.7631\n",
            "Epoch 116/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7623 - val_loss: 0.5286 - val_accuracy: 0.7638\n",
            "Epoch 117/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7589 - val_loss: 0.5289 - val_accuracy: 0.7644\n",
            "Epoch 118/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7606 - val_loss: 0.5283 - val_accuracy: 0.7631\n",
            "Epoch 119/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7609 - val_loss: 0.5279 - val_accuracy: 0.7650\n",
            "Epoch 120/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7598 - val_loss: 0.5279 - val_accuracy: 0.7644\n",
            "Epoch 121/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7597 - val_loss: 0.5266 - val_accuracy: 0.7644\n",
            "Epoch 122/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7592 - val_loss: 0.5289 - val_accuracy: 0.7644\n",
            "Epoch 123/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7609 - val_loss: 0.5279 - val_accuracy: 0.7644\n",
            "Epoch 124/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7619 - val_loss: 0.5287 - val_accuracy: 0.7644\n",
            "Epoch 125/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7606 - val_loss: 0.5280 - val_accuracy: 0.7631\n",
            "Epoch 126/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7592 - val_loss: 0.5276 - val_accuracy: 0.7631\n",
            "Epoch 127/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7594 - val_loss: 0.5280 - val_accuracy: 0.7644\n",
            "Epoch 128/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7595 - val_loss: 0.5311 - val_accuracy: 0.7681\n",
            "Epoch 129/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7595 - val_loss: 0.5292 - val_accuracy: 0.7669\n",
            "Epoch 130/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7588 - val_loss: 0.5309 - val_accuracy: 0.7656\n",
            "Epoch 131/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7613 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 132/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7611 - val_loss: 0.5273 - val_accuracy: 0.7644\n",
            "Epoch 133/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7619 - val_loss: 0.5295 - val_accuracy: 0.7644\n",
            "Epoch 134/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7586 - val_loss: 0.5277 - val_accuracy: 0.7644\n",
            "Epoch 135/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7600 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
            "Epoch 136/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7597 - val_loss: 0.5276 - val_accuracy: 0.7644\n",
            "Epoch 137/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7602 - val_loss: 0.5278 - val_accuracy: 0.7631\n",
            "Epoch 138/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7595 - val_loss: 0.5282 - val_accuracy: 0.7638\n",
            "Epoch 139/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7619 - val_loss: 0.5322 - val_accuracy: 0.7650\n",
            "Epoch 140/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7613 - val_loss: 0.5272 - val_accuracy: 0.7656\n",
            "Epoch 141/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7597 - val_loss: 0.5282 - val_accuracy: 0.7650\n",
            "Epoch 142/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7608 - val_loss: 0.5280 - val_accuracy: 0.7644\n",
            "Epoch 143/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7595 - val_loss: 0.5297 - val_accuracy: 0.7631\n",
            "Epoch 144/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7608 - val_loss: 0.5276 - val_accuracy: 0.7631\n",
            "Epoch 145/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7600 - val_loss: 0.5282 - val_accuracy: 0.7650\n",
            "Epoch 146/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7600 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
            "Epoch 147/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7597 - val_loss: 0.5282 - val_accuracy: 0.7644\n",
            "Epoch 148/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7598 - val_loss: 0.5287 - val_accuracy: 0.7650\n",
            "Epoch 149/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7608 - val_loss: 0.5290 - val_accuracy: 0.7650\n",
            "Epoch 150/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7603 - val_loss: 0.5283 - val_accuracy: 0.7644\n",
            "Epoch 151/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7605 - val_loss: 0.5285 - val_accuracy: 0.7669\n",
            "Epoch 152/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7592 - val_loss: 0.5285 - val_accuracy: 0.7644\n",
            "Epoch 153/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7595 - val_loss: 0.5288 - val_accuracy: 0.7644\n",
            "Epoch 154/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7614 - val_loss: 0.5280 - val_accuracy: 0.7638\n",
            "Epoch 155/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7608 - val_loss: 0.5283 - val_accuracy: 0.7644\n",
            "Epoch 156/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7584 - val_loss: 0.5283 - val_accuracy: 0.7625\n",
            "Epoch 157/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7611 - val_loss: 0.5282 - val_accuracy: 0.7644\n",
            "Epoch 158/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7597 - val_loss: 0.5300 - val_accuracy: 0.7638\n",
            "Epoch 159/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7595 - val_loss: 0.5281 - val_accuracy: 0.7625\n",
            "Epoch 160/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5290 - val_accuracy: 0.7644\n",
            "Epoch 161/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7619 - val_loss: 0.5291 - val_accuracy: 0.7650\n",
            "Epoch 162/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7592 - val_loss: 0.5284 - val_accuracy: 0.7656\n",
            "Epoch 163/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7600 - val_loss: 0.5284 - val_accuracy: 0.7625\n",
            "Epoch 164/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7591 - val_loss: 0.5285 - val_accuracy: 0.7619\n",
            "Epoch 165/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7602 - val_loss: 0.5292 - val_accuracy: 0.7644\n",
            "Epoch 166/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7597 - val_loss: 0.5285 - val_accuracy: 0.7644\n",
            "Epoch 167/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7602 - val_loss: 0.5289 - val_accuracy: 0.7625\n",
            "Epoch 168/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7598 - val_loss: 0.5293 - val_accuracy: 0.7625\n",
            "Epoch 169/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7603 - val_loss: 0.5289 - val_accuracy: 0.7625\n",
            "Epoch 170/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7609 - val_loss: 0.5281 - val_accuracy: 0.7631\n",
            "Epoch 171/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7606 - val_loss: 0.5293 - val_accuracy: 0.7631\n",
            "Epoch 172/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7606 - val_loss: 0.5286 - val_accuracy: 0.7625\n",
            "Epoch 173/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7597 - val_loss: 0.5292 - val_accuracy: 0.7644\n",
            "Epoch 174/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7586 - val_loss: 0.5305 - val_accuracy: 0.7675\n",
            "Epoch 175/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7608 - val_loss: 0.5289 - val_accuracy: 0.7644\n",
            "Epoch 176/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7605 - val_loss: 0.5289 - val_accuracy: 0.7625\n",
            "Epoch 177/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5294 - val_accuracy: 0.7644\n",
            "Epoch 178/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7600 - val_loss: 0.5289 - val_accuracy: 0.7663\n",
            "Epoch 179/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7617 - val_loss: 0.5286 - val_accuracy: 0.7638\n",
            "Epoch 180/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7603 - val_loss: 0.5282 - val_accuracy: 0.7644\n",
            "Epoch 181/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7597 - val_loss: 0.5283 - val_accuracy: 0.7625\n",
            "Epoch 182/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7611 - val_loss: 0.5288 - val_accuracy: 0.7644\n",
            "Epoch 183/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7605 - val_loss: 0.5291 - val_accuracy: 0.7656\n",
            "Epoch 184/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7606 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 185/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7613 - val_loss: 0.5298 - val_accuracy: 0.7644\n",
            "Epoch 186/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7605 - val_loss: 0.5288 - val_accuracy: 0.7638\n",
            "Epoch 187/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7613 - val_loss: 0.5292 - val_accuracy: 0.7644\n",
            "Epoch 188/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7595 - val_loss: 0.5280 - val_accuracy: 0.7650\n",
            "Epoch 189/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7603 - val_loss: 0.5299 - val_accuracy: 0.7644\n",
            "Epoch 190/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7594 - val_loss: 0.5285 - val_accuracy: 0.7625\n",
            "Epoch 191/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7598 - val_loss: 0.5287 - val_accuracy: 0.7638\n",
            "Epoch 192/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7588 - val_loss: 0.5289 - val_accuracy: 0.7644\n",
            "Epoch 193/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7600 - val_loss: 0.5285 - val_accuracy: 0.7638\n",
            "Epoch 194/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7600 - val_loss: 0.5285 - val_accuracy: 0.7631\n",
            "Epoch 195/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7598 - val_loss: 0.5287 - val_accuracy: 0.7650\n",
            "Epoch 196/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7606 - val_loss: 0.5305 - val_accuracy: 0.7638\n",
            "Epoch 197/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7617 - val_loss: 0.5313 - val_accuracy: 0.7656\n",
            "Epoch 198/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7616 - val_loss: 0.5300 - val_accuracy: 0.7663\n",
            "Epoch 199/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7620 - val_loss: 0.5282 - val_accuracy: 0.7619\n",
            "Epoch 200/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7600 - val_loss: 0.5286 - val_accuracy: 0.7644\n",
            "Epoch 201/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7603 - val_loss: 0.5285 - val_accuracy: 0.7650\n",
            "Epoch 202/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7602 - val_loss: 0.5299 - val_accuracy: 0.7650\n",
            "Epoch 203/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7600 - val_loss: 0.5286 - val_accuracy: 0.7631\n",
            "Epoch 204/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7591 - val_loss: 0.5286 - val_accuracy: 0.7631\n",
            "Epoch 205/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7605 - val_loss: 0.5292 - val_accuracy: 0.7681\n",
            "Epoch 206/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7605 - val_loss: 0.5290 - val_accuracy: 0.7644\n",
            "Epoch 207/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7592 - val_loss: 0.5287 - val_accuracy: 0.7644\n",
            "Epoch 208/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7600 - val_loss: 0.5301 - val_accuracy: 0.7644\n",
            "Epoch 209/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7606 - val_loss: 0.5295 - val_accuracy: 0.7638\n",
            "Epoch 210/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7603 - val_loss: 0.5285 - val_accuracy: 0.7644\n",
            "Epoch 211/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7614 - val_loss: 0.5289 - val_accuracy: 0.7644\n",
            "Epoch 212/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7606 - val_loss: 0.5293 - val_accuracy: 0.7644\n",
            "Epoch 213/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7606 - val_loss: 0.5288 - val_accuracy: 0.7644\n",
            "Epoch 214/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7600 - val_loss: 0.5278 - val_accuracy: 0.7625\n",
            "Epoch 215/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7603 - val_loss: 0.5289 - val_accuracy: 0.7669\n",
            "Epoch 216/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7606 - val_loss: 0.5291 - val_accuracy: 0.7650\n",
            "Epoch 217/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7600 - val_loss: 0.5303 - val_accuracy: 0.7681\n",
            "Epoch 218/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7609 - val_loss: 0.5285 - val_accuracy: 0.7656\n",
            "Epoch 219/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7609 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 220/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7606 - val_loss: 0.5287 - val_accuracy: 0.7625\n",
            "Epoch 221/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7600 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 222/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7597 - val_loss: 0.5288 - val_accuracy: 0.7644\n",
            "Epoch 223/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7609 - val_loss: 0.5286 - val_accuracy: 0.7638\n",
            "Epoch 224/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7613 - val_loss: 0.5293 - val_accuracy: 0.7644\n",
            "Epoch 225/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 226/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7598 - val_loss: 0.5284 - val_accuracy: 0.7644\n",
            "Epoch 227/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7611 - val_loss: 0.5286 - val_accuracy: 0.7650\n",
            "Epoch 228/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7605 - val_loss: 0.5281 - val_accuracy: 0.7631\n",
            "Epoch 229/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7606 - val_loss: 0.5307 - val_accuracy: 0.7644\n",
            "Epoch 230/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7606 - val_loss: 0.5293 - val_accuracy: 0.7650\n",
            "Epoch 231/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7614 - val_loss: 0.5300 - val_accuracy: 0.7638\n",
            "Epoch 232/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7613 - val_loss: 0.5290 - val_accuracy: 0.7650\n",
            "Epoch 233/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7605 - val_loss: 0.5301 - val_accuracy: 0.7669\n",
            "Epoch 234/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7606 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
            "Epoch 235/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7613 - val_loss: 0.5335 - val_accuracy: 0.7650\n",
            "Epoch 236/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7600 - val_loss: 0.5290 - val_accuracy: 0.7638\n",
            "Epoch 237/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7597 - val_loss: 0.5306 - val_accuracy: 0.7650\n",
            "Epoch 238/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7592 - val_loss: 0.5280 - val_accuracy: 0.7625\n",
            "Epoch 239/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7608 - val_loss: 0.5290 - val_accuracy: 0.7644\n",
            "Epoch 240/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7597 - val_loss: 0.5289 - val_accuracy: 0.7644\n",
            "Epoch 241/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7592 - val_loss: 0.5287 - val_accuracy: 0.7650\n",
            "Epoch 242/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7614 - val_loss: 0.5293 - val_accuracy: 0.7644\n",
            "Epoch 243/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7602 - val_loss: 0.5290 - val_accuracy: 0.7625\n",
            "Epoch 244/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7603 - val_loss: 0.5290 - val_accuracy: 0.7644\n",
            "Epoch 245/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7611 - val_loss: 0.5289 - val_accuracy: 0.7650\n",
            "Epoch 246/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7597 - val_loss: 0.5294 - val_accuracy: 0.7663\n",
            "Epoch 247/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7600 - val_loss: 0.5298 - val_accuracy: 0.7656\n",
            "Epoch 248/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7603 - val_loss: 0.5301 - val_accuracy: 0.7644\n",
            "Epoch 249/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 250/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7609 - val_loss: 0.5299 - val_accuracy: 0.7644\n",
            "Epoch 251/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7605 - val_loss: 0.5284 - val_accuracy: 0.7656\n",
            "Epoch 252/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7594 - val_loss: 0.5289 - val_accuracy: 0.7669\n",
            "Epoch 253/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7603 - val_loss: 0.5287 - val_accuracy: 0.7644\n",
            "Epoch 254/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7597 - val_loss: 0.5293 - val_accuracy: 0.7656\n",
            "Epoch 255/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7608 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 256/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7603 - val_loss: 0.5293 - val_accuracy: 0.7644\n",
            "Epoch 257/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7613 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 258/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7605 - val_loss: 0.5307 - val_accuracy: 0.7644\n",
            "Epoch 259/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7611 - val_loss: 0.5293 - val_accuracy: 0.7625\n",
            "Epoch 260/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7614 - val_loss: 0.5309 - val_accuracy: 0.7650\n",
            "Epoch 261/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5297 - val_accuracy: 0.7644\n",
            "Epoch 262/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7606 - val_loss: 0.5289 - val_accuracy: 0.7638\n",
            "Epoch 263/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7598 - val_loss: 0.5291 - val_accuracy: 0.7650\n",
            "Epoch 264/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7594 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 265/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7613 - val_loss: 0.5295 - val_accuracy: 0.7644\n",
            "Epoch 266/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7611 - val_loss: 0.5285 - val_accuracy: 0.7631\n",
            "Epoch 267/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7608 - val_loss: 0.5296 - val_accuracy: 0.7650\n",
            "Epoch 268/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7608 - val_loss: 0.5287 - val_accuracy: 0.7631\n",
            "Epoch 269/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7600 - val_loss: 0.5290 - val_accuracy: 0.7631\n",
            "Epoch 270/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7614 - val_loss: 0.5303 - val_accuracy: 0.7644\n",
            "Epoch 271/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7608 - val_loss: 0.5320 - val_accuracy: 0.7656\n",
            "Epoch 272/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7588 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 273/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5284 - val_accuracy: 0.7656\n",
            "Epoch 274/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7616 - val_loss: 0.5304 - val_accuracy: 0.7650\n",
            "Epoch 275/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7595 - val_loss: 0.5281 - val_accuracy: 0.7619\n",
            "Epoch 276/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7605 - val_loss: 0.5318 - val_accuracy: 0.7688\n",
            "Epoch 277/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7603 - val_loss: 0.5292 - val_accuracy: 0.7656\n",
            "Epoch 278/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7609 - val_loss: 0.5289 - val_accuracy: 0.7638\n",
            "Epoch 279/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7611 - val_loss: 0.5304 - val_accuracy: 0.7644\n",
            "Epoch 280/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7614 - val_loss: 0.5293 - val_accuracy: 0.7644\n",
            "Epoch 281/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7614 - val_loss: 0.5295 - val_accuracy: 0.7644\n",
            "Epoch 282/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7606 - val_loss: 0.5293 - val_accuracy: 0.7644\n",
            "Epoch 283/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7609 - val_loss: 0.5291 - val_accuracy: 0.7650\n",
            "Epoch 284/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7608 - val_loss: 0.5285 - val_accuracy: 0.7631\n",
            "Epoch 285/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7602 - val_loss: 0.5288 - val_accuracy: 0.7644\n",
            "Epoch 286/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7608 - val_loss: 0.5297 - val_accuracy: 0.7656\n",
            "Epoch 287/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7605 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 288/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7608 - val_loss: 0.5298 - val_accuracy: 0.7644\n",
            "Epoch 289/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7609 - val_loss: 0.5288 - val_accuracy: 0.7644\n",
            "Epoch 290/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7598 - val_loss: 0.5288 - val_accuracy: 0.7656\n",
            "Epoch 291/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7608 - val_loss: 0.5294 - val_accuracy: 0.7656\n",
            "Epoch 292/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7603 - val_loss: 0.5291 - val_accuracy: 0.7631\n",
            "Epoch 293/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7606 - val_loss: 0.5290 - val_accuracy: 0.7663\n",
            "Epoch 294/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7588 - val_loss: 0.5291 - val_accuracy: 0.7638\n",
            "Epoch 295/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7613 - val_loss: 0.5292 - val_accuracy: 0.7644\n",
            "Epoch 296/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7602 - val_loss: 0.5287 - val_accuracy: 0.7644\n",
            "Epoch 297/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7606 - val_loss: 0.5287 - val_accuracy: 0.7663\n",
            "Epoch 298/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7606 - val_loss: 0.5300 - val_accuracy: 0.7644\n",
            "Epoch 299/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7600 - val_loss: 0.5287 - val_accuracy: 0.7663\n",
            "Epoch 300/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7609 - val_loss: 0.5289 - val_accuracy: 0.7644\n",
            "Epoch 301/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7589 - val_loss: 0.5291 - val_accuracy: 0.7681\n",
            "Epoch 302/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7597 - val_loss: 0.5288 - val_accuracy: 0.7638\n",
            "Epoch 303/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7611 - val_loss: 0.5292 - val_accuracy: 0.7656\n",
            "Epoch 304/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7605 - val_loss: 0.5287 - val_accuracy: 0.7631\n",
            "Epoch 305/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7611 - val_loss: 0.5302 - val_accuracy: 0.7669\n",
            "Epoch 306/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7611 - val_loss: 0.5306 - val_accuracy: 0.7644\n",
            "Epoch 307/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7611 - val_loss: 0.5290 - val_accuracy: 0.7650\n",
            "Epoch 308/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7606 - val_loss: 0.5300 - val_accuracy: 0.7650\n",
            "Epoch 309/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7605 - val_loss: 0.5286 - val_accuracy: 0.7663\n",
            "Epoch 310/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7609 - val_loss: 0.5301 - val_accuracy: 0.7656\n",
            "Epoch 311/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7602 - val_loss: 0.5297 - val_accuracy: 0.7644\n",
            "Epoch 312/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7611 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 313/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7608 - val_loss: 0.5317 - val_accuracy: 0.7656\n",
            "Epoch 314/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7608 - val_loss: 0.5288 - val_accuracy: 0.7656\n",
            "Epoch 315/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7600 - val_loss: 0.5286 - val_accuracy: 0.7631\n",
            "Epoch 316/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7600 - val_loss: 0.5289 - val_accuracy: 0.7638\n",
            "Epoch 317/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7619 - val_loss: 0.5299 - val_accuracy: 0.7650\n",
            "Epoch 318/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7611 - val_loss: 0.5295 - val_accuracy: 0.7631\n",
            "Epoch 319/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7605 - val_loss: 0.5289 - val_accuracy: 0.7631\n",
            "Epoch 320/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7598 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 321/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7608 - val_loss: 0.5292 - val_accuracy: 0.7644\n",
            "Epoch 322/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7603 - val_loss: 0.5286 - val_accuracy: 0.7650\n",
            "Epoch 323/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7595 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 324/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7605 - val_loss: 0.5289 - val_accuracy: 0.7650\n",
            "Epoch 325/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7600 - val_loss: 0.5288 - val_accuracy: 0.7650\n",
            "Epoch 326/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7620 - val_loss: 0.5293 - val_accuracy: 0.7656\n",
            "Epoch 327/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7598 - val_loss: 0.5302 - val_accuracy: 0.7650\n",
            "Epoch 328/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7608 - val_loss: 0.5295 - val_accuracy: 0.7638\n",
            "Epoch 329/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7620 - val_loss: 0.5289 - val_accuracy: 0.7631\n",
            "Epoch 330/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7603 - val_loss: 0.5293 - val_accuracy: 0.7650\n",
            "Epoch 331/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7609 - val_loss: 0.5290 - val_accuracy: 0.7638\n",
            "Epoch 332/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7617 - val_loss: 0.5291 - val_accuracy: 0.7638\n",
            "Epoch 333/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7608 - val_loss: 0.5297 - val_accuracy: 0.7656\n",
            "Epoch 334/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7613 - val_loss: 0.5293 - val_accuracy: 0.7638\n",
            "Epoch 335/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7600 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 336/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5291 - val_accuracy: 0.7638\n",
            "Epoch 337/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7609 - val_loss: 0.5306 - val_accuracy: 0.7644\n",
            "Epoch 338/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7598 - val_loss: 0.5296 - val_accuracy: 0.7638\n",
            "Epoch 339/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7597 - val_loss: 0.5294 - val_accuracy: 0.7638\n",
            "Epoch 340/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7616 - val_loss: 0.5289 - val_accuracy: 0.7638\n",
            "Epoch 341/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5293 - val_accuracy: 0.7663\n",
            "Epoch 342/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7605 - val_loss: 0.5290 - val_accuracy: 0.7638\n",
            "Epoch 343/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7623 - val_loss: 0.5288 - val_accuracy: 0.7644\n",
            "Epoch 344/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7606 - val_loss: 0.5299 - val_accuracy: 0.7650\n",
            "Epoch 345/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7611 - val_loss: 0.5288 - val_accuracy: 0.7656\n",
            "Epoch 346/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7595 - val_loss: 0.5292 - val_accuracy: 0.7644\n",
            "Epoch 347/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7609 - val_loss: 0.5308 - val_accuracy: 0.7644\n",
            "Epoch 348/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5326 - accuracy: 0.7614 - val_loss: 0.5302 - val_accuracy: 0.7644\n",
            "Epoch 349/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7597 - val_loss: 0.5300 - val_accuracy: 0.7656\n",
            "Epoch 350/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7603 - val_loss: 0.5295 - val_accuracy: 0.7656\n",
            "Epoch 351/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7606 - val_loss: 0.5292 - val_accuracy: 0.7656\n",
            "Epoch 352/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7608 - val_loss: 0.5290 - val_accuracy: 0.7650\n",
            "Epoch 353/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7611 - val_loss: 0.5301 - val_accuracy: 0.7625\n",
            "Epoch 354/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7611 - val_loss: 0.5291 - val_accuracy: 0.7650\n",
            "Epoch 355/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5295 - val_accuracy: 0.7650\n",
            "Epoch 356/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7595 - val_loss: 0.5306 - val_accuracy: 0.7644\n",
            "Epoch 357/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7608 - val_loss: 0.5294 - val_accuracy: 0.7638\n",
            "Epoch 358/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7600 - val_loss: 0.5290 - val_accuracy: 0.7650\n",
            "Epoch 359/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7606 - val_loss: 0.5289 - val_accuracy: 0.7663\n",
            "Epoch 360/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7597 - val_loss: 0.5295 - val_accuracy: 0.7631\n",
            "Epoch 361/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7613 - val_loss: 0.5292 - val_accuracy: 0.7638\n",
            "Epoch 362/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5293 - val_accuracy: 0.7663\n",
            "Epoch 363/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5327 - accuracy: 0.7608 - val_loss: 0.5297 - val_accuracy: 0.7650\n",
            "Epoch 364/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7608 - val_loss: 0.5298 - val_accuracy: 0.7656\n",
            "Epoch 365/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7594 - val_loss: 0.5292 - val_accuracy: 0.7656\n",
            "Epoch 366/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7602 - val_loss: 0.5292 - val_accuracy: 0.7644\n",
            "Epoch 367/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7586 - val_loss: 0.5293 - val_accuracy: 0.7663\n",
            "Epoch 368/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7600 - val_loss: 0.5300 - val_accuracy: 0.7644\n",
            "Epoch 369/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5297 - val_accuracy: 0.7644\n",
            "Epoch 370/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7597 - val_loss: 0.5291 - val_accuracy: 0.7631\n",
            "Epoch 371/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7605 - val_loss: 0.5297 - val_accuracy: 0.7638\n",
            "Epoch 372/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7588 - val_loss: 0.5291 - val_accuracy: 0.7638\n",
            "Epoch 373/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7586 - val_loss: 0.5290 - val_accuracy: 0.7656\n",
            "Epoch 374/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7620 - val_loss: 0.5288 - val_accuracy: 0.7638\n",
            "Epoch 375/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7616 - val_loss: 0.5287 - val_accuracy: 0.7644\n",
            "Epoch 376/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7602 - val_loss: 0.5296 - val_accuracy: 0.7663\n",
            "Epoch 377/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7608 - val_loss: 0.5293 - val_accuracy: 0.7656\n",
            "Epoch 378/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7606 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 379/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7589 - val_loss: 0.5311 - val_accuracy: 0.7644\n",
            "Epoch 380/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7613 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
            "Epoch 381/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7605 - val_loss: 0.5298 - val_accuracy: 0.7663\n",
            "Epoch 382/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7600 - val_loss: 0.5299 - val_accuracy: 0.7650\n",
            "Epoch 383/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7606 - val_loss: 0.5295 - val_accuracy: 0.7638\n",
            "Epoch 384/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7600 - val_loss: 0.5302 - val_accuracy: 0.7650\n",
            "Epoch 385/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7603 - val_loss: 0.5292 - val_accuracy: 0.7663\n",
            "Epoch 386/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7602 - val_loss: 0.5295 - val_accuracy: 0.7656\n",
            "Epoch 387/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7602 - val_loss: 0.5291 - val_accuracy: 0.7644\n",
            "Epoch 388/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7611 - val_loss: 0.5299 - val_accuracy: 0.7644\n",
            "Epoch 389/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7606 - val_loss: 0.5306 - val_accuracy: 0.7650\n",
            "Epoch 390/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7595 - val_loss: 0.5296 - val_accuracy: 0.7663\n",
            "Epoch 391/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7614 - val_loss: 0.5304 - val_accuracy: 0.7644\n",
            "Epoch 392/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7600 - val_loss: 0.5294 - val_accuracy: 0.7650\n",
            "Epoch 393/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7605 - val_loss: 0.5295 - val_accuracy: 0.7644\n",
            "Epoch 394/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7603 - val_loss: 0.5304 - val_accuracy: 0.7663\n",
            "Epoch 395/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7623 - val_loss: 0.5298 - val_accuracy: 0.7638\n",
            "Epoch 396/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7602 - val_loss: 0.5292 - val_accuracy: 0.7650\n",
            "Epoch 397/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7602 - val_loss: 0.5300 - val_accuracy: 0.7644\n",
            "Epoch 398/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7617 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 399/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7594 - val_loss: 0.5290 - val_accuracy: 0.7663\n",
            "Epoch 400/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7603 - val_loss: 0.5298 - val_accuracy: 0.7656\n",
            "Epoch 401/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7595 - val_loss: 0.5294 - val_accuracy: 0.7644\n",
            "Epoch 402/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7595 - val_loss: 0.5296 - val_accuracy: 0.7644\n",
            "Epoch 403/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7606 - val_loss: 0.5298 - val_accuracy: 0.7650\n",
            "Epoch 404/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7614 - val_loss: 0.5299 - val_accuracy: 0.7650\n",
            "Epoch 405/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7597 - val_loss: 0.5291 - val_accuracy: 0.7656\n",
            "Epoch 406/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7613 - val_loss: 0.5301 - val_accuracy: 0.7644\n",
            "Epoch 407/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7598 - val_loss: 0.5290 - val_accuracy: 0.7644\n",
            "Epoch 408/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7606 - val_loss: 0.5297 - val_accuracy: 0.7650\n",
            "Epoch 409/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7609 - val_loss: 0.5299 - val_accuracy: 0.7656\n",
            "Epoch 410/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7606 - val_loss: 0.5295 - val_accuracy: 0.7656\n",
            "Epoch 411/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7605 - val_loss: 0.5301 - val_accuracy: 0.7644\n",
            "Epoch 412/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7617 - val_loss: 0.5290 - val_accuracy: 0.7663\n",
            "Epoch 413/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7592 - val_loss: 0.5293 - val_accuracy: 0.7663\n",
            "Epoch 414/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7614 - val_loss: 0.5295 - val_accuracy: 0.7663\n",
            "Epoch 415/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7602 - val_loss: 0.5297 - val_accuracy: 0.7644\n",
            "Epoch 416/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7608 - val_loss: 0.5298 - val_accuracy: 0.7631\n",
            "Epoch 417/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7595 - val_loss: 0.5304 - val_accuracy: 0.7644\n",
            "Epoch 418/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7605 - val_loss: 0.5297 - val_accuracy: 0.7656\n",
            "Epoch 419/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7597 - val_loss: 0.5292 - val_accuracy: 0.7663\n",
            "Epoch 420/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7608 - val_loss: 0.5291 - val_accuracy: 0.7663\n",
            "Epoch 421/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7613 - val_loss: 0.5299 - val_accuracy: 0.7663\n",
            "Epoch 422/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7616 - val_loss: 0.5302 - val_accuracy: 0.7644\n",
            "Epoch 423/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7609 - val_loss: 0.5307 - val_accuracy: 0.7644\n",
            "Epoch 424/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7605 - val_loss: 0.5300 - val_accuracy: 0.7656\n",
            "Epoch 425/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7595 - val_loss: 0.5294 - val_accuracy: 0.7644\n",
            "Epoch 426/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5327 - accuracy: 0.7614 - val_loss: 0.5330 - val_accuracy: 0.7644\n",
            "Epoch 427/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7606 - val_loss: 0.5295 - val_accuracy: 0.7644\n",
            "Epoch 428/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7606 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
            "Epoch 429/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7595 - val_loss: 0.5305 - val_accuracy: 0.7644\n",
            "Epoch 430/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7602 - val_loss: 0.5302 - val_accuracy: 0.7638\n",
            "Epoch 431/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7617 - val_loss: 0.5298 - val_accuracy: 0.7631\n",
            "Epoch 432/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7598 - val_loss: 0.5297 - val_accuracy: 0.7631\n",
            "Epoch 433/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7600 - val_loss: 0.5300 - val_accuracy: 0.7638\n",
            "Epoch 434/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7611 - val_loss: 0.5298 - val_accuracy: 0.7638\n",
            "Epoch 435/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7606 - val_loss: 0.5299 - val_accuracy: 0.7644\n",
            "Epoch 436/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7613 - val_loss: 0.5299 - val_accuracy: 0.7644\n",
            "Epoch 437/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7603 - val_loss: 0.5298 - val_accuracy: 0.7663\n",
            "Epoch 438/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5311 - val_accuracy: 0.7644\n",
            "Epoch 439/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7609 - val_loss: 0.5294 - val_accuracy: 0.7644\n",
            "Epoch 440/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5287 - val_accuracy: 0.7631\n",
            "Epoch 441/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7609 - val_loss: 0.5290 - val_accuracy: 0.7631\n",
            "Epoch 442/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7606 - val_loss: 0.5289 - val_accuracy: 0.7631\n",
            "Epoch 443/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7602 - val_loss: 0.5293 - val_accuracy: 0.7644\n",
            "Epoch 444/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7598 - val_loss: 0.5297 - val_accuracy: 0.7644\n",
            "Epoch 445/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7606 - val_loss: 0.5303 - val_accuracy: 0.7644\n",
            "Epoch 446/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7594 - val_loss: 0.5292 - val_accuracy: 0.7644\n",
            "Epoch 447/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7603 - val_loss: 0.5295 - val_accuracy: 0.7656\n",
            "Epoch 448/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7606 - val_loss: 0.5299 - val_accuracy: 0.7644\n",
            "Epoch 449/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7597 - val_loss: 0.5316 - val_accuracy: 0.7650\n",
            "Epoch 450/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7600 - val_loss: 0.5294 - val_accuracy: 0.7650\n",
            "Epoch 451/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7617 - val_loss: 0.5302 - val_accuracy: 0.7644\n",
            "Epoch 452/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7605 - val_loss: 0.5302 - val_accuracy: 0.7663\n",
            "Epoch 453/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7606 - val_loss: 0.5303 - val_accuracy: 0.7638\n",
            "Epoch 454/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7605 - val_loss: 0.5300 - val_accuracy: 0.7650\n",
            "Epoch 455/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7606 - val_loss: 0.5298 - val_accuracy: 0.7638\n",
            "Epoch 456/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5327 - accuracy: 0.7611 - val_loss: 0.5311 - val_accuracy: 0.7656\n",
            "Epoch 457/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7600 - val_loss: 0.5300 - val_accuracy: 0.7631\n",
            "Epoch 458/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7598 - val_loss: 0.5300 - val_accuracy: 0.7656\n",
            "Epoch 459/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7591 - val_loss: 0.5309 - val_accuracy: 0.7644\n",
            "Epoch 460/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7609 - val_loss: 0.5299 - val_accuracy: 0.7656\n",
            "Epoch 461/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7603 - val_loss: 0.5303 - val_accuracy: 0.7650\n",
            "Epoch 462/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7602 - val_loss: 0.5294 - val_accuracy: 0.7638\n",
            "Epoch 463/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7600 - val_loss: 0.5293 - val_accuracy: 0.7644\n",
            "Epoch 464/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7600 - val_loss: 0.5297 - val_accuracy: 0.7631\n",
            "Epoch 465/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7606 - val_loss: 0.5300 - val_accuracy: 0.7663\n",
            "Epoch 466/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5306 - val_accuracy: 0.7644\n",
            "Epoch 467/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7602 - val_loss: 0.5300 - val_accuracy: 0.7656\n",
            "Epoch 468/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7603 - val_loss: 0.5294 - val_accuracy: 0.7650\n",
            "Epoch 469/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5294 - val_accuracy: 0.7644\n",
            "Epoch 470/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7603 - val_loss: 0.5302 - val_accuracy: 0.7650\n",
            "Epoch 471/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7595 - val_loss: 0.5298 - val_accuracy: 0.7644\n",
            "Epoch 472/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7608 - val_loss: 0.5299 - val_accuracy: 0.7644\n",
            "Epoch 473/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7605 - val_loss: 0.5300 - val_accuracy: 0.7644\n",
            "Epoch 474/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7608 - val_loss: 0.5309 - val_accuracy: 0.7644\n",
            "Epoch 475/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7611 - val_loss: 0.5299 - val_accuracy: 0.7638\n",
            "Epoch 476/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7609 - val_loss: 0.5318 - val_accuracy: 0.7644\n",
            "Epoch 477/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7608 - val_loss: 0.5297 - val_accuracy: 0.7663\n",
            "Epoch 478/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5300 - val_accuracy: 0.7644\n",
            "Epoch 479/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7605 - val_loss: 0.5300 - val_accuracy: 0.7644\n",
            "Epoch 480/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5299 - val_accuracy: 0.7644\n",
            "Epoch 481/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7606 - val_loss: 0.5298 - val_accuracy: 0.7644\n",
            "Epoch 482/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5297 - val_accuracy: 0.7663\n",
            "Epoch 483/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5299 - val_accuracy: 0.7625\n",
            "Epoch 484/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7589 - val_loss: 0.5297 - val_accuracy: 0.7663\n",
            "Epoch 485/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7602 - val_loss: 0.5299 - val_accuracy: 0.7638\n",
            "Epoch 486/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7608 - val_loss: 0.5306 - val_accuracy: 0.7663\n",
            "Epoch 487/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7603 - val_loss: 0.5304 - val_accuracy: 0.7638\n",
            "Epoch 488/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7616 - val_loss: 0.5298 - val_accuracy: 0.7638\n",
            "Epoch 489/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7600 - val_loss: 0.5302 - val_accuracy: 0.7631\n",
            "Epoch 490/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5303 - val_accuracy: 0.7644\n",
            "Epoch 491/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7608 - val_loss: 0.5299 - val_accuracy: 0.7650\n",
            "Epoch 492/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5326 - accuracy: 0.7598 - val_loss: 0.5301 - val_accuracy: 0.7663\n",
            "Epoch 493/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5298 - val_accuracy: 0.7650\n",
            "Epoch 494/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7605 - val_loss: 0.5302 - val_accuracy: 0.7638\n",
            "Epoch 495/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7600 - val_loss: 0.5296 - val_accuracy: 0.7656\n",
            "Epoch 496/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5302 - val_accuracy: 0.7644\n",
            "Epoch 497/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7603 - val_loss: 0.5303 - val_accuracy: 0.7644\n",
            "Epoch 498/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7608 - val_loss: 0.5298 - val_accuracy: 0.7663\n",
            "Epoch 499/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5309 - val_accuracy: 0.7644\n",
            "Epoch 500/500\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7609 - val_loss: 0.5311 - val_accuracy: 0.7656\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6e55aad630>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred_DNN = (model.predict(DNNX) > 0.5).astype(\"int32\")\n",
        "DTr = model.evaluate(DX_train, Dy_train,verbose=0)[1]\n",
        "DTe = model.evaluate(DX_test, Dy_test,verbose=0)[1]\n",
        "DTr,DTe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhG0YCbhASH9",
        "outputId": "5d146310-c26d-4ec3-e305-f621fe544ea4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7618749737739563, 0.7524999976158142)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_DNN = (model.predict(DX_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKbxrS57kOBE",
        "outputId": "1ac7cd7f-4de1-4928-cf14-f499bddb23be"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 751us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"DNN\":[DTr,DTe]})\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "v_YjlCdOO1JY",
        "outputId": "e543f39f-6a46-4ba6-eb79-d9523e3f635f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        train    test\n",
              "SVM  0.755750  0.7555\n",
              "KNN  0.772250  0.7340\n",
              "RF   0.751750  0.7565\n",
              "LR   0.761500  0.7565\n",
              "ANN  0.764875  0.7610\n",
              "XGB  0.983125  0.7240\n",
              "DNN  0.761875  0.7525"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9510d77f-7a23-4652-b2ef-f1aea0da237a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.755750</td>\n",
              "      <td>0.7555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>0.772250</td>\n",
              "      <td>0.7340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>0.751750</td>\n",
              "      <td>0.7565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.7565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>0.764875</td>\n",
              "      <td>0.7610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>0.983125</td>\n",
              "      <td>0.7240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>0.761875</td>\n",
              "      <td>0.7525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9510d77f-7a23-4652-b2ef-f1aea0da237a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9510d77f-7a23-4652-b2ef-f1aea0da237a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9510d77f-7a23-4652-b2ef-f1aea0da237a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AutoML Individual and AutoML DNN**"
      ],
      "metadata": {
        "id": "nJaAMLDKKfKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H2O AutoML"
      ],
      "metadata": {
        "id": "EhNt_UkjDKcV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h2o\n",
        "import h2o\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "h2o.init()\n",
        "from h2o.model.segment_models import H2OFrame\n",
        "from h2o.automl import H2OAutoML\n",
        "print(\"All Library Loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "iwMF9ROeDOzF",
        "outputId": "3798ec88-866e-461a-c744-37ad735a5f24"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h2o\n",
            "  Downloading h2o-3.40.0.4.tar.gz (177.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.8.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from h2o) (0.18.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.4)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.40.0.4-py2.py3-none-any.whl size=177697886 sha256=9f5d6cc666cf88c62ade2a92d57e711a61e798dfbd9503627c3fa3fb272545e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/f2/b0/5bb4d702a0467e82d77c45088db3eef25114c26b0eec8e7f6a\n",
            "Successfully built h2o\n",
            "Installing collected packages: h2o\n",
            "Successfully installed h2o-3.40.0.4\n",
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.19\" 2023-04-18; OpenJDK Runtime Environment (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1); OpenJDK 64-Bit Server VM (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.10/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpgsv3fw7v\n",
            "  JVM stdout: /tmp/tmpgsv3fw7v/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpgsv3fw7v/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         02 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.40.0.4\n",
              "H2O_cluster_version_age:    1 month and 25 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_nig6b4\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.170 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-1.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-1 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-1 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-1 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table th,\n",
              "#h2o-table-1 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>02 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.40.0.4</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>1 month and 25 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_nig6b4</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.170 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Library Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWdoOLbsF2qs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-QiglXeF_XE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8P1HTOHbugE-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asKywHvIu83f"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5wVvB0pvWHe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train, valid = hdf.split_frame(ratios=[.8], seed=123)\n",
        "#hdf = h2o.H2OFrame(df)\n",
        "#hdf[\"diagnosis\"] = hdf[\"diagnosis\"].asfactor()\n",
        "hy = \"diagnosis\"\n",
        "hx = list(df.columns)\n",
        "hx.remove(hy)\n",
        "hdf  = df.copy()\n",
        "hdf.iloc[:,1:] = StandardScaler().fit_transform(hdf.iloc[:,1:])\n",
        "hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n",
        "hdf.iloc[:,0] = hdf.iloc[:,0].astype('category')\n",
        "train1, valid1 = train_test_split(hdf, test_size=0.2,random_state=123)\n",
        "train = h2o.H2OFrame(train1)\n",
        "valid = h2o.H2OFrame(valid1)\n",
        "train[\"diagnosis\"] = train[\"diagnosis\"].asfactor()\n",
        "valid[\"diagnosis\"] = valid[\"diagnosis\"].asfactor()"
      ],
      "metadata": {
        "id": "NVoFNbYCGxGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b3a1886-bbc7-45a6-ddad-5f26c80c1548"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-460708a37676>:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\",\n",
        "                nfolds=10, sort_metric='accuracy')"
      ],
      "metadata": {
        "id": "7JNO6XnVHH5P"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aml.train(x = hx, y = hy, training_frame = train,\n",
        "          validation_frame = valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lbo606kFH4Zc",
        "outputId": "e36cc666-1599-4db5-cf6c-e02927fba70a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoML progress: |\n",
            "20:46:04.532: Project: AutoML_1_20230622_204604\n",
            "20:46:04.532: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "20:46:04.535: Setting stopping tolerance adaptively based on the training frame: 0.011180339887498949\n",
            "20:46:04.535: Build control seed: 123\n",
            "20:46:04.536: training frame: Frame key: AutoML_1_20230622_204604_training_py_1_sid_8d46    cols: 31    rows: 8000  chunks: 2    size: 1926169  checksum: -5743320669190601360\n",
            "20:46:04.537: validation frame: Frame key: py_2_sid_8d46    cols: 31    rows: 2000  chunks: 1    size: 482912  checksum: 7051888715556309816\n",
            "20:46:04.537: leaderboard frame: NULL\n",
            "20:46:04.537: blending frame: NULL\n",
            "20:46:04.537: response column: diagnosis\n",
            "20:46:04.537: fold column: null\n",
            "20:46:04.537: weights column: null\n",
            "20:46:04.564: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "20:46:04.584: AutoML job created: 2023.06.22 20:46:04.507\n",
            "20:46:04.585: AutoML build started: 2023.06.22 20:46:04.585\n",
            "20:46:04.606: AutoML: starting XGBoost_1_AutoML_1_20230622_204604 model training\n",
            "\n",
            "██\n",
            "20:46:22.626: New leader: XGBoost_1_AutoML_1_20230622_204604, accuracy: 0.731375\n",
            "20:46:22.630: AutoML: starting GLM_1_AutoML_1_20230622_204604 model training\n",
            "\n",
            "██\n",
            "20:46:32.276: AutoML: starting GBM_1_AutoML_1_20230622_204604 model training\n",
            "\n",
            "█\n",
            "20:46:49.171: AutoML: starting XGBoost_2_AutoML_1_20230622_204604 model training\n",
            "\n",
            "\n",
            "20:47:01.530: AutoML: starting DRF_1_AutoML_1_20230622_204604 model training\n",
            "\n",
            "███\n",
            "20:47:25.586: AutoML: starting GBM_2_AutoML_1_20230622_204604 model training\n",
            "\n",
            "\n",
            "20:47:34.432: AutoML: starting GBM_3_AutoML_1_20230622_204604 model training\n",
            "\n",
            "█\n",
            "20:47:43.417: AutoML: starting GBM_4_AutoML_1_20230622_204604 model training\n",
            "\n",
            "██\n",
            "20:47:54.231: AutoML: starting XGBoost_3_AutoML_1_20230622_204604 model training\n",
            "\n",
            "\n",
            "20:48:00.587: AutoML: starting XRT_1_AutoML_1_20230622_204604 model training\n",
            "\n",
            "███\n",
            "20:48:27.992: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "20:48:27.999: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_1_20230622_204604 model training\n",
            "\n",
            "██\n",
            "20:48:34.55: AutoML: starting StackedEnsemble_AllModels_1_AutoML_1_20230622_204604 model training\n",
            "\n",
            "███████████████████████████████████████████████| (done) 100%\n",
            "\n",
            "20:48:42.309: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "20:48:42.309: AutoML build stopped: 2023.06.22 20:48:42.309\n",
            "20:48:42.309: AutoML build done: built 10 models\n",
            "20:48:42.310: AutoML duration:  2 min 37.724 sec\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230622_204604\n",
              "\n",
              "\n",
              "Model Summary: \n",
              "    number_of_trees\n",
              "--  -----------------\n",
              "    30\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.0862762318888866\n",
              "RMSE: 0.2937281598500331\n",
              "LogLoss: 0.2961195502729631\n",
              "Mean Per-Class Error: 0.10560430092631945\n",
              "AUC: 0.9574856020131994\n",
              "AUCPR: 0.9379440756884645\n",
              "Gini: 0.9149712040263989\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40771300310180303\n",
              "       0     1     Error    Rate\n",
              "-----  ----  ----  -------  --------------\n",
              "0      4317  551   0.1132   (551.0/4868.0)\n",
              "1      307   2825  0.098    (307.0/3132.0)\n",
              "Total  4624  3376  0.1072   (858.0/8000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.407713     0.868162  213\n",
              "max f2                       0.291031     0.900738  254\n",
              "max f0point5                 0.608403     0.885993  148\n",
              "max accuracy                 0.44304      0.894625  201\n",
              "max precision                0.993199     1         0\n",
              "max recall                   0.0188069    1         390\n",
              "max specificity              0.993199     1         0\n",
              "max absolute_mcc             0.426081     0.780124  207\n",
              "max min_per_class_accuracy   0.419471     0.89272   209\n",
              "max mean_per_class_accuracy  0.383103     0.894835  222\n",
              "max tns                      0.993199     4868      0\n",
              "max fns                      0.993199     3120      0\n",
              "max fps                      0.0052119    4868      399\n",
              "max tps                      0.0188069    3132      390\n",
              "max tnr                      0.993199     1         0\n",
              "max fnr                      0.993199     0.996169  0\n",
              "max fpr                      0.0052119    1         399\n",
              "max tpr                      0.0188069    1         390\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 39.32 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.983273           2.55428    2.55428            1                0.988143   1                           0.988143            0.0255428       0.0255428                  155.428   155.428            0.0255428\n",
              "2        0.02                        0.974009           2.55428    2.55428            1                0.978908   1                           0.983526            0.0255428       0.0510856                  155.428   155.428            0.0510856\n",
              "3        0.03                        0.965              2.52235    2.54364            0.9875           0.969758   0.995833                    0.978936            0.0252235       0.0763091                  152.235   154.364            0.0761036\n",
              "4        0.04                        0.956618           2.55428    2.5463             1                0.960834   0.996875                    0.974411            0.0255428       0.101852                   155.428   154.63             0.101646\n",
              "5        0.05                        0.948257           2.49042    2.53512            0.975            0.952464   0.9925                      0.970021            0.0249042       0.126756                   149.042   153.512            0.12614\n",
              "6        0.1                         0.891808           2.52235    2.52874            0.9875           0.92015    0.99                        0.945086            0.126117        0.252874                   152.235   152.874            0.25123\n",
              "7        0.15                        0.834331           2.49042    2.51596            0.975            0.864595   0.985                       0.918255            0.124521        0.377395                   149.042   151.596            0.373697\n",
              "8        0.2                         0.778986           2.40741    2.48883            0.9425           0.807305   0.974375                    0.890518            0.12037         0.497765                   140.741   148.883            0.489343\n",
              "9        0.3                         0.626419           2.20626    2.39464            0.86375          0.704981   0.9375                      0.828672            0.220626        0.718391                   120.626   139.464            0.687577\n",
              "10       0.4                         0.439384           1.57727    2.19029            0.6175           0.53251    0.8575                      0.754632            0.157727        0.876117                   57.7267   119.029            0.782445\n",
              "11       0.5                         0.291733           0.740741   1.90038            0.29             0.362017   0.744                       0.676109            0.0740741       0.950192                   -25.9259  90.0383            0.739838\n",
              "12       0.6                         0.186727           0.322478   1.6374             0.12625          0.235522   0.641042                    0.602678            0.0322478       0.982439                   -67.7522  63.7399            0.628495\n",
              "13       0.7                         0.118291           0.0957854  1.41717            0.0375           0.14983    0.554821                    0.537985            0.00957854      0.992018                   -90.4215  41.7168            0.479898\n",
              "14       0.8                         0.0704665          0.0510856  1.24641            0.02             0.0932805  0.487969                    0.482397            0.00510856      0.997126                   -94.8914  24.6408            0.323955\n",
              "15       0.9                         0.0350014          0.0159642  1.10969            0.00625          0.0520768  0.434444                    0.434584            0.00159642      0.998723                   -98.4036  10.9692            0.16224\n",
              "16       1                           0.00378296         0.0127714  1                  0.005            0.0206824  0.3915                      0.393194            0.00127714      1                          -98.7229  0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.18840019637319533\n",
              "RMSE: 0.4340509144941355\n",
              "LogLoss: 0.5694655292500879\n",
              "Mean Per-Class Error: 0.29928818116277384\n",
              "AUC: 0.7805786689418005\n",
              "AUCPR: 0.7132335417937792\n",
              "Gini: 0.561157337883601\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24615386625130972\n",
              "       0    1     Error    Rate\n",
              "-----  ---  ----  -------  --------------\n",
              "0      731  486   0.3993   (486.0/1217.0)\n",
              "1      156  627   0.1992   (156.0/783.0)\n",
              "Total  887  1113  0.321    (642.0/2000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.246154     0.661392  271\n",
              "max f2                       0.0822697    0.783171  350\n",
              "max f0point5                 0.628715     0.680179  134\n",
              "max accuracy                 0.628715     0.7385    134\n",
              "max precision                0.996239     1         0\n",
              "max recall                   0.00914458   1         397\n",
              "max specificity              0.996239     1         0\n",
              "max absolute_mcc             0.628715     0.435211  134\n",
              "max min_per_class_accuracy   0.354121     0.699872  226\n",
              "max mean_per_class_accuracy  0.552612     0.706514  157\n",
              "max tns                      0.996239     1217      0\n",
              "max fns                      0.996239     782       0\n",
              "max fps                      0.00487721   1217      399\n",
              "max tps                      0.00914458   783       397\n",
              "max tnr                      0.996239     1         0\n",
              "max fnr                      0.996239     0.998723  0\n",
              "max fpr                      0.00487721   1         399\n",
              "max tpr                      0.00914458   1         397\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.46 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.977798           2.55428   2.55428            1                0.985284   1                           0.985284            0.0255428       0.0255428                  155.428   155.428            0.0255428\n",
              "2        0.02                        0.964573           2.55428   2.55428            1                0.972878   1                           0.979081            0.0255428       0.0510856                  155.428   155.428            0.0510856\n",
              "3        0.03                        0.954787           2.29885   2.46914            0.9              0.958362   0.966667                    0.972175            0.0229885       0.0740741                  129.885   146.914            0.0724307\n",
              "4        0.04                        0.943778           2.17114   2.39464            0.85             0.950531   0.9375                      0.966764            0.0217114       0.0957854                  117.114   139.464            0.091677\n",
              "5        0.05                        0.928851           2.04342   2.32439            0.8              0.935592   0.91                        0.960529            0.0204342       0.11622                    104.342   132.439            0.108824\n",
              "6        0.1                         0.874916           1.94125   2.13282            0.76             0.902028   0.835                       0.931279            0.0970626       0.213282                   94.1252   113.282            0.186166\n",
              "7        0.15                        0.800584           1.96679   2.07748            0.77             0.837897   0.813333                    0.900151            0.0983397       0.311622                   96.6794   107.748            0.265607\n",
              "8        0.2                         0.727813           1.76245   1.99872            0.69             0.76954    0.7825                      0.867499            0.0881226       0.399745                   76.2452   99.8723            0.328257\n",
              "9        0.3                         0.572579           1.45594   1.81779            0.57             0.649716   0.711667                    0.794904            0.145594        0.545338                   45.5939   81.7795            0.403186\n",
              "10       0.4                         0.427932           0.970626  1.606              0.38             0.50131    0.62875                     0.721506            0.0970626       0.642401                   -2.93742  60.6003            0.398358\n",
              "11       0.5                         0.303219           0.91954   1.46871            0.36             0.363956   0.575                       0.649996            0.091954        0.734355                   -8.04598  46.871             0.385136\n",
              "12       0.6                         0.208844           0.881226  1.3708             0.345            0.252344   0.536667                    0.58372             0.0881226       0.822478                   -11.8774  37.0796            0.365617\n",
              "13       0.7                         0.130075           0.702427  1.27531            0.275            0.165416   0.499286                    0.523963            0.0702427       0.89272                    -29.7573  27.5315            0.316714\n",
              "14       0.8                         0.0777253          0.523627  1.18135            0.205            0.10211    0.4625                      0.471231            0.0523627       0.945083                   -47.6373  18.1354            0.238427\n",
              "15       0.9                         0.0354974          0.332056  1.08699            0.13             0.0543392  0.425556                    0.42491             0.0332056       0.978289                   -66.7944  8.69874            0.128658\n",
              "16       1                           0.00410937         0.217114  1                  0.085            0.0213709  0.3915                      0.384556            0.0217114       1                          -78.2886  0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.18615605059426837\n",
              "RMSE: 0.43145805195206216\n",
              "LogLoss: 0.5613540167599764\n",
              "Mean Per-Class Error: 0.28157062936622623\n",
              "AUC: 0.7899126007045779\n",
              "AUCPR: 0.704715036081932\n",
              "Gini: 0.5798252014091558\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3172192946076393\n",
              "       0     1     Error    Rate\n",
              "-----  ----  ----  -------  ---------------\n",
              "0      3297  1571  0.3227   (1571.0/4868.0)\n",
              "1      753   2379  0.2404   (753.0/3132.0)\n",
              "Total  4050  3950  0.2905   (2324.0/8000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.317219     0.671844  248\n",
              "max f2                       0.0903366    0.790684  347\n",
              "max f0point5                 0.562613     0.661942  158\n",
              "max accuracy                 0.548578     0.731375  163\n",
              "max precision                0.984683     0.941176  2\n",
              "max recall                   0.00676566   1         398\n",
              "max specificity              0.991555     0.999795  0\n",
              "max absolute_mcc             0.395143     0.434769  217\n",
              "max min_per_class_accuracy   0.369361     0.71871   227\n",
              "max mean_per_class_accuracy  0.395143     0.720713  217\n",
              "max tns                      0.991555     4867      0\n",
              "max fns                      0.991555     3120      0\n",
              "max fps                      0.00360703   4868      399\n",
              "max tps                      0.00676566   3132      398\n",
              "max tnr                      0.991555     0.999795  0\n",
              "max fnr                      0.991555     0.996169  0\n",
              "max fpr                      0.00360703   1         399\n",
              "max tpr                      0.00676566   1         398\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.96 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.976782           2.29885   2.29885            0.9              0.98354    0.9                         0.98354             0.0229885       0.0229885                  129.885   129.885            0.0213451\n",
              "2        0.02                        0.96877            2.36271   2.33078            0.925            0.972395   0.9125                      0.977967            0.0236271       0.0466156                  136.271   133.078            0.0437397\n",
              "3        0.03                        0.958395           2.26692   2.30949            0.8875           0.963849   0.904167                    0.973261            0.0226692       0.0692848                  126.692   130.949            0.0645601\n",
              "4        0.04                        0.947065           2.26692   2.29885            0.8875           0.952408   0.9                         0.968048            0.0226692       0.091954                   126.692   129.885            0.0853805\n",
              "5        0.05                        0.93624            2.23499   2.28608            0.875            0.941784   0.895                       0.962795            0.0223499       0.114304                   123.499   128.608            0.105676\n",
              "6        0.1                         0.879107           1.94764   2.11686            0.7625           0.908667   0.82875                     0.935731            0.0973819       0.211686                   94.7637   111.686            0.183543\n",
              "7        0.15                        0.814872           1.81354   2.01575            0.71             0.846586   0.789167                    0.906016            0.0906769       0.302363                   81.3538   101.575            0.250391\n",
              "8        0.2                         0.743607           1.67305   1.93008            0.655            0.781333   0.755625                    0.874845            0.0836526       0.386015                   67.3052   93.0077            0.305695\n",
              "9        0.3                         0.588081           1.47829   1.77948            0.57875          0.666473   0.696667                    0.805388            0.147829        0.533844                   47.8289   77.9481            0.384296\n",
              "10       0.4                         0.443764           1.28033   1.65469            0.50125          0.512584   0.647813                    0.732187            0.128033        0.661877                   28.0332   65.4693            0.430365\n",
              "11       0.5                         0.306942           1.01533   1.52682            0.3975           0.37215    0.59775                     0.66018             0.101533        0.76341                    1.53257   52.682             0.432884\n",
              "12       0.6                         0.202824           0.766284  1.40006            0.3              0.251989   0.548125                    0.592148            0.0766284       0.840038                   -23.3716  40.0064            0.394475\n",
              "13       0.7                         0.130317           0.664112  1.29493            0.26             0.163729   0.506964                    0.530945            0.0664112       0.90645                    -33.5888  29.4928            0.339276\n",
              "14       0.8                         0.0758979          0.472542  1.19213            0.185            0.101729   0.466719                    0.477293            0.0472542       0.953704                   -52.7458  19.213             0.252594\n",
              "15       0.9                         0.0371696          0.322478  1.0955             0.12625          0.0557451  0.428889                    0.430455            0.0322478       0.985951                   -67.7522  9.55016            0.141251\n",
              "16       1                           0.00228818         0.140485  1                  0.055            0.0215713  0.3915                      0.389566            0.0140485       1                          -85.9515  0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                         mean      sd         cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid    cv_6_valid    cv_7_valid    cv_8_valid    cv_9_valid    cv_10_valid\n",
              "-----------------------  --------  ---------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  -------------\n",
              "accuracy                 0.70825   0.0371661  0.7275        0.74375       0.74          0.70375       0.61875       0.6975        0.7375        0.705         0.72375       0.685\n",
              "auc                      0.789564  0.0191126  0.791147      0.797844      0.78883       0.801228      0.766344      0.776762      0.831018      0.78962       0.788937      0.763911\n",
              "err                      0.29175   0.0371661  0.2725        0.25625       0.26          0.29625       0.38125       0.3025        0.2625        0.295         0.27625       0.315\n",
              "err_count                233.4     29.7329    218           205           208           237           305           242           210           236           221           252\n",
              "f0point5                 0.629483  0.0312905  0.641696      0.648259      0.655488      0.60241       0.570673      0.637743      0.675287      0.638809      0.629467      0.595\n",
              "f1                       0.679528  0.0206461  0.669697      0.678179      0.673981      0.669456      0.664466      0.689744      0.728682      0.692708      0.674521      0.653846\n",
              "f2                       0.740414  0.0352846  0.700253      0.710994      0.693548      0.753296      0.795155      0.750977      0.791246      0.756542      0.726523      0.72561\n",
              "lift_top_group           2.34294   0.336948   1.96078       2.38095       2.63158       2.73973       2.42424       1.78042       2.08333       2.12121       2.67559       2.63158\n",
              "logloss                  0.561354  0.0306372  0.563873      0.538003      0.554789      0.533056      0.596285      0.5928        0.506254      0.576416      0.552059      0.600005\n",
              "max_per_class_error      0.354375  0.0925364  0.277778      0.265306      0.292763      0.364173      0.589362      0.37581       0.336207      0.365957      0.301397      0.375\n",
              "mcc                      0.437511  0.0391816  0.443246      0.471221      0.459964      0.441607      0.358681      0.419569      0.498854      0.43528       0.450001      0.396688\n",
              "mean_per_class_accuracy  0.721276  0.0247419  0.726496      0.741853      0.733659      0.728872      0.662895      0.711205      0.751539      0.720052      0.732245      0.703947\n",
              "mean_per_class_error     0.278724  0.0247419  0.273504      0.258147      0.266341      0.271128      0.337105      0.288795      0.248461      0.279948      0.267755      0.296053\n",
              "mse                      0.186156  0.0112054  0.185587      0.175314      0.181778      0.177739      0.200074      0.198868      0.167041      0.191238      0.184604      0.199318\n",
              "pr_auc                   0.706007  0.0296863  0.684173      0.702552      0.70519       0.707094      0.706578      0.687794      0.782989      0.702086      0.709587      0.672029\n",
              "precision                0.600636  0.0398683  0.624294      0.629738      0.643713      0.564706      0.521589      0.607223      0.643836      0.607306      0.602632      0.561321\n",
              "r2                       0.21708   0.0442381  0.214258      0.245779      0.228448      0.233138      0.174422      0.184293      0.314283      0.210882      0.2113        0.153998\n",
              "recall                   0.789357  0.0619735  0.722222      0.734694      0.707237      0.821918      0.915152      0.79822       0.839286      0.806061      0.765886      0.782895\n",
              "rmse                     0.431281  0.0130248  0.430798      0.418705      0.426354      0.421592      0.447296      0.445946      0.408706      0.437308      0.429656      0.446451\n",
              "specificity              0.653196  0.0999643  0.730769      0.749012      0.760081      0.635827      0.410638      0.62419       0.663793      0.634043      0.698603      0.625\n",
              "\n",
              "Scoring History: \n",
              "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
              "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
              "    2023-06-22 20:46:20  16.375 sec  0                  0.5              0.693147            0.5             0.3915             1                0.6085                           0.5                0.693147              0.5               0.3915               1                  0.6085\n",
              "    2023-06-22 20:46:21  16.595 sec  5                  0.387143         0.469449            0.8686          0.815761           2.52235          0.211                            0.420583           0.532308              0.797085          0.718904             2.17114            0.2715\n",
              "    2023-06-22 20:46:21  16.828 sec  10                 0.35995          0.410703            0.896726        0.85392            2.55428          0.182                            0.420456           0.531574              0.795512          0.722968             2.42656            0.2735\n",
              "    2023-06-22 20:46:21  17.066 sec  15                 0.340863         0.374872            0.917331        0.881451           2.55428          0.15525                          0.425685           0.546093              0.788397          0.717152             2.42656            0.3105\n",
              "    2023-06-22 20:46:21  17.307 sec  20                 0.322802         0.343603            0.934676        0.906835           2.55428          0.135625                         0.428904           0.556092              0.784902          0.714457             2.55428            0.2845\n",
              "    2023-06-22 20:46:22  17.557 sec  25                 0.30702          0.31811             0.947853        0.924756           2.55428          0.118625                         0.431961           0.564559              0.781737          0.712846             2.55428            0.2935\n",
              "    2023-06-22 20:46:22  17.809 sec  30                 0.293728         0.29612             0.957486        0.937944           2.55428          0.10725                          0.434051           0.569466              0.780579          0.713234             2.55428            0.321\n",
              "\n",
              "Variable Importances: \n",
              "variable                 relative_importance    scaled_importance    percentage\n",
              "-----------------------  ---------------------  -------------------  --------------------\n",
              "area_se                  1904.8582763671875     1.0                  0.17710163529899145\n",
              "perimeter_worst          670.3985595703125      0.35194143726474486  0.06232940406906372\n",
              "smoothness_worst         490.8770751953125      0.25769742625235037  0.0456386356016325\n",
              "texture_worst            471.06378173828125     0.24729597344987847  0.04379652130082945\n",
              "radius_se                406.78167724609375     0.2135495759935901   0.03781997912587105\n",
              "symmetry_se              383.16064453125        0.2011491612184331   0.03562384539080497\n",
              "area_worst               379.52288818359375     0.19923943575865039  0.03528562988890534\n",
              "texture_se               376.7027893066406      0.19775895875312144  0.035023434990203606\n",
              "radius_worst             356.6051025390625      0.18720820701640586  0.03315487960399761\n",
              "symmetry_mean            341.8689880371094      0.17947213830999437  0.031784809185306774\n",
              "---                      ---                    ---                  ---\n",
              "compactness_mean         257.6020812988281      0.13523425049243495  0.02395020691064367\n",
              "smoothness_se            247.90921020507812     0.13014575062133926  0.023049025262253914\n",
              "fractal_dimension_worst  239.37608337402344     0.12566608568409865  0.02225566927627705\n",
              "perimeter_mean           235.8315887451172      0.1238053201495277   0.021926124657196535\n",
              "fractal_dimension_mean   229.67420959472656     0.12057285964221189  0.021353650615311495\n",
              "compactness_worst        214.68325805664062     0.11270300826057754  0.019959887066064025\n",
              "concave points_mean      202.2340850830078      0.10616752311289768  0.01880244195893765\n",
              "radius_mean              184.11338806152344     0.09665463848189883  0.01711769453437711\n",
              "area_mean                175.94384765625        0.09236584676094528  0.01635814250713946\n",
              "compactness_se           169.79820251464844     0.08913954629657578  0.015786759418933725\n",
              "[30 rows x 4 columns]\n",
              "\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ],
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230622_204604\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-2.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-2 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-2 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-2 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table th,\n",
              "#h2o-table-2 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>number_of_trees</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>30.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.0862762318888866\n",
              "RMSE: 0.2937281598500331\n",
              "LogLoss: 0.2961195502729631\n",
              "Mean Per-Class Error: 0.10560430092631945\n",
              "AUC: 0.9574856020131994\n",
              "AUCPR: 0.9379440756884645\n",
              "Gini: 0.9149712040263989</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-3.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-3 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-3 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-3 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table th,\n",
              "#h2o-table-3 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40771300310180303</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>4317.0</td>\n",
              "<td>551.0</td>\n",
              "<td>0.1132</td>\n",
              "<td> (551.0/4868.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>307.0</td>\n",
              "<td>2825.0</td>\n",
              "<td>0.098</td>\n",
              "<td> (307.0/3132.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>4624.0</td>\n",
              "<td>3376.0</td>\n",
              "<td>0.1072</td>\n",
              "<td> (858.0/8000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-4.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-4 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-4 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-4 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table th,\n",
              "#h2o-table-4 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-4 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-4\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4077130</td>\n",
              "<td>0.8681623</td>\n",
              "<td>213.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.2910308</td>\n",
              "<td>0.9007375</td>\n",
              "<td>254.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6084025</td>\n",
              "<td>0.8859932</td>\n",
              "<td>148.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.4430403</td>\n",
              "<td>0.894625</td>\n",
              "<td>201.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0188069</td>\n",
              "<td>1.0</td>\n",
              "<td>390.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4260813</td>\n",
              "<td>0.7801245</td>\n",
              "<td>207.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.4194713</td>\n",
              "<td>0.8927203</td>\n",
              "<td>209.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3831034</td>\n",
              "<td>0.8948353</td>\n",
              "<td>222.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9931994</td>\n",
              "<td>4868.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9931994</td>\n",
              "<td>3120.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0052119</td>\n",
              "<td>4868.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0188069</td>\n",
              "<td>3132.0</td>\n",
              "<td>390.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9931994</td>\n",
              "<td>0.9961686</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0052119</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0188069</td>\n",
              "<td>1.0</td>\n",
              "<td>390.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-5.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-5 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-5 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-5 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table th,\n",
              "#h2o-table-5 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 39.32 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9832729</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9881432</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9881432</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0255428</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0255428</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9740088</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9789083</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9835258</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0510856</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0510856</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9650004</td>\n",
              "<td>2.5223499</td>\n",
              "<td>2.5436356</td>\n",
              "<td>0.9875</td>\n",
              "<td>0.9697575</td>\n",
              "<td>0.9958333</td>\n",
              "<td>0.9789364</td>\n",
              "<td>0.0252235</td>\n",
              "<td>0.0763091</td>\n",
              "<td>152.2349936</td>\n",
              "<td>154.3635590</td>\n",
              "<td>0.0761036</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9566181</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5462963</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9608343</td>\n",
              "<td>0.996875</td>\n",
              "<td>0.9744108</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.1018519</td>\n",
              "<td>155.4278416</td>\n",
              "<td>154.6296296</td>\n",
              "<td>0.1016464</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9482571</td>\n",
              "<td>2.4904215</td>\n",
              "<td>2.5351213</td>\n",
              "<td>0.975</td>\n",
              "<td>0.9524639</td>\n",
              "<td>0.9925</td>\n",
              "<td>0.9700214</td>\n",
              "<td>0.0249042</td>\n",
              "<td>0.1267561</td>\n",
              "<td>149.0421456</td>\n",
              "<td>153.5121328</td>\n",
              "<td>0.1261398</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8918075</td>\n",
              "<td>2.5223499</td>\n",
              "<td>2.5287356</td>\n",
              "<td>0.9875</td>\n",
              "<td>0.9201499</td>\n",
              "<td>0.99</td>\n",
              "<td>0.9450857</td>\n",
              "<td>0.1261175</td>\n",
              "<td>0.2528736</td>\n",
              "<td>152.2349936</td>\n",
              "<td>152.8735632</td>\n",
              "<td>0.2512302</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8343306</td>\n",
              "<td>2.4904215</td>\n",
              "<td>2.5159642</td>\n",
              "<td>0.975</td>\n",
              "<td>0.8645949</td>\n",
              "<td>0.985</td>\n",
              "<td>0.9182554</td>\n",
              "<td>0.1245211</td>\n",
              "<td>0.3773946</td>\n",
              "<td>149.0421456</td>\n",
              "<td>151.5964240</td>\n",
              "<td>0.3736970</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7789860</td>\n",
              "<td>2.4074074</td>\n",
              "<td>2.4888250</td>\n",
              "<td>0.9425</td>\n",
              "<td>0.8073055</td>\n",
              "<td>0.974375</td>\n",
              "<td>0.8905179</td>\n",
              "<td>0.1203704</td>\n",
              "<td>0.4977650</td>\n",
              "<td>140.7407407</td>\n",
              "<td>148.8825032</td>\n",
              "<td>0.4893427</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.6264191</td>\n",
              "<td>2.2062580</td>\n",
              "<td>2.3946360</td>\n",
              "<td>0.86375</td>\n",
              "<td>0.7049811</td>\n",
              "<td>0.9375</td>\n",
              "<td>0.8286723</td>\n",
              "<td>0.2206258</td>\n",
              "<td>0.7183908</td>\n",
              "<td>120.6257982</td>\n",
              "<td>139.4636015</td>\n",
              "<td>0.6875773</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4393838</td>\n",
              "<td>1.5772669</td>\n",
              "<td>2.1902937</td>\n",
              "<td>0.6175</td>\n",
              "<td>0.5325101</td>\n",
              "<td>0.8575</td>\n",
              "<td>0.7546318</td>\n",
              "<td>0.1577267</td>\n",
              "<td>0.8761175</td>\n",
              "<td>57.7266922</td>\n",
              "<td>119.0293742</td>\n",
              "<td>0.7824445</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.2917330</td>\n",
              "<td>0.7407407</td>\n",
              "<td>1.9003831</td>\n",
              "<td>0.29</td>\n",
              "<td>0.3620169</td>\n",
              "<td>0.744</td>\n",
              "<td>0.6761088</td>\n",
              "<td>0.0740741</td>\n",
              "<td>0.9501916</td>\n",
              "<td>-25.9259259</td>\n",
              "<td>90.0383142</td>\n",
              "<td>0.7398382</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.1867268</td>\n",
              "<td>0.3224777</td>\n",
              "<td>1.6373989</td>\n",
              "<td>0.12625</td>\n",
              "<td>0.2355222</td>\n",
              "<td>0.6410417</td>\n",
              "<td>0.6026777</td>\n",
              "<td>0.0322478</td>\n",
              "<td>0.9824393</td>\n",
              "<td>-67.7522350</td>\n",
              "<td>63.7398893</td>\n",
              "<td>0.6284952</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1182911</td>\n",
              "<td>0.0957854</td>\n",
              "<td>1.4171684</td>\n",
              "<td>0.0375</td>\n",
              "<td>0.1498300</td>\n",
              "<td>0.5548214</td>\n",
              "<td>0.5379852</td>\n",
              "<td>0.0095785</td>\n",
              "<td>0.9920179</td>\n",
              "<td>-90.4214559</td>\n",
              "<td>41.7168400</td>\n",
              "<td>0.4798979</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0704665</td>\n",
              "<td>0.0510856</td>\n",
              "<td>1.2464080</td>\n",
              "<td>0.02</td>\n",
              "<td>0.0932805</td>\n",
              "<td>0.4879688</td>\n",
              "<td>0.4823971</td>\n",
              "<td>0.0051086</td>\n",
              "<td>0.9971264</td>\n",
              "<td>-94.8914432</td>\n",
              "<td>24.6408046</td>\n",
              "<td>0.3239547</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0350014</td>\n",
              "<td>0.0159642</td>\n",
              "<td>1.1096921</td>\n",
              "<td>0.00625</td>\n",
              "<td>0.0520768</td>\n",
              "<td>0.4344444</td>\n",
              "<td>0.4345837</td>\n",
              "<td>0.0015964</td>\n",
              "<td>0.9987229</td>\n",
              "<td>-98.4035760</td>\n",
              "<td>10.9692068</td>\n",
              "<td>0.1622397</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0037830</td>\n",
              "<td>0.0127714</td>\n",
              "<td>1.0</td>\n",
              "<td>0.005</td>\n",
              "<td>0.0206824</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3931936</td>\n",
              "<td>0.0012771</td>\n",
              "<td>1.0</td>\n",
              "<td>-98.7228608</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.18840019637319533\n",
              "RMSE: 0.4340509144941355\n",
              "LogLoss: 0.5694655292500879\n",
              "Mean Per-Class Error: 0.29928818116277384\n",
              "AUC: 0.7805786689418005\n",
              "AUCPR: 0.7132335417937792\n",
              "Gini: 0.561157337883601</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-6.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-6 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-6 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-6 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table th,\n",
              "#h2o-table-6 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24615386625130972</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>731.0</td>\n",
              "<td>486.0</td>\n",
              "<td>0.3993</td>\n",
              "<td> (486.0/1217.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>156.0</td>\n",
              "<td>627.0</td>\n",
              "<td>0.1992</td>\n",
              "<td> (156.0/783.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>887.0</td>\n",
              "<td>1113.0</td>\n",
              "<td>0.321</td>\n",
              "<td> (642.0/2000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-7.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-7 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-7 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-7 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table th,\n",
              "#h2o-table-7 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-7 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-7\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.2461539</td>\n",
              "<td>0.6613924</td>\n",
              "<td>271.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.0822697</td>\n",
              "<td>0.7831708</td>\n",
              "<td>350.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.6801786</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.7385</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0091446</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.4352113</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3541214</td>\n",
              "<td>0.6998723</td>\n",
              "<td>226.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.5526123</td>\n",
              "<td>0.7065135</td>\n",
              "<td>157.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1217.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9962388</td>\n",
              "<td>782.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0048772</td>\n",
              "<td>1217.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0091446</td>\n",
              "<td>783.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9962388</td>\n",
              "<td>0.9987229</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0048772</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0091446</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-8.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-8 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-8 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-8 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table th,\n",
              "#h2o-table-8 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-8 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-8\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.46 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9777978</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9852836</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9852836</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0255428</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0255428</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9645726</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9728781</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9790808</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0510856</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0510856</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9547869</td>\n",
              "<td>2.2988506</td>\n",
              "<td>2.4691358</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9583625</td>\n",
              "<td>0.9666667</td>\n",
              "<td>0.9721747</td>\n",
              "<td>0.0229885</td>\n",
              "<td>0.0740741</td>\n",
              "<td>129.8850575</td>\n",
              "<td>146.9135802</td>\n",
              "<td>0.0724307</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9437785</td>\n",
              "<td>2.1711367</td>\n",
              "<td>2.3946360</td>\n",
              "<td>0.85</td>\n",
              "<td>0.9505307</td>\n",
              "<td>0.9375</td>\n",
              "<td>0.9667637</td>\n",
              "<td>0.0217114</td>\n",
              "<td>0.0957854</td>\n",
              "<td>117.1136654</td>\n",
              "<td>139.4636015</td>\n",
              "<td>0.0916770</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9288514</td>\n",
              "<td>2.0434227</td>\n",
              "<td>2.3243934</td>\n",
              "<td>0.8</td>\n",
              "<td>0.9355922</td>\n",
              "<td>0.91</td>\n",
              "<td>0.9605294</td>\n",
              "<td>0.0204342</td>\n",
              "<td>0.1162197</td>\n",
              "<td>104.3422733</td>\n",
              "<td>132.4393359</td>\n",
              "<td>0.1088244</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8749162</td>\n",
              "<td>1.9412516</td>\n",
              "<td>2.1328225</td>\n",
              "<td>0.76</td>\n",
              "<td>0.9020278</td>\n",
              "<td>0.835</td>\n",
              "<td>0.9312786</td>\n",
              "<td>0.0970626</td>\n",
              "<td>0.2132822</td>\n",
              "<td>94.1251596</td>\n",
              "<td>113.2822478</td>\n",
              "<td>0.1861664</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8005845</td>\n",
              "<td>1.9667944</td>\n",
              "<td>2.0774798</td>\n",
              "<td>0.77</td>\n",
              "<td>0.8378969</td>\n",
              "<td>0.8133333</td>\n",
              "<td>0.9001514</td>\n",
              "<td>0.0983397</td>\n",
              "<td>0.3116220</td>\n",
              "<td>96.6794381</td>\n",
              "<td>107.7479779</td>\n",
              "<td>0.2656072</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7278127</td>\n",
              "<td>1.7624521</td>\n",
              "<td>1.9987229</td>\n",
              "<td>0.69</td>\n",
              "<td>0.7695403</td>\n",
              "<td>0.7825</td>\n",
              "<td>0.8674986</td>\n",
              "<td>0.0881226</td>\n",
              "<td>0.3997446</td>\n",
              "<td>76.2452107</td>\n",
              "<td>99.8722861</td>\n",
              "<td>0.3282573</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.5725785</td>\n",
              "<td>1.4559387</td>\n",
              "<td>1.8177948</td>\n",
              "<td>0.57</td>\n",
              "<td>0.6497156</td>\n",
              "<td>0.7116667</td>\n",
              "<td>0.7949043</td>\n",
              "<td>0.1455939</td>\n",
              "<td>0.5453384</td>\n",
              "<td>45.5938697</td>\n",
              "<td>81.7794806</td>\n",
              "<td>0.4031856</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4279322</td>\n",
              "<td>0.9706258</td>\n",
              "<td>1.6060026</td>\n",
              "<td>0.38</td>\n",
              "<td>0.5013098</td>\n",
              "<td>0.62875</td>\n",
              "<td>0.7215057</td>\n",
              "<td>0.0970626</td>\n",
              "<td>0.6424010</td>\n",
              "<td>-2.9374202</td>\n",
              "<td>60.6002554</td>\n",
              "<td>0.3983583</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3032194</td>\n",
              "<td>0.9195402</td>\n",
              "<td>1.4687101</td>\n",
              "<td>0.36</td>\n",
              "<td>0.3639555</td>\n",
              "<td>0.575</td>\n",
              "<td>0.6499956</td>\n",
              "<td>0.0919540</td>\n",
              "<td>0.7343550</td>\n",
              "<td>-8.0459770</td>\n",
              "<td>46.8710089</td>\n",
              "<td>0.3851357</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.2088440</td>\n",
              "<td>0.8812261</td>\n",
              "<td>1.3707961</td>\n",
              "<td>0.345</td>\n",
              "<td>0.2523442</td>\n",
              "<td>0.5366667</td>\n",
              "<td>0.5837204</td>\n",
              "<td>0.0881226</td>\n",
              "<td>0.8224777</td>\n",
              "<td>-11.8773946</td>\n",
              "<td>37.0796083</td>\n",
              "<td>0.3656165</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1300754</td>\n",
              "<td>0.7024266</td>\n",
              "<td>1.2753147</td>\n",
              "<td>0.275</td>\n",
              "<td>0.1654164</td>\n",
              "<td>0.4992857</td>\n",
              "<td>0.5239627</td>\n",
              "<td>0.0702427</td>\n",
              "<td>0.8927203</td>\n",
              "<td>-29.7573436</td>\n",
              "<td>27.5314724</td>\n",
              "<td>0.3167137</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0777253</td>\n",
              "<td>0.5236271</td>\n",
              "<td>1.1813538</td>\n",
              "<td>0.205</td>\n",
              "<td>0.1021104</td>\n",
              "<td>0.4625</td>\n",
              "<td>0.4712311</td>\n",
              "<td>0.0523627</td>\n",
              "<td>0.9450830</td>\n",
              "<td>-47.6372925</td>\n",
              "<td>18.1353768</td>\n",
              "<td>0.2384273</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0354974</td>\n",
              "<td>0.3320562</td>\n",
              "<td>1.0869874</td>\n",
              "<td>0.13</td>\n",
              "<td>0.0543392</td>\n",
              "<td>0.4255556</td>\n",
              "<td>0.4249098</td>\n",
              "<td>0.0332056</td>\n",
              "<td>0.9782886</td>\n",
              "<td>-66.7943806</td>\n",
              "<td>8.6987371</td>\n",
              "<td>0.1286584</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0041094</td>\n",
              "<td>0.2171137</td>\n",
              "<td>1.0</td>\n",
              "<td>0.085</td>\n",
              "<td>0.0213709</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3845559</td>\n",
              "<td>0.0217114</td>\n",
              "<td>1.0</td>\n",
              "<td>-78.2886335</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.18615605059426837\n",
              "RMSE: 0.43145805195206216\n",
              "LogLoss: 0.5613540167599764\n",
              "Mean Per-Class Error: 0.28157062936622623\n",
              "AUC: 0.7899126007045779\n",
              "AUCPR: 0.704715036081932\n",
              "Gini: 0.5798252014091558</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-9.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-9 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-9 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-9 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table th,\n",
              "#h2o-table-9 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-9 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-9\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3172192946076393</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>3297.0</td>\n",
              "<td>1571.0</td>\n",
              "<td>0.3227</td>\n",
              "<td> (1571.0/4868.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>753.0</td>\n",
              "<td>2379.0</td>\n",
              "<td>0.2404</td>\n",
              "<td> (753.0/3132.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>4050.0</td>\n",
              "<td>3950.0</td>\n",
              "<td>0.2905</td>\n",
              "<td> (2324.0/8000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-10.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-10 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-10 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-10 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table th,\n",
              "#h2o-table-10 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-10 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-10\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3172193</td>\n",
              "<td>0.6718441</td>\n",
              "<td>248.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.0903366</td>\n",
              "<td>0.7906840</td>\n",
              "<td>347.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.5626134</td>\n",
              "<td>0.6619421</td>\n",
              "<td>158.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.5485779</td>\n",
              "<td>0.731375</td>\n",
              "<td>163.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9846833</td>\n",
              "<td>0.9411765</td>\n",
              "<td>2.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0067657</td>\n",
              "<td>1.0</td>\n",
              "<td>398.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9997946</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.3951432</td>\n",
              "<td>0.4347693</td>\n",
              "<td>217.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3693605</td>\n",
              "<td>0.7187101</td>\n",
              "<td>227.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3951432</td>\n",
              "<td>0.7207126</td>\n",
              "<td>217.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9915547</td>\n",
              "<td>4867.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9915547</td>\n",
              "<td>3120.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0036070</td>\n",
              "<td>4868.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0067657</td>\n",
              "<td>3132.0</td>\n",
              "<td>398.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9997946</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9961686</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0036070</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0067657</td>\n",
              "<td>1.0</td>\n",
              "<td>398.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-11.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-11 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-11 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-11 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table th,\n",
              "#h2o-table-11 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-11 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-11\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.96 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9767816</td>\n",
              "<td>2.2988506</td>\n",
              "<td>2.2988506</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9835398</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9835398</td>\n",
              "<td>0.0229885</td>\n",
              "<td>0.0229885</td>\n",
              "<td>129.8850575</td>\n",
              "<td>129.8850575</td>\n",
              "<td>0.0213451</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9687700</td>\n",
              "<td>2.3627075</td>\n",
              "<td>2.3307791</td>\n",
              "<td>0.925</td>\n",
              "<td>0.9723946</td>\n",
              "<td>0.9125</td>\n",
              "<td>0.9779672</td>\n",
              "<td>0.0236271</td>\n",
              "<td>0.0466156</td>\n",
              "<td>136.2707535</td>\n",
              "<td>133.0779055</td>\n",
              "<td>0.0437397</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9583953</td>\n",
              "<td>2.2669221</td>\n",
              "<td>2.3094934</td>\n",
              "<td>0.8875</td>\n",
              "<td>0.9638492</td>\n",
              "<td>0.9041667</td>\n",
              "<td>0.9732612</td>\n",
              "<td>0.0226692</td>\n",
              "<td>0.0692848</td>\n",
              "<td>126.6922095</td>\n",
              "<td>130.9493401</td>\n",
              "<td>0.0645601</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9470655</td>\n",
              "<td>2.2669221</td>\n",
              "<td>2.2988506</td>\n",
              "<td>0.8875</td>\n",
              "<td>0.9524081</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9680479</td>\n",
              "<td>0.0226692</td>\n",
              "<td>0.0919540</td>\n",
              "<td>126.6922095</td>\n",
              "<td>129.8850575</td>\n",
              "<td>0.0853805</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9362398</td>\n",
              "<td>2.2349936</td>\n",
              "<td>2.2860792</td>\n",
              "<td>0.875</td>\n",
              "<td>0.9417836</td>\n",
              "<td>0.895</td>\n",
              "<td>0.9627951</td>\n",
              "<td>0.0223499</td>\n",
              "<td>0.1143040</td>\n",
              "<td>123.4993614</td>\n",
              "<td>128.6079183</td>\n",
              "<td>0.1056762</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8791068</td>\n",
              "<td>1.9476373</td>\n",
              "<td>2.1168582</td>\n",
              "<td>0.7625</td>\n",
              "<td>0.9086671</td>\n",
              "<td>0.82875</td>\n",
              "<td>0.9357311</td>\n",
              "<td>0.0973819</td>\n",
              "<td>0.2116858</td>\n",
              "<td>94.7637292</td>\n",
              "<td>111.6858238</td>\n",
              "<td>0.1835428</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8148719</td>\n",
              "<td>1.8135377</td>\n",
              "<td>2.0157514</td>\n",
              "<td>0.71</td>\n",
              "<td>0.8465859</td>\n",
              "<td>0.7891667</td>\n",
              "<td>0.9060160</td>\n",
              "<td>0.0906769</td>\n",
              "<td>0.3023627</td>\n",
              "<td>81.3537676</td>\n",
              "<td>101.5751384</td>\n",
              "<td>0.2503906</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7436068</td>\n",
              "<td>1.6730524</td>\n",
              "<td>1.9300766</td>\n",
              "<td>0.655</td>\n",
              "<td>0.7813334</td>\n",
              "<td>0.755625</td>\n",
              "<td>0.8748454</td>\n",
              "<td>0.0836526</td>\n",
              "<td>0.3860153</td>\n",
              "<td>67.3052363</td>\n",
              "<td>93.0076628</td>\n",
              "<td>0.3056949</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.5880806</td>\n",
              "<td>1.4782886</td>\n",
              "<td>1.7794806</td>\n",
              "<td>0.57875</td>\n",
              "<td>0.6664730</td>\n",
              "<td>0.6966667</td>\n",
              "<td>0.8053879</td>\n",
              "<td>0.1478289</td>\n",
              "<td>0.5338442</td>\n",
              "<td>47.8288633</td>\n",
              "<td>77.9480630</td>\n",
              "<td>0.3842961</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4437644</td>\n",
              "<td>1.2803321</td>\n",
              "<td>1.6546935</td>\n",
              "<td>0.50125</td>\n",
              "<td>0.5125842</td>\n",
              "<td>0.6478125</td>\n",
              "<td>0.7321870</td>\n",
              "<td>0.1280332</td>\n",
              "<td>0.6618774</td>\n",
              "<td>28.0332056</td>\n",
              "<td>65.4693487</td>\n",
              "<td>0.4303655</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3069417</td>\n",
              "<td>1.0153257</td>\n",
              "<td>1.5268199</td>\n",
              "<td>0.3975</td>\n",
              "<td>0.3721498</td>\n",
              "<td>0.59775</td>\n",
              "<td>0.6601796</td>\n",
              "<td>0.1015326</td>\n",
              "<td>0.7634100</td>\n",
              "<td>1.5325670</td>\n",
              "<td>52.6819923</td>\n",
              "<td>0.4328841</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.2028235</td>\n",
              "<td>0.7662835</td>\n",
              "<td>1.4000639</td>\n",
              "<td>0.3</td>\n",
              "<td>0.2519894</td>\n",
              "<td>0.548125</td>\n",
              "<td>0.5921479</td>\n",
              "<td>0.0766284</td>\n",
              "<td>0.8400383</td>\n",
              "<td>-23.3716475</td>\n",
              "<td>40.0063857</td>\n",
              "<td>0.3944755</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1303173</td>\n",
              "<td>0.6641124</td>\n",
              "<td>1.2949279</td>\n",
              "<td>0.26</td>\n",
              "<td>0.1637290</td>\n",
              "<td>0.5069643</td>\n",
              "<td>0.5309452</td>\n",
              "<td>0.0664112</td>\n",
              "<td>0.9064496</td>\n",
              "<td>-33.5887612</td>\n",
              "<td>29.4927933</td>\n",
              "<td>0.3392762</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0758979</td>\n",
              "<td>0.4725415</td>\n",
              "<td>1.1921296</td>\n",
              "<td>0.185</td>\n",
              "<td>0.1017293</td>\n",
              "<td>0.4667188</td>\n",
              "<td>0.4772932</td>\n",
              "<td>0.0472542</td>\n",
              "<td>0.9537037</td>\n",
              "<td>-52.7458493</td>\n",
              "<td>19.2129630</td>\n",
              "<td>0.2525944</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0371696</td>\n",
              "<td>0.3224777</td>\n",
              "<td>1.0955016</td>\n",
              "<td>0.12625</td>\n",
              "<td>0.0557451</td>\n",
              "<td>0.4288889</td>\n",
              "<td>0.4304545</td>\n",
              "<td>0.0322478</td>\n",
              "<td>0.9859515</td>\n",
              "<td>-67.7522350</td>\n",
              "<td>9.5501632</td>\n",
              "<td>0.1412514</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0022882</td>\n",
              "<td>0.1404853</td>\n",
              "<td>1.0</td>\n",
              "<td>0.055</td>\n",
              "<td>0.0215713</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3895662</td>\n",
              "<td>0.0140485</td>\n",
              "<td>1.0</td>\n",
              "<td>-85.9514687</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-12.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-12 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-12 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-12 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table th,\n",
              "#h2o-table-12 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-12 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-12\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th>\n",
              "<th>cv_6_valid</th>\n",
              "<th>cv_7_valid</th>\n",
              "<th>cv_8_valid</th>\n",
              "<th>cv_9_valid</th>\n",
              "<th>cv_10_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.70825</td>\n",
              "<td>0.0371661</td>\n",
              "<td>0.7275</td>\n",
              "<td>0.74375</td>\n",
              "<td>0.74</td>\n",
              "<td>0.70375</td>\n",
              "<td>0.61875</td>\n",
              "<td>0.6975</td>\n",
              "<td>0.7375</td>\n",
              "<td>0.705</td>\n",
              "<td>0.72375</td>\n",
              "<td>0.685</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.7895642</td>\n",
              "<td>0.0191126</td>\n",
              "<td>0.7911474</td>\n",
              "<td>0.7978442</td>\n",
              "<td>0.7888304</td>\n",
              "<td>0.8012283</td>\n",
              "<td>0.7663443</td>\n",
              "<td>0.7767623</td>\n",
              "<td>0.8310178</td>\n",
              "<td>0.7896196</td>\n",
              "<td>0.7889371</td>\n",
              "<td>0.7639107</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.29175</td>\n",
              "<td>0.0371661</td>\n",
              "<td>0.2725</td>\n",
              "<td>0.25625</td>\n",
              "<td>0.26</td>\n",
              "<td>0.29625</td>\n",
              "<td>0.38125</td>\n",
              "<td>0.3025</td>\n",
              "<td>0.2625</td>\n",
              "<td>0.295</td>\n",
              "<td>0.27625</td>\n",
              "<td>0.315</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>233.4</td>\n",
              "<td>29.732885</td>\n",
              "<td>218.0</td>\n",
              "<td>205.0</td>\n",
              "<td>208.0</td>\n",
              "<td>237.0</td>\n",
              "<td>305.0</td>\n",
              "<td>242.0</td>\n",
              "<td>210.0</td>\n",
              "<td>236.0</td>\n",
              "<td>221.0</td>\n",
              "<td>252.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.6294831</td>\n",
              "<td>0.0312905</td>\n",
              "<td>0.6416957</td>\n",
              "<td>0.6482593</td>\n",
              "<td>0.6554878</td>\n",
              "<td>0.6024097</td>\n",
              "<td>0.5706727</td>\n",
              "<td>0.637743</td>\n",
              "<td>0.6752874</td>\n",
              "<td>0.6388088</td>\n",
              "<td>0.6294667</td>\n",
              "<td>0.595</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.6795281</td>\n",
              "<td>0.0206461</td>\n",
              "<td>0.669697</td>\n",
              "<td>0.6781790</td>\n",
              "<td>0.6739812</td>\n",
              "<td>0.6694561</td>\n",
              "<td>0.6644664</td>\n",
              "<td>0.6897436</td>\n",
              "<td>0.7286822</td>\n",
              "<td>0.6927083</td>\n",
              "<td>0.6745213</td>\n",
              "<td>0.6538461</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.7404144</td>\n",
              "<td>0.0352846</td>\n",
              "<td>0.7002535</td>\n",
              "<td>0.7109941</td>\n",
              "<td>0.6935484</td>\n",
              "<td>0.7532957</td>\n",
              "<td>0.7951554</td>\n",
              "<td>0.7509771</td>\n",
              "<td>0.7912458</td>\n",
              "<td>0.7565416</td>\n",
              "<td>0.7265229</td>\n",
              "<td>0.7256098</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.3429408</td>\n",
              "<td>0.3369483</td>\n",
              "<td>1.9607843</td>\n",
              "<td>2.3809524</td>\n",
              "<td>2.631579</td>\n",
              "<td>2.739726</td>\n",
              "<td>2.4242425</td>\n",
              "<td>1.7804154</td>\n",
              "<td>2.0833333</td>\n",
              "<td>2.121212</td>\n",
              "<td>2.6755853</td>\n",
              "<td>2.631579</td></tr>\n",
              "<tr><td>logloss</td>\n",
              "<td>0.5613540</td>\n",
              "<td>0.0306372</td>\n",
              "<td>0.5638731</td>\n",
              "<td>0.5380028</td>\n",
              "<td>0.5547891</td>\n",
              "<td>0.5330562</td>\n",
              "<td>0.5962847</td>\n",
              "<td>0.5928004</td>\n",
              "<td>0.5062535</td>\n",
              "<td>0.576416</td>\n",
              "<td>0.5520594</td>\n",
              "<td>0.600005</td></tr>\n",
              "<tr><td>max_per_class_error</td>\n",
              "<td>0.3543753</td>\n",
              "<td>0.0925364</td>\n",
              "<td>0.2777778</td>\n",
              "<td>0.2653061</td>\n",
              "<td>0.2927631</td>\n",
              "<td>0.3641732</td>\n",
              "<td>0.5893617</td>\n",
              "<td>0.3758099</td>\n",
              "<td>0.3362069</td>\n",
              "<td>0.3659574</td>\n",
              "<td>0.3013972</td>\n",
              "<td>0.375</td></tr>\n",
              "<tr><td>mcc</td>\n",
              "<td>0.4375112</td>\n",
              "<td>0.0391816</td>\n",
              "<td>0.4432463</td>\n",
              "<td>0.4712214</td>\n",
              "<td>0.4599636</td>\n",
              "<td>0.4416075</td>\n",
              "<td>0.3586807</td>\n",
              "<td>0.4195689</td>\n",
              "<td>0.4988538</td>\n",
              "<td>0.4352803</td>\n",
              "<td>0.4500011</td>\n",
              "<td>0.3966878</td></tr>\n",
              "<tr><td>mean_per_class_accuracy</td>\n",
              "<td>0.7212762</td>\n",
              "<td>0.0247419</td>\n",
              "<td>0.7264957</td>\n",
              "<td>0.7418529</td>\n",
              "<td>0.7336587</td>\n",
              "<td>0.7288723</td>\n",
              "<td>0.6628949</td>\n",
              "<td>0.7112048</td>\n",
              "<td>0.7515394</td>\n",
              "<td>0.7200516</td>\n",
              "<td>0.7322445</td>\n",
              "<td>0.7039474</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.2787238</td>\n",
              "<td>0.0247419</td>\n",
              "<td>0.2735043</td>\n",
              "<td>0.2581471</td>\n",
              "<td>0.2663413</td>\n",
              "<td>0.2711277</td>\n",
              "<td>0.3371051</td>\n",
              "<td>0.2887952</td>\n",
              "<td>0.2484606</td>\n",
              "<td>0.2799484</td>\n",
              "<td>0.2677555</td>\n",
              "<td>0.2960526</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.1861560</td>\n",
              "<td>0.0112054</td>\n",
              "<td>0.1855874</td>\n",
              "<td>0.1753139</td>\n",
              "<td>0.1817776</td>\n",
              "<td>0.1777394</td>\n",
              "<td>0.2000736</td>\n",
              "<td>0.1988681</td>\n",
              "<td>0.1670406</td>\n",
              "<td>0.1912379</td>\n",
              "<td>0.1846039</td>\n",
              "<td>0.1993182</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.706007</td>\n",
              "<td>0.0296863</td>\n",
              "<td>0.6841726</td>\n",
              "<td>0.7025523</td>\n",
              "<td>0.7051897</td>\n",
              "<td>0.7070935</td>\n",
              "<td>0.7065780</td>\n",
              "<td>0.6877939</td>\n",
              "<td>0.7829888</td>\n",
              "<td>0.7020859</td>\n",
              "<td>0.7095867</td>\n",
              "<td>0.6720285</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.6006356</td>\n",
              "<td>0.0398683</td>\n",
              "<td>0.6242938</td>\n",
              "<td>0.6297376</td>\n",
              "<td>0.6437126</td>\n",
              "<td>0.5647059</td>\n",
              "<td>0.5215889</td>\n",
              "<td>0.6072234</td>\n",
              "<td>0.6438356</td>\n",
              "<td>0.6073059</td>\n",
              "<td>0.6026316</td>\n",
              "<td>0.5613208</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.2170801</td>\n",
              "<td>0.0442381</td>\n",
              "<td>0.2142580</td>\n",
              "<td>0.2457793</td>\n",
              "<td>0.2284481</td>\n",
              "<td>0.2331382</td>\n",
              "<td>0.1744221</td>\n",
              "<td>0.1842931</td>\n",
              "<td>0.3142833</td>\n",
              "<td>0.2108816</td>\n",
              "<td>0.2113000</td>\n",
              "<td>0.1539976</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.7893569</td>\n",
              "<td>0.0619735</td>\n",
              "<td>0.7222222</td>\n",
              "<td>0.7346939</td>\n",
              "<td>0.7072368</td>\n",
              "<td>0.8219178</td>\n",
              "<td>0.9151515</td>\n",
              "<td>0.7982196</td>\n",
              "<td>0.8392857</td>\n",
              "<td>0.8060606</td>\n",
              "<td>0.7658863</td>\n",
              "<td>0.7828947</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.4312811</td>\n",
              "<td>0.0130248</td>\n",
              "<td>0.4307985</td>\n",
              "<td>0.4187050</td>\n",
              "<td>0.4263539</td>\n",
              "<td>0.4215915</td>\n",
              "<td>0.4472959</td>\n",
              "<td>0.4459462</td>\n",
              "<td>0.408706</td>\n",
              "<td>0.4373076</td>\n",
              "<td>0.4296555</td>\n",
              "<td>0.4464506</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.6531956</td>\n",
              "<td>0.0999643</td>\n",
              "<td>0.7307692</td>\n",
              "<td>0.7490119</td>\n",
              "<td>0.7600806</td>\n",
              "<td>0.6358268</td>\n",
              "<td>0.4106383</td>\n",
              "<td>0.6241901</td>\n",
              "<td>0.6637931</td>\n",
              "<td>0.6340426</td>\n",
              "<td>0.6986028</td>\n",
              "<td>0.625</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-13.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-13 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-13 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-13 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table th,\n",
              "#h2o-table-13 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-13 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-13\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Scoring History: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>timestamp</th>\n",
              "<th>duration</th>\n",
              "<th>number_of_trees</th>\n",
              "<th>training_rmse</th>\n",
              "<th>training_logloss</th>\n",
              "<th>training_auc</th>\n",
              "<th>training_pr_auc</th>\n",
              "<th>training_lift</th>\n",
              "<th>training_classification_error</th>\n",
              "<th>validation_rmse</th>\n",
              "<th>validation_logloss</th>\n",
              "<th>validation_auc</th>\n",
              "<th>validation_pr_auc</th>\n",
              "<th>validation_lift</th>\n",
              "<th>validation_classification_error</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>2023-06-22 20:46:20</td>\n",
              "<td>16.375 sec</td>\n",
              "<td>0.0</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3915000</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6085</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3915000</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6085</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:21</td>\n",
              "<td>16.595 sec</td>\n",
              "<td>5.0</td>\n",
              "<td>0.3871426</td>\n",
              "<td>0.4694493</td>\n",
              "<td>0.8686003</td>\n",
              "<td>0.8157606</td>\n",
              "<td>2.5223499</td>\n",
              "<td>0.211</td>\n",
              "<td>0.4205830</td>\n",
              "<td>0.5323082</td>\n",
              "<td>0.7970855</td>\n",
              "<td>0.7189038</td>\n",
              "<td>2.1711367</td>\n",
              "<td>0.2715</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:21</td>\n",
              "<td>16.828 sec</td>\n",
              "<td>10.0</td>\n",
              "<td>0.3599499</td>\n",
              "<td>0.4107030</td>\n",
              "<td>0.8967256</td>\n",
              "<td>0.8539203</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.182</td>\n",
              "<td>0.4204562</td>\n",
              "<td>0.5315744</td>\n",
              "<td>0.7955124</td>\n",
              "<td>0.7229683</td>\n",
              "<td>2.4265645</td>\n",
              "<td>0.2735</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:21</td>\n",
              "<td>17.066 sec</td>\n",
              "<td>15.0</td>\n",
              "<td>0.3408629</td>\n",
              "<td>0.3748717</td>\n",
              "<td>0.9173308</td>\n",
              "<td>0.8814513</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.15525</td>\n",
              "<td>0.4256850</td>\n",
              "<td>0.5460932</td>\n",
              "<td>0.7883968</td>\n",
              "<td>0.7171523</td>\n",
              "<td>2.4265645</td>\n",
              "<td>0.3105</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:21</td>\n",
              "<td>17.307 sec</td>\n",
              "<td>20.0</td>\n",
              "<td>0.3228023</td>\n",
              "<td>0.3436028</td>\n",
              "<td>0.9346762</td>\n",
              "<td>0.9068350</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.135625</td>\n",
              "<td>0.4289042</td>\n",
              "<td>0.5560917</td>\n",
              "<td>0.7849017</td>\n",
              "<td>0.7144570</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.2845</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:22</td>\n",
              "<td>17.557 sec</td>\n",
              "<td>25.0</td>\n",
              "<td>0.3070198</td>\n",
              "<td>0.3181104</td>\n",
              "<td>0.9478527</td>\n",
              "<td>0.9247560</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.118625</td>\n",
              "<td>0.4319605</td>\n",
              "<td>0.5645589</td>\n",
              "<td>0.7817372</td>\n",
              "<td>0.7128463</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.2935</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:22</td>\n",
              "<td>17.809 sec</td>\n",
              "<td>30.0</td>\n",
              "<td>0.2937282</td>\n",
              "<td>0.2961196</td>\n",
              "<td>0.9574856</td>\n",
              "<td>0.9379441</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.10725</td>\n",
              "<td>0.4340509</td>\n",
              "<td>0.5694655</td>\n",
              "<td>0.7805787</td>\n",
              "<td>0.7132335</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.321</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-14.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-14 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-14 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-14 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table th,\n",
              "#h2o-table-14 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-14 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-14\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Variable Importances: </caption>\n",
              "    <thead><tr><th>variable</th>\n",
              "<th>relative_importance</th>\n",
              "<th>scaled_importance</th>\n",
              "<th>percentage</th></tr></thead>\n",
              "    <tbody><tr><td>area_se</td>\n",
              "<td>1904.8582764</td>\n",
              "<td>1.0</td>\n",
              "<td>0.1771016</td></tr>\n",
              "<tr><td>perimeter_worst</td>\n",
              "<td>670.3985596</td>\n",
              "<td>0.3519414</td>\n",
              "<td>0.0623294</td></tr>\n",
              "<tr><td>smoothness_worst</td>\n",
              "<td>490.8770752</td>\n",
              "<td>0.2576974</td>\n",
              "<td>0.0456386</td></tr>\n",
              "<tr><td>texture_worst</td>\n",
              "<td>471.0637817</td>\n",
              "<td>0.2472960</td>\n",
              "<td>0.0437965</td></tr>\n",
              "<tr><td>radius_se</td>\n",
              "<td>406.7816772</td>\n",
              "<td>0.2135496</td>\n",
              "<td>0.0378200</td></tr>\n",
              "<tr><td>symmetry_se</td>\n",
              "<td>383.1606445</td>\n",
              "<td>0.2011492</td>\n",
              "<td>0.0356238</td></tr>\n",
              "<tr><td>area_worst</td>\n",
              "<td>379.5228882</td>\n",
              "<td>0.1992394</td>\n",
              "<td>0.0352856</td></tr>\n",
              "<tr><td>texture_se</td>\n",
              "<td>376.7027893</td>\n",
              "<td>0.1977590</td>\n",
              "<td>0.0350234</td></tr>\n",
              "<tr><td>radius_worst</td>\n",
              "<td>356.6051025</td>\n",
              "<td>0.1872082</td>\n",
              "<td>0.0331549</td></tr>\n",
              "<tr><td>symmetry_mean</td>\n",
              "<td>341.8689880</td>\n",
              "<td>0.1794721</td>\n",
              "<td>0.0317848</td></tr>\n",
              "<tr><td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td></tr>\n",
              "<tr><td>compactness_mean</td>\n",
              "<td>257.6020813</td>\n",
              "<td>0.1352343</td>\n",
              "<td>0.0239502</td></tr>\n",
              "<tr><td>smoothness_se</td>\n",
              "<td>247.9092102</td>\n",
              "<td>0.1301458</td>\n",
              "<td>0.0230490</td></tr>\n",
              "<tr><td>fractal_dimension_worst</td>\n",
              "<td>239.3760834</td>\n",
              "<td>0.1256661</td>\n",
              "<td>0.0222557</td></tr>\n",
              "<tr><td>perimeter_mean</td>\n",
              "<td>235.8315887</td>\n",
              "<td>0.1238053</td>\n",
              "<td>0.0219261</td></tr>\n",
              "<tr><td>fractal_dimension_mean</td>\n",
              "<td>229.6742096</td>\n",
              "<td>0.1205729</td>\n",
              "<td>0.0213537</td></tr>\n",
              "<tr><td>compactness_worst</td>\n",
              "<td>214.6832581</td>\n",
              "<td>0.1127030</td>\n",
              "<td>0.0199599</td></tr>\n",
              "<tr><td>concave points_mean</td>\n",
              "<td>202.2340851</td>\n",
              "<td>0.1061675</td>\n",
              "<td>0.0188024</td></tr>\n",
              "<tr><td>radius_mean</td>\n",
              "<td>184.1133881</td>\n",
              "<td>0.0966546</td>\n",
              "<td>0.0171177</td></tr>\n",
              "<tr><td>area_mean</td>\n",
              "<td>175.9438477</td>\n",
              "<td>0.0923658</td>\n",
              "<td>0.0163581</td></tr>\n",
              "<tr><td>compactness_se</td>\n",
              "<td>169.7982025</td>\n",
              "<td>0.0891395</td>\n",
              "<td>0.0157868</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "<pre style='font-size: smaller; margin-bottom: 1em;'>[30 rows x 4 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb = aml.leaderboard\n",
        "lb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "gmCXw_D_4y1D",
        "outputId": "99ca6e59-a5e8-44fa-afb5-b47b680048f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_id                                                accuracy       auc    logloss     aucpr    mean_per_class_error      rmse       mse\n",
              "----------------------------------------------------  ----------  --------  ---------  --------  ----------------------  --------  --------\n",
              "XGBoost_1_AutoML_1_20230622_204604                      0.731375  0.789913   0.561354  0.704715                0.281571  0.431458  0.186156\n",
              "XGBoost_2_AutoML_1_20230622_204604                      0.732875  0.786979   0.569302  0.710217                0.292189  0.432906  0.187407\n",
              "GBM_4_AutoML_1_20230622_204604                          0.74325   0.804633   0.521738  0.73211                 0.271387  0.416969  0.173863\n",
              "XGBoost_3_AutoML_1_20230622_204604                      0.746125  0.807007   0.522189  0.733116                0.269031  0.41621   0.173231\n",
              "DRF_1_AutoML_1_20230622_204604                          0.747375  0.804947   0.538154  0.731415                0.271721  0.415924  0.172993\n",
              "XRT_1_AutoML_1_20230622_204604                          0.747875  0.806856   0.550448  0.732292                0.261301  0.415221  0.172409\n",
              "GBM_3_AutoML_1_20230622_204604                          0.750125  0.812317   0.513038  0.737965                0.266742  0.412843  0.170439\n",
              "GBM_2_AutoML_1_20230622_204604                          0.7515    0.813887   0.51051   0.741607                0.261279  0.411848  0.169619\n",
              "GBM_1_AutoML_1_20230622_204604                          0.752625  0.816537   0.506866  0.747142                0.259403  0.409777  0.167917\n",
              "StackedEnsemble_AllModels_1_AutoML_1_20230622_204604    0.760375  0.830748   0.489772  0.765725                0.249518  0.402301  0.161846\n",
              "[10 rows x 8 columns]\n"
            ],
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th>model_id                                            </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>XGBoost_1_AutoML_1_20230622_204604                  </td><td style=\"text-align: right;\">  0.731375</td><td style=\"text-align: right;\">0.789913</td><td style=\"text-align: right;\"> 0.561354</td><td style=\"text-align: right;\">0.704715</td><td style=\"text-align: right;\">              0.281571</td><td style=\"text-align: right;\">0.431458</td><td style=\"text-align: right;\">0.186156</td></tr>\n",
              "<tr><td>XGBoost_2_AutoML_1_20230622_204604                  </td><td style=\"text-align: right;\">  0.732875</td><td style=\"text-align: right;\">0.786979</td><td style=\"text-align: right;\"> 0.569302</td><td style=\"text-align: right;\">0.710217</td><td style=\"text-align: right;\">              0.292189</td><td style=\"text-align: right;\">0.432906</td><td style=\"text-align: right;\">0.187407</td></tr>\n",
              "<tr><td>GBM_4_AutoML_1_20230622_204604                      </td><td style=\"text-align: right;\">  0.74325 </td><td style=\"text-align: right;\">0.804633</td><td style=\"text-align: right;\"> 0.521738</td><td style=\"text-align: right;\">0.73211 </td><td style=\"text-align: right;\">              0.271387</td><td style=\"text-align: right;\">0.416969</td><td style=\"text-align: right;\">0.173863</td></tr>\n",
              "<tr><td>XGBoost_3_AutoML_1_20230622_204604                  </td><td style=\"text-align: right;\">  0.746125</td><td style=\"text-align: right;\">0.807007</td><td style=\"text-align: right;\"> 0.522189</td><td style=\"text-align: right;\">0.733116</td><td style=\"text-align: right;\">              0.269031</td><td style=\"text-align: right;\">0.41621 </td><td style=\"text-align: right;\">0.173231</td></tr>\n",
              "<tr><td>DRF_1_AutoML_1_20230622_204604                      </td><td style=\"text-align: right;\">  0.747375</td><td style=\"text-align: right;\">0.804947</td><td style=\"text-align: right;\"> 0.538154</td><td style=\"text-align: right;\">0.731415</td><td style=\"text-align: right;\">              0.271721</td><td style=\"text-align: right;\">0.415924</td><td style=\"text-align: right;\">0.172993</td></tr>\n",
              "<tr><td>XRT_1_AutoML_1_20230622_204604                      </td><td style=\"text-align: right;\">  0.747875</td><td style=\"text-align: right;\">0.806856</td><td style=\"text-align: right;\"> 0.550448</td><td style=\"text-align: right;\">0.732292</td><td style=\"text-align: right;\">              0.261301</td><td style=\"text-align: right;\">0.415221</td><td style=\"text-align: right;\">0.172409</td></tr>\n",
              "<tr><td>GBM_3_AutoML_1_20230622_204604                      </td><td style=\"text-align: right;\">  0.750125</td><td style=\"text-align: right;\">0.812317</td><td style=\"text-align: right;\"> 0.513038</td><td style=\"text-align: right;\">0.737965</td><td style=\"text-align: right;\">              0.266742</td><td style=\"text-align: right;\">0.412843</td><td style=\"text-align: right;\">0.170439</td></tr>\n",
              "<tr><td>GBM_2_AutoML_1_20230622_204604                      </td><td style=\"text-align: right;\">  0.7515  </td><td style=\"text-align: right;\">0.813887</td><td style=\"text-align: right;\"> 0.51051 </td><td style=\"text-align: right;\">0.741607</td><td style=\"text-align: right;\">              0.261279</td><td style=\"text-align: right;\">0.411848</td><td style=\"text-align: right;\">0.169619</td></tr>\n",
              "<tr><td>GBM_1_AutoML_1_20230622_204604                      </td><td style=\"text-align: right;\">  0.752625</td><td style=\"text-align: right;\">0.816537</td><td style=\"text-align: right;\"> 0.506866</td><td style=\"text-align: right;\">0.747142</td><td style=\"text-align: right;\">              0.259403</td><td style=\"text-align: right;\">0.409777</td><td style=\"text-align: right;\">0.167917</td></tr>\n",
              "<tr><td>StackedEnsemble_AllModels_1_AutoML_1_20230622_204604</td><td style=\"text-align: right;\">  0.760375</td><td style=\"text-align: right;\">0.830748</td><td style=\"text-align: right;\"> 0.489772</td><td style=\"text-align: right;\">0.765725</td><td style=\"text-align: right;\">              0.249518</td><td style=\"text-align: right;\">0.402301</td><td style=\"text-align: right;\">0.161846</td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10 rows x 8 columns]</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "best_model"
      ],
      "metadata": {
        "id": "vlH8zwtisQU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "884b6a38-8f79-4b07-c2d7-93bcfe251ce8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230622_204604\n",
              "\n",
              "\n",
              "Model Summary: \n",
              "    number_of_trees\n",
              "--  -----------------\n",
              "    30\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.0862762318888866\n",
              "RMSE: 0.2937281598500331\n",
              "LogLoss: 0.2961195502729631\n",
              "Mean Per-Class Error: 0.10560430092631945\n",
              "AUC: 0.9574856020131994\n",
              "AUCPR: 0.9379440756884645\n",
              "Gini: 0.9149712040263989\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40771300310180303\n",
              "       0     1     Error    Rate\n",
              "-----  ----  ----  -------  --------------\n",
              "0      4317  551   0.1132   (551.0/4868.0)\n",
              "1      307   2825  0.098    (307.0/3132.0)\n",
              "Total  4624  3376  0.1072   (858.0/8000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.407713     0.868162  213\n",
              "max f2                       0.291031     0.900738  254\n",
              "max f0point5                 0.608403     0.885993  148\n",
              "max accuracy                 0.44304      0.894625  201\n",
              "max precision                0.993199     1         0\n",
              "max recall                   0.0188069    1         390\n",
              "max specificity              0.993199     1         0\n",
              "max absolute_mcc             0.426081     0.780124  207\n",
              "max min_per_class_accuracy   0.419471     0.89272   209\n",
              "max mean_per_class_accuracy  0.383103     0.894835  222\n",
              "max tns                      0.993199     4868      0\n",
              "max fns                      0.993199     3120      0\n",
              "max fps                      0.0052119    4868      399\n",
              "max tps                      0.0188069    3132      390\n",
              "max tnr                      0.993199     1         0\n",
              "max fnr                      0.993199     0.996169  0\n",
              "max fpr                      0.0052119    1         399\n",
              "max tpr                      0.0188069    1         390\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 39.32 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.983273           2.55428    2.55428            1                0.988143   1                           0.988143            0.0255428       0.0255428                  155.428   155.428            0.0255428\n",
              "2        0.02                        0.974009           2.55428    2.55428            1                0.978908   1                           0.983526            0.0255428       0.0510856                  155.428   155.428            0.0510856\n",
              "3        0.03                        0.965              2.52235    2.54364            0.9875           0.969758   0.995833                    0.978936            0.0252235       0.0763091                  152.235   154.364            0.0761036\n",
              "4        0.04                        0.956618           2.55428    2.5463             1                0.960834   0.996875                    0.974411            0.0255428       0.101852                   155.428   154.63             0.101646\n",
              "5        0.05                        0.948257           2.49042    2.53512            0.975            0.952464   0.9925                      0.970021            0.0249042       0.126756                   149.042   153.512            0.12614\n",
              "6        0.1                         0.891808           2.52235    2.52874            0.9875           0.92015    0.99                        0.945086            0.126117        0.252874                   152.235   152.874            0.25123\n",
              "7        0.15                        0.834331           2.49042    2.51596            0.975            0.864595   0.985                       0.918255            0.124521        0.377395                   149.042   151.596            0.373697\n",
              "8        0.2                         0.778986           2.40741    2.48883            0.9425           0.807305   0.974375                    0.890518            0.12037         0.497765                   140.741   148.883            0.489343\n",
              "9        0.3                         0.626419           2.20626    2.39464            0.86375          0.704981   0.9375                      0.828672            0.220626        0.718391                   120.626   139.464            0.687577\n",
              "10       0.4                         0.439384           1.57727    2.19029            0.6175           0.53251    0.8575                      0.754632            0.157727        0.876117                   57.7267   119.029            0.782445\n",
              "11       0.5                         0.291733           0.740741   1.90038            0.29             0.362017   0.744                       0.676109            0.0740741       0.950192                   -25.9259  90.0383            0.739838\n",
              "12       0.6                         0.186727           0.322478   1.6374             0.12625          0.235522   0.641042                    0.602678            0.0322478       0.982439                   -67.7522  63.7399            0.628495\n",
              "13       0.7                         0.118291           0.0957854  1.41717            0.0375           0.14983    0.554821                    0.537985            0.00957854      0.992018                   -90.4215  41.7168            0.479898\n",
              "14       0.8                         0.0704665          0.0510856  1.24641            0.02             0.0932805  0.487969                    0.482397            0.00510856      0.997126                   -94.8914  24.6408            0.323955\n",
              "15       0.9                         0.0350014          0.0159642  1.10969            0.00625          0.0520768  0.434444                    0.434584            0.00159642      0.998723                   -98.4036  10.9692            0.16224\n",
              "16       1                           0.00378296         0.0127714  1                  0.005            0.0206824  0.3915                      0.393194            0.00127714      1                          -98.7229  0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.18840019637319533\n",
              "RMSE: 0.4340509144941355\n",
              "LogLoss: 0.5694655292500879\n",
              "Mean Per-Class Error: 0.29928818116277384\n",
              "AUC: 0.7805786689418005\n",
              "AUCPR: 0.7132335417937792\n",
              "Gini: 0.561157337883601\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24615386625130972\n",
              "       0    1     Error    Rate\n",
              "-----  ---  ----  -------  --------------\n",
              "0      731  486   0.3993   (486.0/1217.0)\n",
              "1      156  627   0.1992   (156.0/783.0)\n",
              "Total  887  1113  0.321    (642.0/2000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.246154     0.661392  271\n",
              "max f2                       0.0822697    0.783171  350\n",
              "max f0point5                 0.628715     0.680179  134\n",
              "max accuracy                 0.628715     0.7385    134\n",
              "max precision                0.996239     1         0\n",
              "max recall                   0.00914458   1         397\n",
              "max specificity              0.996239     1         0\n",
              "max absolute_mcc             0.628715     0.435211  134\n",
              "max min_per_class_accuracy   0.354121     0.699872  226\n",
              "max mean_per_class_accuracy  0.552612     0.706514  157\n",
              "max tns                      0.996239     1217      0\n",
              "max fns                      0.996239     782       0\n",
              "max fps                      0.00487721   1217      399\n",
              "max tps                      0.00914458   783       397\n",
              "max tnr                      0.996239     1         0\n",
              "max fnr                      0.996239     0.998723  0\n",
              "max fpr                      0.00487721   1         399\n",
              "max tpr                      0.00914458   1         397\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.46 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.977798           2.55428   2.55428            1                0.985284   1                           0.985284            0.0255428       0.0255428                  155.428   155.428            0.0255428\n",
              "2        0.02                        0.964573           2.55428   2.55428            1                0.972878   1                           0.979081            0.0255428       0.0510856                  155.428   155.428            0.0510856\n",
              "3        0.03                        0.954787           2.29885   2.46914            0.9              0.958362   0.966667                    0.972175            0.0229885       0.0740741                  129.885   146.914            0.0724307\n",
              "4        0.04                        0.943778           2.17114   2.39464            0.85             0.950531   0.9375                      0.966764            0.0217114       0.0957854                  117.114   139.464            0.091677\n",
              "5        0.05                        0.928851           2.04342   2.32439            0.8              0.935592   0.91                        0.960529            0.0204342       0.11622                    104.342   132.439            0.108824\n",
              "6        0.1                         0.874916           1.94125   2.13282            0.76             0.902028   0.835                       0.931279            0.0970626       0.213282                   94.1252   113.282            0.186166\n",
              "7        0.15                        0.800584           1.96679   2.07748            0.77             0.837897   0.813333                    0.900151            0.0983397       0.311622                   96.6794   107.748            0.265607\n",
              "8        0.2                         0.727813           1.76245   1.99872            0.69             0.76954    0.7825                      0.867499            0.0881226       0.399745                   76.2452   99.8723            0.328257\n",
              "9        0.3                         0.572579           1.45594   1.81779            0.57             0.649716   0.711667                    0.794904            0.145594        0.545338                   45.5939   81.7795            0.403186\n",
              "10       0.4                         0.427932           0.970626  1.606              0.38             0.50131    0.62875                     0.721506            0.0970626       0.642401                   -2.93742  60.6003            0.398358\n",
              "11       0.5                         0.303219           0.91954   1.46871            0.36             0.363956   0.575                       0.649996            0.091954        0.734355                   -8.04598  46.871             0.385136\n",
              "12       0.6                         0.208844           0.881226  1.3708             0.345            0.252344   0.536667                    0.58372             0.0881226       0.822478                   -11.8774  37.0796            0.365617\n",
              "13       0.7                         0.130075           0.702427  1.27531            0.275            0.165416   0.499286                    0.523963            0.0702427       0.89272                    -29.7573  27.5315            0.316714\n",
              "14       0.8                         0.0777253          0.523627  1.18135            0.205            0.10211    0.4625                      0.471231            0.0523627       0.945083                   -47.6373  18.1354            0.238427\n",
              "15       0.9                         0.0354974          0.332056  1.08699            0.13             0.0543392  0.425556                    0.42491             0.0332056       0.978289                   -66.7944  8.69874            0.128658\n",
              "16       1                           0.00410937         0.217114  1                  0.085            0.0213709  0.3915                      0.384556            0.0217114       1                          -78.2886  0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.18615605059426837\n",
              "RMSE: 0.43145805195206216\n",
              "LogLoss: 0.5613540167599764\n",
              "Mean Per-Class Error: 0.28157062936622623\n",
              "AUC: 0.7899126007045779\n",
              "AUCPR: 0.704715036081932\n",
              "Gini: 0.5798252014091558\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3172192946076393\n",
              "       0     1     Error    Rate\n",
              "-----  ----  ----  -------  ---------------\n",
              "0      3297  1571  0.3227   (1571.0/4868.0)\n",
              "1      753   2379  0.2404   (753.0/3132.0)\n",
              "Total  4050  3950  0.2905   (2324.0/8000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.317219     0.671844  248\n",
              "max f2                       0.0903366    0.790684  347\n",
              "max f0point5                 0.562613     0.661942  158\n",
              "max accuracy                 0.548578     0.731375  163\n",
              "max precision                0.984683     0.941176  2\n",
              "max recall                   0.00676566   1         398\n",
              "max specificity              0.991555     0.999795  0\n",
              "max absolute_mcc             0.395143     0.434769  217\n",
              "max min_per_class_accuracy   0.369361     0.71871   227\n",
              "max mean_per_class_accuracy  0.395143     0.720713  217\n",
              "max tns                      0.991555     4867      0\n",
              "max fns                      0.991555     3120      0\n",
              "max fps                      0.00360703   4868      399\n",
              "max tps                      0.00676566   3132      398\n",
              "max tnr                      0.991555     0.999795  0\n",
              "max fnr                      0.991555     0.996169  0\n",
              "max fpr                      0.00360703   1         399\n",
              "max tpr                      0.00676566   1         398\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.96 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.976782           2.29885   2.29885            0.9              0.98354    0.9                         0.98354             0.0229885       0.0229885                  129.885   129.885            0.0213451\n",
              "2        0.02                        0.96877            2.36271   2.33078            0.925            0.972395   0.9125                      0.977967            0.0236271       0.0466156                  136.271   133.078            0.0437397\n",
              "3        0.03                        0.958395           2.26692   2.30949            0.8875           0.963849   0.904167                    0.973261            0.0226692       0.0692848                  126.692   130.949            0.0645601\n",
              "4        0.04                        0.947065           2.26692   2.29885            0.8875           0.952408   0.9                         0.968048            0.0226692       0.091954                   126.692   129.885            0.0853805\n",
              "5        0.05                        0.93624            2.23499   2.28608            0.875            0.941784   0.895                       0.962795            0.0223499       0.114304                   123.499   128.608            0.105676\n",
              "6        0.1                         0.879107           1.94764   2.11686            0.7625           0.908667   0.82875                     0.935731            0.0973819       0.211686                   94.7637   111.686            0.183543\n",
              "7        0.15                        0.814872           1.81354   2.01575            0.71             0.846586   0.789167                    0.906016            0.0906769       0.302363                   81.3538   101.575            0.250391\n",
              "8        0.2                         0.743607           1.67305   1.93008            0.655            0.781333   0.755625                    0.874845            0.0836526       0.386015                   67.3052   93.0077            0.305695\n",
              "9        0.3                         0.588081           1.47829   1.77948            0.57875          0.666473   0.696667                    0.805388            0.147829        0.533844                   47.8289   77.9481            0.384296\n",
              "10       0.4                         0.443764           1.28033   1.65469            0.50125          0.512584   0.647813                    0.732187            0.128033        0.661877                   28.0332   65.4693            0.430365\n",
              "11       0.5                         0.306942           1.01533   1.52682            0.3975           0.37215    0.59775                     0.66018             0.101533        0.76341                    1.53257   52.682             0.432884\n",
              "12       0.6                         0.202824           0.766284  1.40006            0.3              0.251989   0.548125                    0.592148            0.0766284       0.840038                   -23.3716  40.0064            0.394475\n",
              "13       0.7                         0.130317           0.664112  1.29493            0.26             0.163729   0.506964                    0.530945            0.0664112       0.90645                    -33.5888  29.4928            0.339276\n",
              "14       0.8                         0.0758979          0.472542  1.19213            0.185            0.101729   0.466719                    0.477293            0.0472542       0.953704                   -52.7458  19.213             0.252594\n",
              "15       0.9                         0.0371696          0.322478  1.0955             0.12625          0.0557451  0.428889                    0.430455            0.0322478       0.985951                   -67.7522  9.55016            0.141251\n",
              "16       1                           0.00228818         0.140485  1                  0.055            0.0215713  0.3915                      0.389566            0.0140485       1                          -85.9515  0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                         mean      sd         cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid    cv_6_valid    cv_7_valid    cv_8_valid    cv_9_valid    cv_10_valid\n",
              "-----------------------  --------  ---------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  -------------\n",
              "accuracy                 0.70825   0.0371661  0.7275        0.74375       0.74          0.70375       0.61875       0.6975        0.7375        0.705         0.72375       0.685\n",
              "auc                      0.789564  0.0191126  0.791147      0.797844      0.78883       0.801228      0.766344      0.776762      0.831018      0.78962       0.788937      0.763911\n",
              "err                      0.29175   0.0371661  0.2725        0.25625       0.26          0.29625       0.38125       0.3025        0.2625        0.295         0.27625       0.315\n",
              "err_count                233.4     29.7329    218           205           208           237           305           242           210           236           221           252\n",
              "f0point5                 0.629483  0.0312905  0.641696      0.648259      0.655488      0.60241       0.570673      0.637743      0.675287      0.638809      0.629467      0.595\n",
              "f1                       0.679528  0.0206461  0.669697      0.678179      0.673981      0.669456      0.664466      0.689744      0.728682      0.692708      0.674521      0.653846\n",
              "f2                       0.740414  0.0352846  0.700253      0.710994      0.693548      0.753296      0.795155      0.750977      0.791246      0.756542      0.726523      0.72561\n",
              "lift_top_group           2.34294   0.336948   1.96078       2.38095       2.63158       2.73973       2.42424       1.78042       2.08333       2.12121       2.67559       2.63158\n",
              "logloss                  0.561354  0.0306372  0.563873      0.538003      0.554789      0.533056      0.596285      0.5928        0.506254      0.576416      0.552059      0.600005\n",
              "max_per_class_error      0.354375  0.0925364  0.277778      0.265306      0.292763      0.364173      0.589362      0.37581       0.336207      0.365957      0.301397      0.375\n",
              "mcc                      0.437511  0.0391816  0.443246      0.471221      0.459964      0.441607      0.358681      0.419569      0.498854      0.43528       0.450001      0.396688\n",
              "mean_per_class_accuracy  0.721276  0.0247419  0.726496      0.741853      0.733659      0.728872      0.662895      0.711205      0.751539      0.720052      0.732245      0.703947\n",
              "mean_per_class_error     0.278724  0.0247419  0.273504      0.258147      0.266341      0.271128      0.337105      0.288795      0.248461      0.279948      0.267755      0.296053\n",
              "mse                      0.186156  0.0112054  0.185587      0.175314      0.181778      0.177739      0.200074      0.198868      0.167041      0.191238      0.184604      0.199318\n",
              "pr_auc                   0.706007  0.0296863  0.684173      0.702552      0.70519       0.707094      0.706578      0.687794      0.782989      0.702086      0.709587      0.672029\n",
              "precision                0.600636  0.0398683  0.624294      0.629738      0.643713      0.564706      0.521589      0.607223      0.643836      0.607306      0.602632      0.561321\n",
              "r2                       0.21708   0.0442381  0.214258      0.245779      0.228448      0.233138      0.174422      0.184293      0.314283      0.210882      0.2113        0.153998\n",
              "recall                   0.789357  0.0619735  0.722222      0.734694      0.707237      0.821918      0.915152      0.79822       0.839286      0.806061      0.765886      0.782895\n",
              "rmse                     0.431281  0.0130248  0.430798      0.418705      0.426354      0.421592      0.447296      0.445946      0.408706      0.437308      0.429656      0.446451\n",
              "specificity              0.653196  0.0999643  0.730769      0.749012      0.760081      0.635827      0.410638      0.62419       0.663793      0.634043      0.698603      0.625\n",
              "\n",
              "Scoring History: \n",
              "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
              "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
              "    2023-06-22 20:46:20  16.375 sec  0                  0.5              0.693147            0.5             0.3915             1                0.6085                           0.5                0.693147              0.5               0.3915               1                  0.6085\n",
              "    2023-06-22 20:46:21  16.595 sec  5                  0.387143         0.469449            0.8686          0.815761           2.52235          0.211                            0.420583           0.532308              0.797085          0.718904             2.17114            0.2715\n",
              "    2023-06-22 20:46:21  16.828 sec  10                 0.35995          0.410703            0.896726        0.85392            2.55428          0.182                            0.420456           0.531574              0.795512          0.722968             2.42656            0.2735\n",
              "    2023-06-22 20:46:21  17.066 sec  15                 0.340863         0.374872            0.917331        0.881451           2.55428          0.15525                          0.425685           0.546093              0.788397          0.717152             2.42656            0.3105\n",
              "    2023-06-22 20:46:21  17.307 sec  20                 0.322802         0.343603            0.934676        0.906835           2.55428          0.135625                         0.428904           0.556092              0.784902          0.714457             2.55428            0.2845\n",
              "    2023-06-22 20:46:22  17.557 sec  25                 0.30702          0.31811             0.947853        0.924756           2.55428          0.118625                         0.431961           0.564559              0.781737          0.712846             2.55428            0.2935\n",
              "    2023-06-22 20:46:22  17.809 sec  30                 0.293728         0.29612             0.957486        0.937944           2.55428          0.10725                          0.434051           0.569466              0.780579          0.713234             2.55428            0.321\n",
              "\n",
              "Variable Importances: \n",
              "variable                 relative_importance    scaled_importance    percentage\n",
              "-----------------------  ---------------------  -------------------  --------------------\n",
              "area_se                  1904.8582763671875     1.0                  0.17710163529899145\n",
              "perimeter_worst          670.3985595703125      0.35194143726474486  0.06232940406906372\n",
              "smoothness_worst         490.8770751953125      0.25769742625235037  0.0456386356016325\n",
              "texture_worst            471.06378173828125     0.24729597344987847  0.04379652130082945\n",
              "radius_se                406.78167724609375     0.2135495759935901   0.03781997912587105\n",
              "symmetry_se              383.16064453125        0.2011491612184331   0.03562384539080497\n",
              "area_worst               379.52288818359375     0.19923943575865039  0.03528562988890534\n",
              "texture_se               376.7027893066406      0.19775895875312144  0.035023434990203606\n",
              "radius_worst             356.6051025390625      0.18720820701640586  0.03315487960399761\n",
              "symmetry_mean            341.8689880371094      0.17947213830999437  0.031784809185306774\n",
              "---                      ---                    ---                  ---\n",
              "compactness_mean         257.6020812988281      0.13523425049243495  0.02395020691064367\n",
              "smoothness_se            247.90921020507812     0.13014575062133926  0.023049025262253914\n",
              "fractal_dimension_worst  239.37608337402344     0.12566608568409865  0.02225566927627705\n",
              "perimeter_mean           235.8315887451172      0.1238053201495277   0.021926124657196535\n",
              "fractal_dimension_mean   229.67420959472656     0.12057285964221189  0.021353650615311495\n",
              "compactness_worst        214.68325805664062     0.11270300826057754  0.019959887066064025\n",
              "concave points_mean      202.2340850830078      0.10616752311289768  0.01880244195893765\n",
              "radius_mean              184.11338806152344     0.09665463848189883  0.01711769453437711\n",
              "area_mean                175.94384765625        0.09236584676094528  0.01635814250713946\n",
              "compactness_se           169.79820251464844     0.08913954629657578  0.015786759418933725\n",
              "[30 rows x 4 columns]\n",
              "\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ],
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230622_204604\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-15.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-15 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-15 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-15 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-15 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-15 .h2o-table th,\n",
              "#h2o-table-15 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-15 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-15\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>number_of_trees</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>30.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.0862762318888866\n",
              "RMSE: 0.2937281598500331\n",
              "LogLoss: 0.2961195502729631\n",
              "Mean Per-Class Error: 0.10560430092631945\n",
              "AUC: 0.9574856020131994\n",
              "AUCPR: 0.9379440756884645\n",
              "Gini: 0.9149712040263989</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-16.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-16 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-16 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-16 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-16 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-16 .h2o-table th,\n",
              "#h2o-table-16 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-16 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-16\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40771300310180303</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>4317.0</td>\n",
              "<td>551.0</td>\n",
              "<td>0.1132</td>\n",
              "<td> (551.0/4868.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>307.0</td>\n",
              "<td>2825.0</td>\n",
              "<td>0.098</td>\n",
              "<td> (307.0/3132.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>4624.0</td>\n",
              "<td>3376.0</td>\n",
              "<td>0.1072</td>\n",
              "<td> (858.0/8000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-17.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-17 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-17 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-17 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-17 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-17 .h2o-table th,\n",
              "#h2o-table-17 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-17 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-17\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4077130</td>\n",
              "<td>0.8681623</td>\n",
              "<td>213.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.2910308</td>\n",
              "<td>0.9007375</td>\n",
              "<td>254.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6084025</td>\n",
              "<td>0.8859932</td>\n",
              "<td>148.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.4430403</td>\n",
              "<td>0.894625</td>\n",
              "<td>201.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0188069</td>\n",
              "<td>1.0</td>\n",
              "<td>390.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4260813</td>\n",
              "<td>0.7801245</td>\n",
              "<td>207.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.4194713</td>\n",
              "<td>0.8927203</td>\n",
              "<td>209.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3831034</td>\n",
              "<td>0.8948353</td>\n",
              "<td>222.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9931994</td>\n",
              "<td>4868.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9931994</td>\n",
              "<td>3120.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0052119</td>\n",
              "<td>4868.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0188069</td>\n",
              "<td>3132.0</td>\n",
              "<td>390.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9931994</td>\n",
              "<td>0.9961686</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0052119</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0188069</td>\n",
              "<td>1.0</td>\n",
              "<td>390.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-18.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-18 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-18 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-18 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-18 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-18 .h2o-table th,\n",
              "#h2o-table-18 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-18 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-18\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 39.32 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9832729</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9881432</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9881432</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0255428</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0255428</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9740088</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9789083</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9835258</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0510856</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0510856</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9650004</td>\n",
              "<td>2.5223499</td>\n",
              "<td>2.5436356</td>\n",
              "<td>0.9875</td>\n",
              "<td>0.9697575</td>\n",
              "<td>0.9958333</td>\n",
              "<td>0.9789364</td>\n",
              "<td>0.0252235</td>\n",
              "<td>0.0763091</td>\n",
              "<td>152.2349936</td>\n",
              "<td>154.3635590</td>\n",
              "<td>0.0761036</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9566181</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5462963</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9608343</td>\n",
              "<td>0.996875</td>\n",
              "<td>0.9744108</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.1018519</td>\n",
              "<td>155.4278416</td>\n",
              "<td>154.6296296</td>\n",
              "<td>0.1016464</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9482571</td>\n",
              "<td>2.4904215</td>\n",
              "<td>2.5351213</td>\n",
              "<td>0.975</td>\n",
              "<td>0.9524639</td>\n",
              "<td>0.9925</td>\n",
              "<td>0.9700214</td>\n",
              "<td>0.0249042</td>\n",
              "<td>0.1267561</td>\n",
              "<td>149.0421456</td>\n",
              "<td>153.5121328</td>\n",
              "<td>0.1261398</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8918075</td>\n",
              "<td>2.5223499</td>\n",
              "<td>2.5287356</td>\n",
              "<td>0.9875</td>\n",
              "<td>0.9201499</td>\n",
              "<td>0.99</td>\n",
              "<td>0.9450857</td>\n",
              "<td>0.1261175</td>\n",
              "<td>0.2528736</td>\n",
              "<td>152.2349936</td>\n",
              "<td>152.8735632</td>\n",
              "<td>0.2512302</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8343306</td>\n",
              "<td>2.4904215</td>\n",
              "<td>2.5159642</td>\n",
              "<td>0.975</td>\n",
              "<td>0.8645949</td>\n",
              "<td>0.985</td>\n",
              "<td>0.9182554</td>\n",
              "<td>0.1245211</td>\n",
              "<td>0.3773946</td>\n",
              "<td>149.0421456</td>\n",
              "<td>151.5964240</td>\n",
              "<td>0.3736970</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7789860</td>\n",
              "<td>2.4074074</td>\n",
              "<td>2.4888250</td>\n",
              "<td>0.9425</td>\n",
              "<td>0.8073055</td>\n",
              "<td>0.974375</td>\n",
              "<td>0.8905179</td>\n",
              "<td>0.1203704</td>\n",
              "<td>0.4977650</td>\n",
              "<td>140.7407407</td>\n",
              "<td>148.8825032</td>\n",
              "<td>0.4893427</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.6264191</td>\n",
              "<td>2.2062580</td>\n",
              "<td>2.3946360</td>\n",
              "<td>0.86375</td>\n",
              "<td>0.7049811</td>\n",
              "<td>0.9375</td>\n",
              "<td>0.8286723</td>\n",
              "<td>0.2206258</td>\n",
              "<td>0.7183908</td>\n",
              "<td>120.6257982</td>\n",
              "<td>139.4636015</td>\n",
              "<td>0.6875773</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4393838</td>\n",
              "<td>1.5772669</td>\n",
              "<td>2.1902937</td>\n",
              "<td>0.6175</td>\n",
              "<td>0.5325101</td>\n",
              "<td>0.8575</td>\n",
              "<td>0.7546318</td>\n",
              "<td>0.1577267</td>\n",
              "<td>0.8761175</td>\n",
              "<td>57.7266922</td>\n",
              "<td>119.0293742</td>\n",
              "<td>0.7824445</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.2917330</td>\n",
              "<td>0.7407407</td>\n",
              "<td>1.9003831</td>\n",
              "<td>0.29</td>\n",
              "<td>0.3620169</td>\n",
              "<td>0.744</td>\n",
              "<td>0.6761088</td>\n",
              "<td>0.0740741</td>\n",
              "<td>0.9501916</td>\n",
              "<td>-25.9259259</td>\n",
              "<td>90.0383142</td>\n",
              "<td>0.7398382</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.1867268</td>\n",
              "<td>0.3224777</td>\n",
              "<td>1.6373989</td>\n",
              "<td>0.12625</td>\n",
              "<td>0.2355222</td>\n",
              "<td>0.6410417</td>\n",
              "<td>0.6026777</td>\n",
              "<td>0.0322478</td>\n",
              "<td>0.9824393</td>\n",
              "<td>-67.7522350</td>\n",
              "<td>63.7398893</td>\n",
              "<td>0.6284952</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1182911</td>\n",
              "<td>0.0957854</td>\n",
              "<td>1.4171684</td>\n",
              "<td>0.0375</td>\n",
              "<td>0.1498300</td>\n",
              "<td>0.5548214</td>\n",
              "<td>0.5379852</td>\n",
              "<td>0.0095785</td>\n",
              "<td>0.9920179</td>\n",
              "<td>-90.4214559</td>\n",
              "<td>41.7168400</td>\n",
              "<td>0.4798979</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0704665</td>\n",
              "<td>0.0510856</td>\n",
              "<td>1.2464080</td>\n",
              "<td>0.02</td>\n",
              "<td>0.0932805</td>\n",
              "<td>0.4879688</td>\n",
              "<td>0.4823971</td>\n",
              "<td>0.0051086</td>\n",
              "<td>0.9971264</td>\n",
              "<td>-94.8914432</td>\n",
              "<td>24.6408046</td>\n",
              "<td>0.3239547</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0350014</td>\n",
              "<td>0.0159642</td>\n",
              "<td>1.1096921</td>\n",
              "<td>0.00625</td>\n",
              "<td>0.0520768</td>\n",
              "<td>0.4344444</td>\n",
              "<td>0.4345837</td>\n",
              "<td>0.0015964</td>\n",
              "<td>0.9987229</td>\n",
              "<td>-98.4035760</td>\n",
              "<td>10.9692068</td>\n",
              "<td>0.1622397</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0037830</td>\n",
              "<td>0.0127714</td>\n",
              "<td>1.0</td>\n",
              "<td>0.005</td>\n",
              "<td>0.0206824</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3931936</td>\n",
              "<td>0.0012771</td>\n",
              "<td>1.0</td>\n",
              "<td>-98.7228608</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.18840019637319533\n",
              "RMSE: 0.4340509144941355\n",
              "LogLoss: 0.5694655292500879\n",
              "Mean Per-Class Error: 0.29928818116277384\n",
              "AUC: 0.7805786689418005\n",
              "AUCPR: 0.7132335417937792\n",
              "Gini: 0.561157337883601</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-19.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-19 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-19 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-19 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-19 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-19 .h2o-table th,\n",
              "#h2o-table-19 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-19 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-19\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24615386625130972</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>731.0</td>\n",
              "<td>486.0</td>\n",
              "<td>0.3993</td>\n",
              "<td> (486.0/1217.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>156.0</td>\n",
              "<td>627.0</td>\n",
              "<td>0.1992</td>\n",
              "<td> (156.0/783.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>887.0</td>\n",
              "<td>1113.0</td>\n",
              "<td>0.321</td>\n",
              "<td> (642.0/2000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-20.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-20 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-20 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-20 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-20 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-20 .h2o-table th,\n",
              "#h2o-table-20 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-20 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-20\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.2461539</td>\n",
              "<td>0.6613924</td>\n",
              "<td>271.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.0822697</td>\n",
              "<td>0.7831708</td>\n",
              "<td>350.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.6801786</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.7385</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0091446</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.4352113</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3541214</td>\n",
              "<td>0.6998723</td>\n",
              "<td>226.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.5526123</td>\n",
              "<td>0.7065135</td>\n",
              "<td>157.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1217.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9962388</td>\n",
              "<td>782.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0048772</td>\n",
              "<td>1217.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0091446</td>\n",
              "<td>783.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9962388</td>\n",
              "<td>0.9987229</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0048772</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0091446</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-21.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-21 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-21 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-21 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-21 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-21 .h2o-table th,\n",
              "#h2o-table-21 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-21 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-21\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.46 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9777978</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9852836</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9852836</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0255428</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0255428</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9645726</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9728781</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9790808</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0510856</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0510856</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9547869</td>\n",
              "<td>2.2988506</td>\n",
              "<td>2.4691358</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9583625</td>\n",
              "<td>0.9666667</td>\n",
              "<td>0.9721747</td>\n",
              "<td>0.0229885</td>\n",
              "<td>0.0740741</td>\n",
              "<td>129.8850575</td>\n",
              "<td>146.9135802</td>\n",
              "<td>0.0724307</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9437785</td>\n",
              "<td>2.1711367</td>\n",
              "<td>2.3946360</td>\n",
              "<td>0.85</td>\n",
              "<td>0.9505307</td>\n",
              "<td>0.9375</td>\n",
              "<td>0.9667637</td>\n",
              "<td>0.0217114</td>\n",
              "<td>0.0957854</td>\n",
              "<td>117.1136654</td>\n",
              "<td>139.4636015</td>\n",
              "<td>0.0916770</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9288514</td>\n",
              "<td>2.0434227</td>\n",
              "<td>2.3243934</td>\n",
              "<td>0.8</td>\n",
              "<td>0.9355922</td>\n",
              "<td>0.91</td>\n",
              "<td>0.9605294</td>\n",
              "<td>0.0204342</td>\n",
              "<td>0.1162197</td>\n",
              "<td>104.3422733</td>\n",
              "<td>132.4393359</td>\n",
              "<td>0.1088244</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8749162</td>\n",
              "<td>1.9412516</td>\n",
              "<td>2.1328225</td>\n",
              "<td>0.76</td>\n",
              "<td>0.9020278</td>\n",
              "<td>0.835</td>\n",
              "<td>0.9312786</td>\n",
              "<td>0.0970626</td>\n",
              "<td>0.2132822</td>\n",
              "<td>94.1251596</td>\n",
              "<td>113.2822478</td>\n",
              "<td>0.1861664</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8005845</td>\n",
              "<td>1.9667944</td>\n",
              "<td>2.0774798</td>\n",
              "<td>0.77</td>\n",
              "<td>0.8378969</td>\n",
              "<td>0.8133333</td>\n",
              "<td>0.9001514</td>\n",
              "<td>0.0983397</td>\n",
              "<td>0.3116220</td>\n",
              "<td>96.6794381</td>\n",
              "<td>107.7479779</td>\n",
              "<td>0.2656072</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7278127</td>\n",
              "<td>1.7624521</td>\n",
              "<td>1.9987229</td>\n",
              "<td>0.69</td>\n",
              "<td>0.7695403</td>\n",
              "<td>0.7825</td>\n",
              "<td>0.8674986</td>\n",
              "<td>0.0881226</td>\n",
              "<td>0.3997446</td>\n",
              "<td>76.2452107</td>\n",
              "<td>99.8722861</td>\n",
              "<td>0.3282573</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.5725785</td>\n",
              "<td>1.4559387</td>\n",
              "<td>1.8177948</td>\n",
              "<td>0.57</td>\n",
              "<td>0.6497156</td>\n",
              "<td>0.7116667</td>\n",
              "<td>0.7949043</td>\n",
              "<td>0.1455939</td>\n",
              "<td>0.5453384</td>\n",
              "<td>45.5938697</td>\n",
              "<td>81.7794806</td>\n",
              "<td>0.4031856</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4279322</td>\n",
              "<td>0.9706258</td>\n",
              "<td>1.6060026</td>\n",
              "<td>0.38</td>\n",
              "<td>0.5013098</td>\n",
              "<td>0.62875</td>\n",
              "<td>0.7215057</td>\n",
              "<td>0.0970626</td>\n",
              "<td>0.6424010</td>\n",
              "<td>-2.9374202</td>\n",
              "<td>60.6002554</td>\n",
              "<td>0.3983583</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3032194</td>\n",
              "<td>0.9195402</td>\n",
              "<td>1.4687101</td>\n",
              "<td>0.36</td>\n",
              "<td>0.3639555</td>\n",
              "<td>0.575</td>\n",
              "<td>0.6499956</td>\n",
              "<td>0.0919540</td>\n",
              "<td>0.7343550</td>\n",
              "<td>-8.0459770</td>\n",
              "<td>46.8710089</td>\n",
              "<td>0.3851357</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.2088440</td>\n",
              "<td>0.8812261</td>\n",
              "<td>1.3707961</td>\n",
              "<td>0.345</td>\n",
              "<td>0.2523442</td>\n",
              "<td>0.5366667</td>\n",
              "<td>0.5837204</td>\n",
              "<td>0.0881226</td>\n",
              "<td>0.8224777</td>\n",
              "<td>-11.8773946</td>\n",
              "<td>37.0796083</td>\n",
              "<td>0.3656165</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1300754</td>\n",
              "<td>0.7024266</td>\n",
              "<td>1.2753147</td>\n",
              "<td>0.275</td>\n",
              "<td>0.1654164</td>\n",
              "<td>0.4992857</td>\n",
              "<td>0.5239627</td>\n",
              "<td>0.0702427</td>\n",
              "<td>0.8927203</td>\n",
              "<td>-29.7573436</td>\n",
              "<td>27.5314724</td>\n",
              "<td>0.3167137</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0777253</td>\n",
              "<td>0.5236271</td>\n",
              "<td>1.1813538</td>\n",
              "<td>0.205</td>\n",
              "<td>0.1021104</td>\n",
              "<td>0.4625</td>\n",
              "<td>0.4712311</td>\n",
              "<td>0.0523627</td>\n",
              "<td>0.9450830</td>\n",
              "<td>-47.6372925</td>\n",
              "<td>18.1353768</td>\n",
              "<td>0.2384273</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0354974</td>\n",
              "<td>0.3320562</td>\n",
              "<td>1.0869874</td>\n",
              "<td>0.13</td>\n",
              "<td>0.0543392</td>\n",
              "<td>0.4255556</td>\n",
              "<td>0.4249098</td>\n",
              "<td>0.0332056</td>\n",
              "<td>0.9782886</td>\n",
              "<td>-66.7943806</td>\n",
              "<td>8.6987371</td>\n",
              "<td>0.1286584</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0041094</td>\n",
              "<td>0.2171137</td>\n",
              "<td>1.0</td>\n",
              "<td>0.085</td>\n",
              "<td>0.0213709</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3845559</td>\n",
              "<td>0.0217114</td>\n",
              "<td>1.0</td>\n",
              "<td>-78.2886335</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.18615605059426837\n",
              "RMSE: 0.43145805195206216\n",
              "LogLoss: 0.5613540167599764\n",
              "Mean Per-Class Error: 0.28157062936622623\n",
              "AUC: 0.7899126007045779\n",
              "AUCPR: 0.704715036081932\n",
              "Gini: 0.5798252014091558</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-22.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-22 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-22 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-22 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-22 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-22 .h2o-table th,\n",
              "#h2o-table-22 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-22 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-22\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3172192946076393</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>3297.0</td>\n",
              "<td>1571.0</td>\n",
              "<td>0.3227</td>\n",
              "<td> (1571.0/4868.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>753.0</td>\n",
              "<td>2379.0</td>\n",
              "<td>0.2404</td>\n",
              "<td> (753.0/3132.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>4050.0</td>\n",
              "<td>3950.0</td>\n",
              "<td>0.2905</td>\n",
              "<td> (2324.0/8000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-23.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-23 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-23 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-23 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-23 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-23 .h2o-table th,\n",
              "#h2o-table-23 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-23 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-23\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3172193</td>\n",
              "<td>0.6718441</td>\n",
              "<td>248.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.0903366</td>\n",
              "<td>0.7906840</td>\n",
              "<td>347.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.5626134</td>\n",
              "<td>0.6619421</td>\n",
              "<td>158.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.5485779</td>\n",
              "<td>0.731375</td>\n",
              "<td>163.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9846833</td>\n",
              "<td>0.9411765</td>\n",
              "<td>2.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0067657</td>\n",
              "<td>1.0</td>\n",
              "<td>398.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9997946</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.3951432</td>\n",
              "<td>0.4347693</td>\n",
              "<td>217.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3693605</td>\n",
              "<td>0.7187101</td>\n",
              "<td>227.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3951432</td>\n",
              "<td>0.7207126</td>\n",
              "<td>217.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9915547</td>\n",
              "<td>4867.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9915547</td>\n",
              "<td>3120.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0036070</td>\n",
              "<td>4868.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0067657</td>\n",
              "<td>3132.0</td>\n",
              "<td>398.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9997946</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9961686</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0036070</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0067657</td>\n",
              "<td>1.0</td>\n",
              "<td>398.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-24.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-24 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-24 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-24 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-24 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-24 .h2o-table th,\n",
              "#h2o-table-24 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-24 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-24\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.96 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9767816</td>\n",
              "<td>2.2988506</td>\n",
              "<td>2.2988506</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9835398</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9835398</td>\n",
              "<td>0.0229885</td>\n",
              "<td>0.0229885</td>\n",
              "<td>129.8850575</td>\n",
              "<td>129.8850575</td>\n",
              "<td>0.0213451</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9687700</td>\n",
              "<td>2.3627075</td>\n",
              "<td>2.3307791</td>\n",
              "<td>0.925</td>\n",
              "<td>0.9723946</td>\n",
              "<td>0.9125</td>\n",
              "<td>0.9779672</td>\n",
              "<td>0.0236271</td>\n",
              "<td>0.0466156</td>\n",
              "<td>136.2707535</td>\n",
              "<td>133.0779055</td>\n",
              "<td>0.0437397</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9583953</td>\n",
              "<td>2.2669221</td>\n",
              "<td>2.3094934</td>\n",
              "<td>0.8875</td>\n",
              "<td>0.9638492</td>\n",
              "<td>0.9041667</td>\n",
              "<td>0.9732612</td>\n",
              "<td>0.0226692</td>\n",
              "<td>0.0692848</td>\n",
              "<td>126.6922095</td>\n",
              "<td>130.9493401</td>\n",
              "<td>0.0645601</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9470655</td>\n",
              "<td>2.2669221</td>\n",
              "<td>2.2988506</td>\n",
              "<td>0.8875</td>\n",
              "<td>0.9524081</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9680479</td>\n",
              "<td>0.0226692</td>\n",
              "<td>0.0919540</td>\n",
              "<td>126.6922095</td>\n",
              "<td>129.8850575</td>\n",
              "<td>0.0853805</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9362398</td>\n",
              "<td>2.2349936</td>\n",
              "<td>2.2860792</td>\n",
              "<td>0.875</td>\n",
              "<td>0.9417836</td>\n",
              "<td>0.895</td>\n",
              "<td>0.9627951</td>\n",
              "<td>0.0223499</td>\n",
              "<td>0.1143040</td>\n",
              "<td>123.4993614</td>\n",
              "<td>128.6079183</td>\n",
              "<td>0.1056762</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8791068</td>\n",
              "<td>1.9476373</td>\n",
              "<td>2.1168582</td>\n",
              "<td>0.7625</td>\n",
              "<td>0.9086671</td>\n",
              "<td>0.82875</td>\n",
              "<td>0.9357311</td>\n",
              "<td>0.0973819</td>\n",
              "<td>0.2116858</td>\n",
              "<td>94.7637292</td>\n",
              "<td>111.6858238</td>\n",
              "<td>0.1835428</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8148719</td>\n",
              "<td>1.8135377</td>\n",
              "<td>2.0157514</td>\n",
              "<td>0.71</td>\n",
              "<td>0.8465859</td>\n",
              "<td>0.7891667</td>\n",
              "<td>0.9060160</td>\n",
              "<td>0.0906769</td>\n",
              "<td>0.3023627</td>\n",
              "<td>81.3537676</td>\n",
              "<td>101.5751384</td>\n",
              "<td>0.2503906</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7436068</td>\n",
              "<td>1.6730524</td>\n",
              "<td>1.9300766</td>\n",
              "<td>0.655</td>\n",
              "<td>0.7813334</td>\n",
              "<td>0.755625</td>\n",
              "<td>0.8748454</td>\n",
              "<td>0.0836526</td>\n",
              "<td>0.3860153</td>\n",
              "<td>67.3052363</td>\n",
              "<td>93.0076628</td>\n",
              "<td>0.3056949</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.5880806</td>\n",
              "<td>1.4782886</td>\n",
              "<td>1.7794806</td>\n",
              "<td>0.57875</td>\n",
              "<td>0.6664730</td>\n",
              "<td>0.6966667</td>\n",
              "<td>0.8053879</td>\n",
              "<td>0.1478289</td>\n",
              "<td>0.5338442</td>\n",
              "<td>47.8288633</td>\n",
              "<td>77.9480630</td>\n",
              "<td>0.3842961</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4437644</td>\n",
              "<td>1.2803321</td>\n",
              "<td>1.6546935</td>\n",
              "<td>0.50125</td>\n",
              "<td>0.5125842</td>\n",
              "<td>0.6478125</td>\n",
              "<td>0.7321870</td>\n",
              "<td>0.1280332</td>\n",
              "<td>0.6618774</td>\n",
              "<td>28.0332056</td>\n",
              "<td>65.4693487</td>\n",
              "<td>0.4303655</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3069417</td>\n",
              "<td>1.0153257</td>\n",
              "<td>1.5268199</td>\n",
              "<td>0.3975</td>\n",
              "<td>0.3721498</td>\n",
              "<td>0.59775</td>\n",
              "<td>0.6601796</td>\n",
              "<td>0.1015326</td>\n",
              "<td>0.7634100</td>\n",
              "<td>1.5325670</td>\n",
              "<td>52.6819923</td>\n",
              "<td>0.4328841</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.2028235</td>\n",
              "<td>0.7662835</td>\n",
              "<td>1.4000639</td>\n",
              "<td>0.3</td>\n",
              "<td>0.2519894</td>\n",
              "<td>0.548125</td>\n",
              "<td>0.5921479</td>\n",
              "<td>0.0766284</td>\n",
              "<td>0.8400383</td>\n",
              "<td>-23.3716475</td>\n",
              "<td>40.0063857</td>\n",
              "<td>0.3944755</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1303173</td>\n",
              "<td>0.6641124</td>\n",
              "<td>1.2949279</td>\n",
              "<td>0.26</td>\n",
              "<td>0.1637290</td>\n",
              "<td>0.5069643</td>\n",
              "<td>0.5309452</td>\n",
              "<td>0.0664112</td>\n",
              "<td>0.9064496</td>\n",
              "<td>-33.5887612</td>\n",
              "<td>29.4927933</td>\n",
              "<td>0.3392762</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0758979</td>\n",
              "<td>0.4725415</td>\n",
              "<td>1.1921296</td>\n",
              "<td>0.185</td>\n",
              "<td>0.1017293</td>\n",
              "<td>0.4667188</td>\n",
              "<td>0.4772932</td>\n",
              "<td>0.0472542</td>\n",
              "<td>0.9537037</td>\n",
              "<td>-52.7458493</td>\n",
              "<td>19.2129630</td>\n",
              "<td>0.2525944</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0371696</td>\n",
              "<td>0.3224777</td>\n",
              "<td>1.0955016</td>\n",
              "<td>0.12625</td>\n",
              "<td>0.0557451</td>\n",
              "<td>0.4288889</td>\n",
              "<td>0.4304545</td>\n",
              "<td>0.0322478</td>\n",
              "<td>0.9859515</td>\n",
              "<td>-67.7522350</td>\n",
              "<td>9.5501632</td>\n",
              "<td>0.1412514</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0022882</td>\n",
              "<td>0.1404853</td>\n",
              "<td>1.0</td>\n",
              "<td>0.055</td>\n",
              "<td>0.0215713</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3895662</td>\n",
              "<td>0.0140485</td>\n",
              "<td>1.0</td>\n",
              "<td>-85.9514687</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-25.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-25 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-25 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-25 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-25 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-25 .h2o-table th,\n",
              "#h2o-table-25 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-25 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-25\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th>\n",
              "<th>cv_6_valid</th>\n",
              "<th>cv_7_valid</th>\n",
              "<th>cv_8_valid</th>\n",
              "<th>cv_9_valid</th>\n",
              "<th>cv_10_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.70825</td>\n",
              "<td>0.0371661</td>\n",
              "<td>0.7275</td>\n",
              "<td>0.74375</td>\n",
              "<td>0.74</td>\n",
              "<td>0.70375</td>\n",
              "<td>0.61875</td>\n",
              "<td>0.6975</td>\n",
              "<td>0.7375</td>\n",
              "<td>0.705</td>\n",
              "<td>0.72375</td>\n",
              "<td>0.685</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.7895642</td>\n",
              "<td>0.0191126</td>\n",
              "<td>0.7911474</td>\n",
              "<td>0.7978442</td>\n",
              "<td>0.7888304</td>\n",
              "<td>0.8012283</td>\n",
              "<td>0.7663443</td>\n",
              "<td>0.7767623</td>\n",
              "<td>0.8310178</td>\n",
              "<td>0.7896196</td>\n",
              "<td>0.7889371</td>\n",
              "<td>0.7639107</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.29175</td>\n",
              "<td>0.0371661</td>\n",
              "<td>0.2725</td>\n",
              "<td>0.25625</td>\n",
              "<td>0.26</td>\n",
              "<td>0.29625</td>\n",
              "<td>0.38125</td>\n",
              "<td>0.3025</td>\n",
              "<td>0.2625</td>\n",
              "<td>0.295</td>\n",
              "<td>0.27625</td>\n",
              "<td>0.315</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>233.4</td>\n",
              "<td>29.732885</td>\n",
              "<td>218.0</td>\n",
              "<td>205.0</td>\n",
              "<td>208.0</td>\n",
              "<td>237.0</td>\n",
              "<td>305.0</td>\n",
              "<td>242.0</td>\n",
              "<td>210.0</td>\n",
              "<td>236.0</td>\n",
              "<td>221.0</td>\n",
              "<td>252.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.6294831</td>\n",
              "<td>0.0312905</td>\n",
              "<td>0.6416957</td>\n",
              "<td>0.6482593</td>\n",
              "<td>0.6554878</td>\n",
              "<td>0.6024097</td>\n",
              "<td>0.5706727</td>\n",
              "<td>0.637743</td>\n",
              "<td>0.6752874</td>\n",
              "<td>0.6388088</td>\n",
              "<td>0.6294667</td>\n",
              "<td>0.595</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.6795281</td>\n",
              "<td>0.0206461</td>\n",
              "<td>0.669697</td>\n",
              "<td>0.6781790</td>\n",
              "<td>0.6739812</td>\n",
              "<td>0.6694561</td>\n",
              "<td>0.6644664</td>\n",
              "<td>0.6897436</td>\n",
              "<td>0.7286822</td>\n",
              "<td>0.6927083</td>\n",
              "<td>0.6745213</td>\n",
              "<td>0.6538461</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.7404144</td>\n",
              "<td>0.0352846</td>\n",
              "<td>0.7002535</td>\n",
              "<td>0.7109941</td>\n",
              "<td>0.6935484</td>\n",
              "<td>0.7532957</td>\n",
              "<td>0.7951554</td>\n",
              "<td>0.7509771</td>\n",
              "<td>0.7912458</td>\n",
              "<td>0.7565416</td>\n",
              "<td>0.7265229</td>\n",
              "<td>0.7256098</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.3429408</td>\n",
              "<td>0.3369483</td>\n",
              "<td>1.9607843</td>\n",
              "<td>2.3809524</td>\n",
              "<td>2.631579</td>\n",
              "<td>2.739726</td>\n",
              "<td>2.4242425</td>\n",
              "<td>1.7804154</td>\n",
              "<td>2.0833333</td>\n",
              "<td>2.121212</td>\n",
              "<td>2.6755853</td>\n",
              "<td>2.631579</td></tr>\n",
              "<tr><td>logloss</td>\n",
              "<td>0.5613540</td>\n",
              "<td>0.0306372</td>\n",
              "<td>0.5638731</td>\n",
              "<td>0.5380028</td>\n",
              "<td>0.5547891</td>\n",
              "<td>0.5330562</td>\n",
              "<td>0.5962847</td>\n",
              "<td>0.5928004</td>\n",
              "<td>0.5062535</td>\n",
              "<td>0.576416</td>\n",
              "<td>0.5520594</td>\n",
              "<td>0.600005</td></tr>\n",
              "<tr><td>max_per_class_error</td>\n",
              "<td>0.3543753</td>\n",
              "<td>0.0925364</td>\n",
              "<td>0.2777778</td>\n",
              "<td>0.2653061</td>\n",
              "<td>0.2927631</td>\n",
              "<td>0.3641732</td>\n",
              "<td>0.5893617</td>\n",
              "<td>0.3758099</td>\n",
              "<td>0.3362069</td>\n",
              "<td>0.3659574</td>\n",
              "<td>0.3013972</td>\n",
              "<td>0.375</td></tr>\n",
              "<tr><td>mcc</td>\n",
              "<td>0.4375112</td>\n",
              "<td>0.0391816</td>\n",
              "<td>0.4432463</td>\n",
              "<td>0.4712214</td>\n",
              "<td>0.4599636</td>\n",
              "<td>0.4416075</td>\n",
              "<td>0.3586807</td>\n",
              "<td>0.4195689</td>\n",
              "<td>0.4988538</td>\n",
              "<td>0.4352803</td>\n",
              "<td>0.4500011</td>\n",
              "<td>0.3966878</td></tr>\n",
              "<tr><td>mean_per_class_accuracy</td>\n",
              "<td>0.7212762</td>\n",
              "<td>0.0247419</td>\n",
              "<td>0.7264957</td>\n",
              "<td>0.7418529</td>\n",
              "<td>0.7336587</td>\n",
              "<td>0.7288723</td>\n",
              "<td>0.6628949</td>\n",
              "<td>0.7112048</td>\n",
              "<td>0.7515394</td>\n",
              "<td>0.7200516</td>\n",
              "<td>0.7322445</td>\n",
              "<td>0.7039474</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.2787238</td>\n",
              "<td>0.0247419</td>\n",
              "<td>0.2735043</td>\n",
              "<td>0.2581471</td>\n",
              "<td>0.2663413</td>\n",
              "<td>0.2711277</td>\n",
              "<td>0.3371051</td>\n",
              "<td>0.2887952</td>\n",
              "<td>0.2484606</td>\n",
              "<td>0.2799484</td>\n",
              "<td>0.2677555</td>\n",
              "<td>0.2960526</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.1861560</td>\n",
              "<td>0.0112054</td>\n",
              "<td>0.1855874</td>\n",
              "<td>0.1753139</td>\n",
              "<td>0.1817776</td>\n",
              "<td>0.1777394</td>\n",
              "<td>0.2000736</td>\n",
              "<td>0.1988681</td>\n",
              "<td>0.1670406</td>\n",
              "<td>0.1912379</td>\n",
              "<td>0.1846039</td>\n",
              "<td>0.1993182</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.706007</td>\n",
              "<td>0.0296863</td>\n",
              "<td>0.6841726</td>\n",
              "<td>0.7025523</td>\n",
              "<td>0.7051897</td>\n",
              "<td>0.7070935</td>\n",
              "<td>0.7065780</td>\n",
              "<td>0.6877939</td>\n",
              "<td>0.7829888</td>\n",
              "<td>0.7020859</td>\n",
              "<td>0.7095867</td>\n",
              "<td>0.6720285</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.6006356</td>\n",
              "<td>0.0398683</td>\n",
              "<td>0.6242938</td>\n",
              "<td>0.6297376</td>\n",
              "<td>0.6437126</td>\n",
              "<td>0.5647059</td>\n",
              "<td>0.5215889</td>\n",
              "<td>0.6072234</td>\n",
              "<td>0.6438356</td>\n",
              "<td>0.6073059</td>\n",
              "<td>0.6026316</td>\n",
              "<td>0.5613208</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.2170801</td>\n",
              "<td>0.0442381</td>\n",
              "<td>0.2142580</td>\n",
              "<td>0.2457793</td>\n",
              "<td>0.2284481</td>\n",
              "<td>0.2331382</td>\n",
              "<td>0.1744221</td>\n",
              "<td>0.1842931</td>\n",
              "<td>0.3142833</td>\n",
              "<td>0.2108816</td>\n",
              "<td>0.2113000</td>\n",
              "<td>0.1539976</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.7893569</td>\n",
              "<td>0.0619735</td>\n",
              "<td>0.7222222</td>\n",
              "<td>0.7346939</td>\n",
              "<td>0.7072368</td>\n",
              "<td>0.8219178</td>\n",
              "<td>0.9151515</td>\n",
              "<td>0.7982196</td>\n",
              "<td>0.8392857</td>\n",
              "<td>0.8060606</td>\n",
              "<td>0.7658863</td>\n",
              "<td>0.7828947</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.4312811</td>\n",
              "<td>0.0130248</td>\n",
              "<td>0.4307985</td>\n",
              "<td>0.4187050</td>\n",
              "<td>0.4263539</td>\n",
              "<td>0.4215915</td>\n",
              "<td>0.4472959</td>\n",
              "<td>0.4459462</td>\n",
              "<td>0.408706</td>\n",
              "<td>0.4373076</td>\n",
              "<td>0.4296555</td>\n",
              "<td>0.4464506</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.6531956</td>\n",
              "<td>0.0999643</td>\n",
              "<td>0.7307692</td>\n",
              "<td>0.7490119</td>\n",
              "<td>0.7600806</td>\n",
              "<td>0.6358268</td>\n",
              "<td>0.4106383</td>\n",
              "<td>0.6241901</td>\n",
              "<td>0.6637931</td>\n",
              "<td>0.6340426</td>\n",
              "<td>0.6986028</td>\n",
              "<td>0.625</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-26.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-26 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-26 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-26 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-26 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-26 .h2o-table th,\n",
              "#h2o-table-26 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-26 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-26\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Scoring History: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>timestamp</th>\n",
              "<th>duration</th>\n",
              "<th>number_of_trees</th>\n",
              "<th>training_rmse</th>\n",
              "<th>training_logloss</th>\n",
              "<th>training_auc</th>\n",
              "<th>training_pr_auc</th>\n",
              "<th>training_lift</th>\n",
              "<th>training_classification_error</th>\n",
              "<th>validation_rmse</th>\n",
              "<th>validation_logloss</th>\n",
              "<th>validation_auc</th>\n",
              "<th>validation_pr_auc</th>\n",
              "<th>validation_lift</th>\n",
              "<th>validation_classification_error</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>2023-06-22 20:46:20</td>\n",
              "<td>16.375 sec</td>\n",
              "<td>0.0</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3915000</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6085</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3915000</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6085</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:21</td>\n",
              "<td>16.595 sec</td>\n",
              "<td>5.0</td>\n",
              "<td>0.3871426</td>\n",
              "<td>0.4694493</td>\n",
              "<td>0.8686003</td>\n",
              "<td>0.8157606</td>\n",
              "<td>2.5223499</td>\n",
              "<td>0.211</td>\n",
              "<td>0.4205830</td>\n",
              "<td>0.5323082</td>\n",
              "<td>0.7970855</td>\n",
              "<td>0.7189038</td>\n",
              "<td>2.1711367</td>\n",
              "<td>0.2715</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:21</td>\n",
              "<td>16.828 sec</td>\n",
              "<td>10.0</td>\n",
              "<td>0.3599499</td>\n",
              "<td>0.4107030</td>\n",
              "<td>0.8967256</td>\n",
              "<td>0.8539203</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.182</td>\n",
              "<td>0.4204562</td>\n",
              "<td>0.5315744</td>\n",
              "<td>0.7955124</td>\n",
              "<td>0.7229683</td>\n",
              "<td>2.4265645</td>\n",
              "<td>0.2735</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:21</td>\n",
              "<td>17.066 sec</td>\n",
              "<td>15.0</td>\n",
              "<td>0.3408629</td>\n",
              "<td>0.3748717</td>\n",
              "<td>0.9173308</td>\n",
              "<td>0.8814513</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.15525</td>\n",
              "<td>0.4256850</td>\n",
              "<td>0.5460932</td>\n",
              "<td>0.7883968</td>\n",
              "<td>0.7171523</td>\n",
              "<td>2.4265645</td>\n",
              "<td>0.3105</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:21</td>\n",
              "<td>17.307 sec</td>\n",
              "<td>20.0</td>\n",
              "<td>0.3228023</td>\n",
              "<td>0.3436028</td>\n",
              "<td>0.9346762</td>\n",
              "<td>0.9068350</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.135625</td>\n",
              "<td>0.4289042</td>\n",
              "<td>0.5560917</td>\n",
              "<td>0.7849017</td>\n",
              "<td>0.7144570</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.2845</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:22</td>\n",
              "<td>17.557 sec</td>\n",
              "<td>25.0</td>\n",
              "<td>0.3070198</td>\n",
              "<td>0.3181104</td>\n",
              "<td>0.9478527</td>\n",
              "<td>0.9247560</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.118625</td>\n",
              "<td>0.4319605</td>\n",
              "<td>0.5645589</td>\n",
              "<td>0.7817372</td>\n",
              "<td>0.7128463</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.2935</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:46:22</td>\n",
              "<td>17.809 sec</td>\n",
              "<td>30.0</td>\n",
              "<td>0.2937282</td>\n",
              "<td>0.2961196</td>\n",
              "<td>0.9574856</td>\n",
              "<td>0.9379441</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.10725</td>\n",
              "<td>0.4340509</td>\n",
              "<td>0.5694655</td>\n",
              "<td>0.7805787</td>\n",
              "<td>0.7132335</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.321</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-27.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-27 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-27 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-27 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-27 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-27 .h2o-table th,\n",
              "#h2o-table-27 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-27 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-27\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Variable Importances: </caption>\n",
              "    <thead><tr><th>variable</th>\n",
              "<th>relative_importance</th>\n",
              "<th>scaled_importance</th>\n",
              "<th>percentage</th></tr></thead>\n",
              "    <tbody><tr><td>area_se</td>\n",
              "<td>1904.8582764</td>\n",
              "<td>1.0</td>\n",
              "<td>0.1771016</td></tr>\n",
              "<tr><td>perimeter_worst</td>\n",
              "<td>670.3985596</td>\n",
              "<td>0.3519414</td>\n",
              "<td>0.0623294</td></tr>\n",
              "<tr><td>smoothness_worst</td>\n",
              "<td>490.8770752</td>\n",
              "<td>0.2576974</td>\n",
              "<td>0.0456386</td></tr>\n",
              "<tr><td>texture_worst</td>\n",
              "<td>471.0637817</td>\n",
              "<td>0.2472960</td>\n",
              "<td>0.0437965</td></tr>\n",
              "<tr><td>radius_se</td>\n",
              "<td>406.7816772</td>\n",
              "<td>0.2135496</td>\n",
              "<td>0.0378200</td></tr>\n",
              "<tr><td>symmetry_se</td>\n",
              "<td>383.1606445</td>\n",
              "<td>0.2011492</td>\n",
              "<td>0.0356238</td></tr>\n",
              "<tr><td>area_worst</td>\n",
              "<td>379.5228882</td>\n",
              "<td>0.1992394</td>\n",
              "<td>0.0352856</td></tr>\n",
              "<tr><td>texture_se</td>\n",
              "<td>376.7027893</td>\n",
              "<td>0.1977590</td>\n",
              "<td>0.0350234</td></tr>\n",
              "<tr><td>radius_worst</td>\n",
              "<td>356.6051025</td>\n",
              "<td>0.1872082</td>\n",
              "<td>0.0331549</td></tr>\n",
              "<tr><td>symmetry_mean</td>\n",
              "<td>341.8689880</td>\n",
              "<td>0.1794721</td>\n",
              "<td>0.0317848</td></tr>\n",
              "<tr><td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td></tr>\n",
              "<tr><td>compactness_mean</td>\n",
              "<td>257.6020813</td>\n",
              "<td>0.1352343</td>\n",
              "<td>0.0239502</td></tr>\n",
              "<tr><td>smoothness_se</td>\n",
              "<td>247.9092102</td>\n",
              "<td>0.1301458</td>\n",
              "<td>0.0230490</td></tr>\n",
              "<tr><td>fractal_dimension_worst</td>\n",
              "<td>239.3760834</td>\n",
              "<td>0.1256661</td>\n",
              "<td>0.0222557</td></tr>\n",
              "<tr><td>perimeter_mean</td>\n",
              "<td>235.8315887</td>\n",
              "<td>0.1238053</td>\n",
              "<td>0.0219261</td></tr>\n",
              "<tr><td>fractal_dimension_mean</td>\n",
              "<td>229.6742096</td>\n",
              "<td>0.1205729</td>\n",
              "<td>0.0213537</td></tr>\n",
              "<tr><td>compactness_worst</td>\n",
              "<td>214.6832581</td>\n",
              "<td>0.1127030</td>\n",
              "<td>0.0199599</td></tr>\n",
              "<tr><td>concave points_mean</td>\n",
              "<td>202.2340851</td>\n",
              "<td>0.1061675</td>\n",
              "<td>0.0188024</td></tr>\n",
              "<tr><td>radius_mean</td>\n",
              "<td>184.1133881</td>\n",
              "<td>0.0966546</td>\n",
              "<td>0.0171177</td></tr>\n",
              "<tr><td>area_mean</td>\n",
              "<td>175.9438477</td>\n",
              "<td>0.0923658</td>\n",
              "<td>0.0163581</td></tr>\n",
              "<tr><td>compactness_se</td>\n",
              "<td>169.7982025</td>\n",
              "<td>0.0891395</td>\n",
              "<td>0.0157868</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "<pre style='font-size: smaller; margin-bottom: 1em;'>[30 rows x 4 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.model_performance(train).accuracy()"
      ],
      "metadata": {
        "id": "G_PyYFNMs97_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60497ea4-17df-4fb8-f323-51ff3457412d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.44304026911656064, 0.894625]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "HATr  = best_model.model_performance(train)\n",
        "HATe  = best_model.model_performance(valid)"
      ],
      "metadata": {
        "id": "jKquKjVVJBL9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_h2o = pd.DataFrame(h2o.as_list(best_model.predict(valid)))['predict']\n",
        "y_test_h2o = np.array(valid1['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-caBWGPlp8P",
        "outputId": "4a44df53-6c54-446c-becd-32ef80136a31"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SFOLD DATA AUTOML\n",
        "#strain, svalid = shdf.split_frame(ratios=[.8], seed=123)\n",
        "shdf  = newdata.copy()\n",
        "#shdf['y_test'] = shdf['y_test'].replace(0,\"B\")\n",
        "#shdf['y_test'] = shdf['y_test'].replace(1,\"M\")\n",
        "shy = \"y_test\"\n",
        "shx = list(shdf.columns)\n",
        "shx.remove(shy)\n",
        "\n",
        "shdf.iloc[:,0:6] = StandardScaler().fit_transform(shdf.iloc[:,0:6])\n",
        "#shdf.iloc[:,-1] = LabelEncoder().fit_transform(shdf.iloc[:,-1])\n",
        "strain1, svalid1 = train_test_split(shdf, test_size=0.2,random_state=123)\n",
        "strain = h2o.H2OFrame(strain1)\n",
        "svalid = h2o.H2OFrame(svalid1)\n",
        "strain[\"y_test\"] = strain[\"y_test\"].asfactor()\n",
        "svalid[\"y_test\"] = svalid[\"y_test\"].asfactor()\n",
        "\n",
        "\n",
        "saml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\", nfolds=10, sort_metric='accuracy')\n",
        "saml.train(x = shx, y = shy, training_frame = strain, validation_frame = svalid)\n",
        "sbest_model = saml.get_best_model()\n",
        "sHATr  = sbest_model.model_performance(strain)\n",
        "sHATe  = sbest_model.model_performance(svalid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMDqylVK7svS",
        "outputId": "46eae50c-5973-4a1d-a548-e3060d02c7a2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "AutoML progress: |\n",
            "20:48:47.511: Project: AutoML_2_20230622_204847\n",
            "20:48:47.511: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "20:48:47.512: Setting stopping tolerance adaptively based on the training frame: 0.011180339887498949\n",
            "20:48:47.512: Build control seed: 123\n",
            "20:48:47.512: training frame: Frame key: AutoML_2_20230622_204847_training_py_11_sid_8d46    cols: 7    rows: 8000  chunks: 1    size: 98174  checksum: -8362635918557851184\n",
            "20:48:47.512: validation frame: Frame key: py_12_sid_8d46    cols: 7    rows: 2000  chunks: 1    size: 25424  checksum: 8020978961634356592\n",
            "20:48:47.512: leaderboard frame: NULL\n",
            "20:48:47.512: blending frame: NULL\n",
            "20:48:47.512: response column: y_test\n",
            "20:48:47.513: fold column: null\n",
            "20:48:47.513: weights column: null\n",
            "20:48:47.513: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "20:48:47.514: AutoML job created: 2023.06.22 20:48:47.511\n",
            "20:48:47.514: AutoML build started: 2023.06.22 20:48:47.514\n",
            "20:48:47.515: AutoML: starting XGBoost_1_AutoML_2_20230622_204847 model training\n",
            "\n",
            "█\n",
            "20:48:50.896: New leader: XGBoost_1_AutoML_2_20230622_204847, accuracy: 0.757125\n",
            "20:48:50.899: AutoML: starting GLM_1_AutoML_2_20230622_204847 model training\n",
            "\n",
            "█\n",
            "20:48:52.833: AutoML: starting GBM_1_AutoML_2_20230622_204847 model training\n",
            "\n",
            "█\n",
            "20:48:55.834: AutoML: starting XGBoost_2_AutoML_2_20230622_204847 model training\n",
            "\n",
            "██\n",
            "20:48:58.235: AutoML: starting DRF_1_AutoML_2_20230622_204847 model training\n",
            "\n",
            "██\n",
            "20:49:02.329: New leader: DRF_1_AutoML_2_20230622_204847, accuracy: 0.757\n",
            "20:49:02.332: AutoML: starting GBM_2_AutoML_2_20230622_204847 model training\n",
            "\n",
            "█\n",
            "20:49:05.242: AutoML: starting GBM_3_AutoML_2_20230622_204847 model training\n",
            "\n",
            "█\n",
            "20:49:08.58: AutoML: starting GBM_4_AutoML_2_20230622_204847 model training\n",
            "\n",
            "██\n",
            "20:49:11.574: AutoML: starting XGBoost_3_AutoML_2_20230622_204847 model training\n",
            "\n",
            "█\n",
            "20:49:13.631: New leader: XGBoost_3_AutoML_2_20230622_204847, accuracy: 0.7565\n",
            "20:49:13.632: AutoML: starting XRT_1_AutoML_2_20230622_204847 model training\n",
            "\n",
            "█\n",
            "20:49:17.338: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "20:49:17.341: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_2_20230622_204847 model training\n",
            "\n",
            "████\n",
            "20:49:23.764: AutoML: starting StackedEnsemble_AllModels_1_AutoML_2_20230622_204847 model training\n",
            "\n",
            "██████████████████████████████████████████████| (done) 100%\n",
            "\n",
            "20:49:31.98: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "20:49:31.98: AutoML build stopped: 2023.06.22 20:49:31.98\n",
            "20:49:31.98: AutoML build done: built 10 models\n",
            "20:49:31.98: AutoML duration: 43.584 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_sh2o = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))['predict']\n",
        "y_test_sh2o = np.array(svalid1['y_test']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivMaaDNNmYY2",
        "outputId": "a9ea440e-b523-4092-afe0-a32454504970"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "nJrcI3Gn8we2",
        "outputId": "8edebc9f-a597-41db-d144-0ae97d01ac66"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 7 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]\n",
        "\n",
        "label = [\"ArtificialNeuralNetwork\", 'DeepNeuralNetwork',\n",
        "         'KNearestNeighborsClassifier', 'LogisticRegression',\n",
        "         'RandomForestClassifier', 'SupportVectorClassifier',\n",
        "         'XGBoost', type(best_model).__name__, type(sbest_model).__name__ ]\n",
        "\n",
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"DNN\":[DTr,DTe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"H_OD\":[HATr.accuracy()[0][1],HATe.accuracy()[0][1]],\n",
        "    \"H_SOD\":[sHATr.accuracy()[0][1],sHATe.accuracy()[0][1]]\n",
        "    })\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc['Model'] = label\n",
        "\n",
        "acc = acc[['Model', 'train', 'test']]\n",
        "acc['avg'] = round((acc['train'] + acc['test'])/2, 6)\n",
        "acc[acc[\"avg\"] == acc[\"avg\"].max()]\n",
        "acc['BestModel'] = 0\n",
        "for i in range(len(acc)):\n",
        "  if acc['avg'][i] >= 90 and acc['avg'][i] < acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"good\"\n",
        "  elif acc['avg'][i] == acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"best\"\n",
        "  else:\n",
        "    acc.iloc[i,-1] = \"not good\"\n",
        "\n",
        "acc[\"Precision\"] = np.zeros(len(acc))\n",
        "acc[\"Recall\"]    = np.zeros(len(acc))\n",
        "acc[\"F1_Score\"]  = np.zeros(len(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ba6kRO-eI60S"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ANNn = []\n",
        "y_pred_DNNn = []\n",
        "for i in range(len(y_pred_ANN)):\n",
        "  y_pred_ANNn.append(y_pred_ANN[i][0])\n",
        "  y_pred_DNNn.append(y_pred_DNN[i][0])"
      ],
      "metadata": {
        "id": "S7pf8G6ao4oF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [np.array(y_pred_ANNn), np.array(y_pred_DNNn), y_pred_knn,\n",
        "        y_pred_lr, y_pred_rf,\n",
        "        y_pred_svm, y_pred_xgb, y_pred_h2o, y_pred_sh2o]\n",
        "\n",
        "tes  = [y_test_indi_ML, np.array(Dy_test), y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_indi_ML, y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_h2o.copy(), y_test_sh2o.copy()]"
      ],
      "metadata": {
        "id": "D2n2VnW1jKp4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pred)):\n",
        "  p,r,f,_ = precision_recall_fscore_support(tes[i], pred[i],\n",
        "                                            average='macro')\n",
        "  acc.iloc[i,5]= p\n",
        "  acc.iloc[i,6]= r\n",
        "  acc.iloc[i,7]= f\n",
        "  p = 0\n",
        "  r = 0\n",
        "  f = 0\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "2DlSQ29moN9T",
        "outputId": "d49c5aeb-f3bc-4f62-dc4f-e31450b14231"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Model     train    test       avg BestModel  \\\n",
              "ANN        ArtificialNeuralNetwork  0.764875  0.7610  0.762937  not good   \n",
              "DNN              DeepNeuralNetwork  0.761875  0.7525  0.757187  not good   \n",
              "KNN    KNearestNeighborsClassifier  0.772250  0.7340  0.753125  not good   \n",
              "LR              LogisticRegression  0.761500  0.7565  0.759000  not good   \n",
              "RF          RandomForestClassifier  0.751750  0.7565  0.754125  not good   \n",
              "SVM        SupportVectorClassifier  0.755750  0.7555  0.755625  not good   \n",
              "XGB                        XGBoost  0.983125  0.7240  0.853562      best   \n",
              "H_OD           H2OXGBoostEstimator  0.894625  0.7385  0.816563  not good   \n",
              "H_SOD          H2OXGBoostEstimator  0.762250  0.7600  0.761125  not good   \n",
              "\n",
              "       Precision    Recall  F1_Score  \n",
              "ANN     0.753332  0.735298  0.740982  \n",
              "DNN     0.745066  0.724215  0.730199  \n",
              "KNN     0.723671  0.705142  0.710304  \n",
              "LR      0.752195  0.725225  0.732152  \n",
              "RF      0.748737  0.729779  0.735544  \n",
              "SVM     0.752952  0.722126  0.729413  \n",
              "XGB     0.709986  0.705351  0.707280  \n",
              "H_OD    0.691755  0.698796  0.676574  \n",
              "H_SOD   0.726268  0.735930  0.727491  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ef3bb99-6933-4e7c-94ec-a5f549b8a2ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>avg</th>\n",
              "      <th>BestModel</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>ArtificialNeuralNetwork</td>\n",
              "      <td>0.764875</td>\n",
              "      <td>0.7610</td>\n",
              "      <td>0.762937</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.753332</td>\n",
              "      <td>0.735298</td>\n",
              "      <td>0.740982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>DeepNeuralNetwork</td>\n",
              "      <td>0.761875</td>\n",
              "      <td>0.7525</td>\n",
              "      <td>0.757187</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.745066</td>\n",
              "      <td>0.724215</td>\n",
              "      <td>0.730199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>KNearestNeighborsClassifier</td>\n",
              "      <td>0.772250</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.753125</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.723671</td>\n",
              "      <td>0.705142</td>\n",
              "      <td>0.710304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.7565</td>\n",
              "      <td>0.759000</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.752195</td>\n",
              "      <td>0.725225</td>\n",
              "      <td>0.732152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.751750</td>\n",
              "      <td>0.7565</td>\n",
              "      <td>0.754125</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.748737</td>\n",
              "      <td>0.729779</td>\n",
              "      <td>0.735544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>SupportVectorClassifier</td>\n",
              "      <td>0.755750</td>\n",
              "      <td>0.7555</td>\n",
              "      <td>0.755625</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.752952</td>\n",
              "      <td>0.722126</td>\n",
              "      <td>0.729413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.983125</td>\n",
              "      <td>0.7240</td>\n",
              "      <td>0.853562</td>\n",
              "      <td>best</td>\n",
              "      <td>0.709986</td>\n",
              "      <td>0.705351</td>\n",
              "      <td>0.707280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_OD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.7385</td>\n",
              "      <td>0.816563</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.691755</td>\n",
              "      <td>0.698796</td>\n",
              "      <td>0.676574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_SOD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.762250</td>\n",
              "      <td>0.7600</td>\n",
              "      <td>0.761125</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.726268</td>\n",
              "      <td>0.735930</td>\n",
              "      <td>0.727491</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ef3bb99-6933-4e7c-94ec-a5f549b8a2ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ef3bb99-6933-4e7c-94ec-a5f549b8a2ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ef3bb99-6933-4e7c-94ec-a5f549b8a2ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ru4oH6-YzGPw",
        "outputId": "0cb20df6-f622-4104-b6d7-9e7838f0ad4e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 9 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "Qma6hWrmDBvf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]"
      ],
      "metadata": {
        "id": "03rwpiYpoAGs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder = LabelEncoder()\n",
        "#y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "#X = df.drop(columns=['diagnosis']).copy()\n",
        "#X = StandardScaler().fit_transform(X).copy()\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "#y_test_indi_ML = y_test.copy()"
      ],
      "metadata": {
        "id": "x5wrzn-qPaXw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
        "\n",
        "y_pred = ANNmodel.predict(X_test).ravel()\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[0]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = model.predict(DX_test).ravel()\n",
        "y_test = Dy_test.copy()\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[1]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = knn.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[2]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = lr.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[3]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = rf.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[4]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = svm.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[5]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = xgb.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[6]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(best_model.predict(valid)))\n",
        "y_test = h2o.as_list(valid['diagnosis'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(best_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))\n",
        "y_test = h2o.as_list(svalid['y_test'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(sbest_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('AUC ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "4Bi4CMUJm6yH",
        "outputId": "de790aa8-978a-4a7d-9b99-e27872f4b6a8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 2ms/step\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n",
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}