{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashiqurrahmankhan21st/BreastCancer/blob/main/Breast_Cancer_V4_Gaussion_Cop_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EUMYU_d6_J-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "#import keras_metrics\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gaussion_Cop\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/GC_SDV_BreastCancer.csv\")\n",
        "del df['Unnamed: 0']\n",
        "df"
      ],
      "metadata": {
        "id": "Ntdqq4T_aGXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "f598f58d-b201-476a-ab95-227009558019"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0            B       12.759         17.02           80.93      509.4   \n",
              "1            M       11.306         19.79           74.48      390.1   \n",
              "2            M       12.325         24.63           81.38      484.2   \n",
              "3            B       10.635         24.89           68.67      341.9   \n",
              "4            M       28.110         29.74          188.50     2286.8   \n",
              "...        ...          ...           ...             ...        ...   \n",
              "9995         M       13.590         25.97           88.89      597.2   \n",
              "9996         B       13.292         19.63           86.72      554.2   \n",
              "9997         B        9.346         22.18           60.79      275.9   \n",
              "9998         B       10.727         19.67           68.48      352.5   \n",
              "9999         B       11.389         15.72           71.71      418.7   \n",
              "\n",
              "      smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0             0.06687           0.03913        0.025046             0.016961   \n",
              "1             0.10894           0.10373        0.114211             0.053879   \n",
              "2             0.11996           0.16272        0.067551             0.047048   \n",
              "3             0.08418           0.08830        0.027259             0.006019   \n",
              "4             0.06442           0.06746        0.191878             0.115942   \n",
              "...               ...               ...             ...                  ...   \n",
              "9995          0.09826           0.10244        0.048636             0.036183   \n",
              "9996          0.09190           0.13122        0.059075             0.031142   \n",
              "9997          0.10120           0.09687        0.040308             0.022831   \n",
              "9998          0.08418           0.07468        0.078046             0.030488   \n",
              "9999          0.08828           0.04033        0.008568             0.006621   \n",
              "\n",
              "      symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
              "0            0.1493  ...        13.858          23.49            88.06   \n",
              "1            0.2135  ...        13.389          27.58            90.55   \n",
              "2            0.2181  ...        13.615          31.28            92.97   \n",
              "3            0.1938  ...        11.340          29.58            75.24   \n",
              "4            0.1850  ...        36.040          36.01           232.15   \n",
              "...             ...  ...           ...            ...              ...   \n",
              "9995         0.1586  ...        14.961          34.97            96.72   \n",
              "9996         0.1773  ...        16.561          27.23           109.80   \n",
              "9997         0.1485  ...        10.957          34.94            71.87   \n",
              "9998         0.1610  ...        11.669          30.02            75.03   \n",
              "9999         0.1473  ...        12.433          17.77            78.42   \n",
              "\n",
              "      area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "0          590.4           0.12370            0.13034         0.145731   \n",
              "1          548.5           0.14599            0.26635         0.364920   \n",
              "2          611.2           0.14752            0.37139         0.172141   \n",
              "3          397.9           0.13029            0.15898         0.044588   \n",
              "4         3615.1           0.11879            0.19572         0.576466   \n",
              "...          ...               ...                ...              ...   \n",
              "9995       719.5           0.14368            0.23676         0.144657   \n",
              "9996       875.1           0.13810            0.30831         0.222822   \n",
              "9997       362.1           0.14212            0.20370         0.096103   \n",
              "9998       408.7           0.13676            0.18993         0.211117   \n",
              "9999       486.1           0.12812            0.06561         0.037096   \n",
              "\n",
              "      concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
              "0                 0.075146          0.2831                  0.07432  \n",
              "1                 0.154583          0.4364                  0.10487  \n",
              "2                 0.110892          0.3186                  0.10515  \n",
              "3                 0.052535          0.2564                  0.08163  \n",
              "4                 0.216770          0.4201                  0.06176  \n",
              "...                    ...             ...                      ...  \n",
              "9995              0.096611          0.2627                  0.08677  \n",
              "9996              0.105450          0.3257                  0.08708  \n",
              "9997              0.080021          0.2338                  0.10824  \n",
              "9998              0.100522          0.2035                  0.08088  \n",
              "9999              0.028837          0.1847                  0.05774  \n",
              "\n",
              "[10000 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50864ba9-8a12-48ef-963a-b5fc7e68a6d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B</td>\n",
              "      <td>12.759</td>\n",
              "      <td>17.02</td>\n",
              "      <td>80.93</td>\n",
              "      <td>509.4</td>\n",
              "      <td>0.06687</td>\n",
              "      <td>0.03913</td>\n",
              "      <td>0.025046</td>\n",
              "      <td>0.016961</td>\n",
              "      <td>0.1493</td>\n",
              "      <td>...</td>\n",
              "      <td>13.858</td>\n",
              "      <td>23.49</td>\n",
              "      <td>88.06</td>\n",
              "      <td>590.4</td>\n",
              "      <td>0.12370</td>\n",
              "      <td>0.13034</td>\n",
              "      <td>0.145731</td>\n",
              "      <td>0.075146</td>\n",
              "      <td>0.2831</td>\n",
              "      <td>0.07432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>11.306</td>\n",
              "      <td>19.79</td>\n",
              "      <td>74.48</td>\n",
              "      <td>390.1</td>\n",
              "      <td>0.10894</td>\n",
              "      <td>0.10373</td>\n",
              "      <td>0.114211</td>\n",
              "      <td>0.053879</td>\n",
              "      <td>0.2135</td>\n",
              "      <td>...</td>\n",
              "      <td>13.389</td>\n",
              "      <td>27.58</td>\n",
              "      <td>90.55</td>\n",
              "      <td>548.5</td>\n",
              "      <td>0.14599</td>\n",
              "      <td>0.26635</td>\n",
              "      <td>0.364920</td>\n",
              "      <td>0.154583</td>\n",
              "      <td>0.4364</td>\n",
              "      <td>0.10487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>12.325</td>\n",
              "      <td>24.63</td>\n",
              "      <td>81.38</td>\n",
              "      <td>484.2</td>\n",
              "      <td>0.11996</td>\n",
              "      <td>0.16272</td>\n",
              "      <td>0.067551</td>\n",
              "      <td>0.047048</td>\n",
              "      <td>0.2181</td>\n",
              "      <td>...</td>\n",
              "      <td>13.615</td>\n",
              "      <td>31.28</td>\n",
              "      <td>92.97</td>\n",
              "      <td>611.2</td>\n",
              "      <td>0.14752</td>\n",
              "      <td>0.37139</td>\n",
              "      <td>0.172141</td>\n",
              "      <td>0.110892</td>\n",
              "      <td>0.3186</td>\n",
              "      <td>0.10515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B</td>\n",
              "      <td>10.635</td>\n",
              "      <td>24.89</td>\n",
              "      <td>68.67</td>\n",
              "      <td>341.9</td>\n",
              "      <td>0.08418</td>\n",
              "      <td>0.08830</td>\n",
              "      <td>0.027259</td>\n",
              "      <td>0.006019</td>\n",
              "      <td>0.1938</td>\n",
              "      <td>...</td>\n",
              "      <td>11.340</td>\n",
              "      <td>29.58</td>\n",
              "      <td>75.24</td>\n",
              "      <td>397.9</td>\n",
              "      <td>0.13029</td>\n",
              "      <td>0.15898</td>\n",
              "      <td>0.044588</td>\n",
              "      <td>0.052535</td>\n",
              "      <td>0.2564</td>\n",
              "      <td>0.08163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>28.110</td>\n",
              "      <td>29.74</td>\n",
              "      <td>188.50</td>\n",
              "      <td>2286.8</td>\n",
              "      <td>0.06442</td>\n",
              "      <td>0.06746</td>\n",
              "      <td>0.191878</td>\n",
              "      <td>0.115942</td>\n",
              "      <td>0.1850</td>\n",
              "      <td>...</td>\n",
              "      <td>36.040</td>\n",
              "      <td>36.01</td>\n",
              "      <td>232.15</td>\n",
              "      <td>3615.1</td>\n",
              "      <td>0.11879</td>\n",
              "      <td>0.19572</td>\n",
              "      <td>0.576466</td>\n",
              "      <td>0.216770</td>\n",
              "      <td>0.4201</td>\n",
              "      <td>0.06176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>M</td>\n",
              "      <td>13.590</td>\n",
              "      <td>25.97</td>\n",
              "      <td>88.89</td>\n",
              "      <td>597.2</td>\n",
              "      <td>0.09826</td>\n",
              "      <td>0.10244</td>\n",
              "      <td>0.048636</td>\n",
              "      <td>0.036183</td>\n",
              "      <td>0.1586</td>\n",
              "      <td>...</td>\n",
              "      <td>14.961</td>\n",
              "      <td>34.97</td>\n",
              "      <td>96.72</td>\n",
              "      <td>719.5</td>\n",
              "      <td>0.14368</td>\n",
              "      <td>0.23676</td>\n",
              "      <td>0.144657</td>\n",
              "      <td>0.096611</td>\n",
              "      <td>0.2627</td>\n",
              "      <td>0.08677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>B</td>\n",
              "      <td>13.292</td>\n",
              "      <td>19.63</td>\n",
              "      <td>86.72</td>\n",
              "      <td>554.2</td>\n",
              "      <td>0.09190</td>\n",
              "      <td>0.13122</td>\n",
              "      <td>0.059075</td>\n",
              "      <td>0.031142</td>\n",
              "      <td>0.1773</td>\n",
              "      <td>...</td>\n",
              "      <td>16.561</td>\n",
              "      <td>27.23</td>\n",
              "      <td>109.80</td>\n",
              "      <td>875.1</td>\n",
              "      <td>0.13810</td>\n",
              "      <td>0.30831</td>\n",
              "      <td>0.222822</td>\n",
              "      <td>0.105450</td>\n",
              "      <td>0.3257</td>\n",
              "      <td>0.08708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>B</td>\n",
              "      <td>9.346</td>\n",
              "      <td>22.18</td>\n",
              "      <td>60.79</td>\n",
              "      <td>275.9</td>\n",
              "      <td>0.10120</td>\n",
              "      <td>0.09687</td>\n",
              "      <td>0.040308</td>\n",
              "      <td>0.022831</td>\n",
              "      <td>0.1485</td>\n",
              "      <td>...</td>\n",
              "      <td>10.957</td>\n",
              "      <td>34.94</td>\n",
              "      <td>71.87</td>\n",
              "      <td>362.1</td>\n",
              "      <td>0.14212</td>\n",
              "      <td>0.20370</td>\n",
              "      <td>0.096103</td>\n",
              "      <td>0.080021</td>\n",
              "      <td>0.2338</td>\n",
              "      <td>0.10824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>B</td>\n",
              "      <td>10.727</td>\n",
              "      <td>19.67</td>\n",
              "      <td>68.48</td>\n",
              "      <td>352.5</td>\n",
              "      <td>0.08418</td>\n",
              "      <td>0.07468</td>\n",
              "      <td>0.078046</td>\n",
              "      <td>0.030488</td>\n",
              "      <td>0.1610</td>\n",
              "      <td>...</td>\n",
              "      <td>11.669</td>\n",
              "      <td>30.02</td>\n",
              "      <td>75.03</td>\n",
              "      <td>408.7</td>\n",
              "      <td>0.13676</td>\n",
              "      <td>0.18993</td>\n",
              "      <td>0.211117</td>\n",
              "      <td>0.100522</td>\n",
              "      <td>0.2035</td>\n",
              "      <td>0.08088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>B</td>\n",
              "      <td>11.389</td>\n",
              "      <td>15.72</td>\n",
              "      <td>71.71</td>\n",
              "      <td>418.7</td>\n",
              "      <td>0.08828</td>\n",
              "      <td>0.04033</td>\n",
              "      <td>0.008568</td>\n",
              "      <td>0.006621</td>\n",
              "      <td>0.1473</td>\n",
              "      <td>...</td>\n",
              "      <td>12.433</td>\n",
              "      <td>17.77</td>\n",
              "      <td>78.42</td>\n",
              "      <td>486.1</td>\n",
              "      <td>0.12812</td>\n",
              "      <td>0.06561</td>\n",
              "      <td>0.037096</td>\n",
              "      <td>0.028837</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.05774</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50864ba9-8a12-48ef-963a-b5fc7e68a6d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-50864ba9-8a12-48ef-963a-b5fc7e68a6d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-50864ba9-8a12-48ef-963a-b5fc7e68a6d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()"
      ],
      "metadata": {
        "id": "4cimS259ti6F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diagnosis'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv9sG-P8M1Fe",
        "outputId": "59050f07-9ab2-4829-9798-3b2404f1dcb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    6085\n",
              "M    3915\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()\n",
        "print(\"Original data : \",df.shape)\n",
        "print(\"tarin         : \",X_train.shape)\n",
        "print(\"test          : \",X_test.shape[0])\n",
        "#print(\"validation    : \",X_val.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpyVwv13CTut",
        "outputId": "3e8600db-256a-4049-d084-b81a2985a9d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data :  (10000, 31)\n",
            "tarin         :  (8000, 30)\n",
            "test          :  2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "st = time.time()\n",
        "svm = SVC(C=0.1, gamma='auto', kernel = 'rbf',probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "send = time.time() - st\n",
        "STr = svm.score(X_train, y_train)\n",
        "STe = svm.score(X_test, y_test)\n",
        "y_pred_svm = svm.predict(X_test)"
      ],
      "metadata": {
        "id": "wnRYFOkyEEZ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANN\n",
        "st = time.time()\n",
        "tf.random.set_seed(123)\n",
        "ANNmodel = Sequential()\n",
        "ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "ANNmodel.add(Dense(10, activation='relu'))\n",
        "ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "aend = time.time() - st\n",
        "ATr = ANNmodel.evaluate(X_train,y_train,verbose=0)[1]\n",
        "ATe = ANNmodel.evaluate(X_test,y_test,verbose=0)[1]\n",
        "y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZX7DbN-OIyu",
        "outputId": "c802b11a-b967-4ed7-f09c-9adc7b6341e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 6ms/step - loss: 0.6173 - accuracy: 0.6774 - val_loss: 0.5241 - val_accuracy: 0.7337\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7350 - val_loss: 0.4987 - val_accuracy: 0.7575\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5116 - accuracy: 0.7456 - val_loss: 0.4928 - val_accuracy: 0.7650\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.5053 - accuracy: 0.7475 - val_loss: 0.4895 - val_accuracy: 0.7688\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7517 - val_loss: 0.4877 - val_accuracy: 0.7725\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4998 - accuracy: 0.7535 - val_loss: 0.4875 - val_accuracy: 0.7750\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4979 - accuracy: 0.7557 - val_loss: 0.4878 - val_accuracy: 0.7700\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4958 - accuracy: 0.7550 - val_loss: 0.4874 - val_accuracy: 0.7713\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4952 - accuracy: 0.7550 - val_loss: 0.4867 - val_accuracy: 0.7713\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4937 - accuracy: 0.7561 - val_loss: 0.4864 - val_accuracy: 0.7725\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4923 - accuracy: 0.7585 - val_loss: 0.4866 - val_accuracy: 0.7750\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4915 - accuracy: 0.7563 - val_loss: 0.4866 - val_accuracy: 0.7725\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4904 - accuracy: 0.7590 - val_loss: 0.4862 - val_accuracy: 0.7763\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4897 - accuracy: 0.7586 - val_loss: 0.4860 - val_accuracy: 0.7700\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.7607 - val_loss: 0.4874 - val_accuracy: 0.7700\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4877 - accuracy: 0.7632 - val_loss: 0.4881 - val_accuracy: 0.7700\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.7606 - val_loss: 0.4880 - val_accuracy: 0.7700\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4862 - accuracy: 0.7625 - val_loss: 0.4879 - val_accuracy: 0.7725\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.7610 - val_loss: 0.4891 - val_accuracy: 0.7638\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.7624 - val_loss: 0.4885 - val_accuracy: 0.7663\n",
            "63/63 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "st = time.time()\n",
        "xgb = XGBClassifier(objective='binary:logistic',max_depth= 6,alpha= 50,learning_rate= 0.01,n_estimators=250)\n",
        "xgb.fit(X_train, y_train)\n",
        "xend = time.time() - st\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "XTr = accuracy_score(y_train, xgb.predict(X_train))\n",
        "XTe = accuracy_score(y_test, xgb.predict(X_test))\n",
        "XTr,XTe"
      ],
      "metadata": {
        "id": "PCP82YZ9RjgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f351ba37-6eef-429c-f94f-313fc126a8ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.775375, 0.7555)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "# Define the range of n_neighbors values to test\n",
        "n_neighbors_values = [1,3, 5, 7, 9, 11]\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_n_neighbors = None\n",
        "\n",
        "for n_neighbors in n_neighbors_values:\n",
        "    print(\"Number of Neighbors:\", n_neighbors)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, knn.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "    print('KNN model train accuracy score: {0:0.4f}'.format(train_accuracy))\n",
        "    print('KNN model test accuracy score: {0:0.4f}'.format(test_accuracy))\n",
        "    print()\n",
        "\n",
        "    # Check if the current test accuracy is better than the previous best\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_n_neighbors = n_neighbors\n",
        "print(\"best neighbours: \", best_n_neighbors)\n",
        "\n",
        "st = time.time()\n",
        "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "knn.fit(X_train, y_train)\n",
        "kend = time.time() - st\n",
        "KTr = accuracy_score(y_train, knn.predict(X_train))\n",
        "KTe = accuracy_score(y_test, knn.predict(X_test))\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "KTr,KTe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y_6b33A1imy",
        "outputId": "5ee4f7b0-a754-413d-b5e3-f5beeef9f0a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Neighbors: 1\n",
            "KNN model train accuracy score: 1.0000\n",
            "KNN model test accuracy score: 0.6770\n",
            "\n",
            "Number of Neighbors: 3\n",
            "KNN model train accuracy score: 0.8323\n",
            "KNN model test accuracy score: 0.6955\n",
            "\n",
            "Number of Neighbors: 5\n",
            "KNN model train accuracy score: 0.7936\n",
            "KNN model test accuracy score: 0.7090\n",
            "\n",
            "Number of Neighbors: 7\n",
            "KNN model train accuracy score: 0.7846\n",
            "KNN model test accuracy score: 0.7160\n",
            "\n",
            "Number of Neighbors: 9\n",
            "KNN model train accuracy score: 0.7774\n",
            "KNN model test accuracy score: 0.7275\n",
            "\n",
            "Number of Neighbors: 11\n",
            "KNN model train accuracy score: 0.7722\n",
            "KNN model test accuracy score: 0.7340\n",
            "\n",
            "best neighbours:  11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.77225, 0.734)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RF\n",
        "st = time.time()\n",
        "rf = RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123)\n",
        "rf.fit(X_train, y_train)\n",
        "rend = time.time() - st\n",
        "RTr = accuracy_score(y_train, rf.predict(X_train))\n",
        "RTe = accuracy_score(y_test, rf.predict(X_test))\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "RTr,RTe"
      ],
      "metadata": {
        "id": "k_kyk_E92bSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f38f12-1d92-43b6-d4ac-609252faae26"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.75175, 0.7565)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LR\n",
        "st = time.time()\n",
        "lr = LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123)\n",
        "lr.fit(X_train, y_train)\n",
        "lend = time.time() - st\n",
        "LTr = accuracy_score(y_train, lr.predict(X_train))\n",
        "LTe = accuracy_score(y_test, lr.predict(X_test))\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "LTr,LTe"
      ],
      "metadata": {
        "id": "OWkZmwL23Fm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03df028d-337b-4e9b-dd52-8593c91aa3bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7615, 0.7565)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E9_l6r5f0w5n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CVal(ML):\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/GC_SDV_BreastCancer.csv\")\n",
        "  del df['Unnamed: 0']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "\n",
        "    ML.fit(X_train, y_train)\n",
        "    y_pred_ML = ML.predict(X_test)\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ML)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]+1\n",
        "\n",
        "  y_pred_final = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_final.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "XqaJZ1Mb0xAe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CValANN():\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/GC_SDV_BreastCancer.csv\")\n",
        "  del df['Unnamed: 0']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "    tf.random.set_seed(123)\n",
        "    ANNmodel = Sequential()\n",
        "    ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    ANNmodel.add(Dense(10, activation='relu'))\n",
        "    ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "    ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "    ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "    y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ANN)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]\n",
        "\n",
        "  y_pred_fina = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_fina.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  y_pred_final = []\n",
        "\n",
        "  for i in range(len(y_pred_fina)):\n",
        "    y_pred_final.append(y_pred_fina[i][0])\n",
        "\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "jRa_a7EY0y6B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = pd.DataFrame({\n",
        "    \"SVM\": CVal(SVC(C=0.1, gamma='auto', kernel = 'rbf'))\n",
        "})\n",
        "newdata[\"KNN\"] = CVal(KNeighborsClassifier(n_neighbors=3))\n",
        "newdata[\"RF\"]  = CVal(RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123))\n",
        "newdata['LR']  = CVal(LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123))\n",
        "newdata[\"ANN\"] = CValANN()\n",
        "newdata[\"XGB\"] = CVal(XGBClassifier(objective='binary:logistic',max_depth= 7,alpha= 10,learning_rate= 1,n_estimators=100))\n",
        "newdata['y_test'] = encoder.fit_transform(df['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CGgdO1V0zA9",
        "outputId": "9f0752a0-3955-4cfb-eda3-f9177aaeae68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 6ms/step - loss: 0.5767 - accuracy: 0.7065 - val_loss: 0.5151 - val_accuracy: 0.7475\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5119 - accuracy: 0.7485 - val_loss: 0.4953 - val_accuracy: 0.7650\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.7539 - val_loss: 0.4888 - val_accuracy: 0.7700\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4983 - accuracy: 0.7569 - val_loss: 0.4863 - val_accuracy: 0.7600\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4955 - accuracy: 0.7593 - val_loss: 0.4843 - val_accuracy: 0.7713\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.7590 - val_loss: 0.4857 - val_accuracy: 0.7675\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4920 - accuracy: 0.7626 - val_loss: 0.4847 - val_accuracy: 0.7650\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4906 - accuracy: 0.7622 - val_loss: 0.4848 - val_accuracy: 0.7663\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4894 - accuracy: 0.7656 - val_loss: 0.4826 - val_accuracy: 0.7650\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4876 - accuracy: 0.7653 - val_loss: 0.4843 - val_accuracy: 0.7625\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4869 - accuracy: 0.7635 - val_loss: 0.4856 - val_accuracy: 0.7588\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4864 - accuracy: 0.7669 - val_loss: 0.4852 - val_accuracy: 0.7650\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4852 - accuracy: 0.7671 - val_loss: 0.4865 - val_accuracy: 0.7625\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4848 - accuracy: 0.7657 - val_loss: 0.4855 - val_accuracy: 0.7625\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4833 - accuracy: 0.7686 - val_loss: 0.4855 - val_accuracy: 0.7638\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4821 - accuracy: 0.7676 - val_loss: 0.4869 - val_accuracy: 0.7625\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4809 - accuracy: 0.7689 - val_loss: 0.4866 - val_accuracy: 0.7563\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4801 - accuracy: 0.7701 - val_loss: 0.4864 - val_accuracy: 0.7588\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.7696 - val_loss: 0.4863 - val_accuracy: 0.7575\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7671 - val_loss: 0.4857 - val_accuracy: 0.7600\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 5ms/step - loss: 0.5754 - accuracy: 0.7033 - val_loss: 0.5072 - val_accuracy: 0.7450\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5123 - accuracy: 0.7471 - val_loss: 0.4908 - val_accuracy: 0.7638\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5060 - accuracy: 0.7535 - val_loss: 0.4863 - val_accuracy: 0.7675\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7531 - val_loss: 0.4852 - val_accuracy: 0.7650\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4995 - accuracy: 0.7549 - val_loss: 0.4840 - val_accuracy: 0.7638\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4985 - accuracy: 0.7558 - val_loss: 0.4847 - val_accuracy: 0.7638\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4962 - accuracy: 0.7553 - val_loss: 0.4840 - val_accuracy: 0.7675\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4947 - accuracy: 0.7588 - val_loss: 0.4843 - val_accuracy: 0.7650\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4935 - accuracy: 0.7601 - val_loss: 0.4830 - val_accuracy: 0.7713\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4919 - accuracy: 0.7607 - val_loss: 0.4835 - val_accuracy: 0.7713\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7606 - val_loss: 0.4834 - val_accuracy: 0.7638\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4909 - accuracy: 0.7638 - val_loss: 0.4842 - val_accuracy: 0.7663\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4894 - accuracy: 0.7622 - val_loss: 0.4866 - val_accuracy: 0.7638\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4889 - accuracy: 0.7597 - val_loss: 0.4843 - val_accuracy: 0.7625\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4874 - accuracy: 0.7614 - val_loss: 0.4836 - val_accuracy: 0.7638\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4866 - accuracy: 0.7646 - val_loss: 0.4835 - val_accuracy: 0.7638\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.7619 - val_loss: 0.4823 - val_accuracy: 0.7650\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4841 - accuracy: 0.7644 - val_loss: 0.4842 - val_accuracy: 0.7675\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4843 - accuracy: 0.7629 - val_loss: 0.4842 - val_accuracy: 0.7663\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.7632 - val_loss: 0.4827 - val_accuracy: 0.7700\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 5ms/step - loss: 0.6543 - accuracy: 0.6285 - val_loss: 0.5960 - val_accuracy: 0.7300\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5581 - accuracy: 0.7410 - val_loss: 0.5153 - val_accuracy: 0.7387\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7490 - val_loss: 0.5010 - val_accuracy: 0.7487\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5137 - accuracy: 0.7489 - val_loss: 0.4940 - val_accuracy: 0.7538\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5090 - accuracy: 0.7497 - val_loss: 0.4900 - val_accuracy: 0.7500\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5070 - accuracy: 0.7521 - val_loss: 0.4874 - val_accuracy: 0.7550\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5038 - accuracy: 0.7538 - val_loss: 0.4853 - val_accuracy: 0.7563\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5021 - accuracy: 0.7547 - val_loss: 0.4847 - val_accuracy: 0.7613\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7571 - val_loss: 0.4834 - val_accuracy: 0.7600\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4989 - accuracy: 0.7581 - val_loss: 0.4822 - val_accuracy: 0.7575\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7576 - val_loss: 0.4810 - val_accuracy: 0.7625\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4968 - accuracy: 0.7586 - val_loss: 0.4815 - val_accuracy: 0.7675\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4949 - accuracy: 0.7606 - val_loss: 0.4821 - val_accuracy: 0.7738\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4955 - accuracy: 0.7592 - val_loss: 0.4796 - val_accuracy: 0.7713\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.7611 - val_loss: 0.4781 - val_accuracy: 0.7738\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7651 - val_loss: 0.4783 - val_accuracy: 0.7663\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4909 - accuracy: 0.7639 - val_loss: 0.4787 - val_accuracy: 0.7613\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4902 - accuracy: 0.7646 - val_loss: 0.4770 - val_accuracy: 0.7725\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4882 - accuracy: 0.7638 - val_loss: 0.4789 - val_accuracy: 0.7713\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4878 - accuracy: 0.7653 - val_loss: 0.4787 - val_accuracy: 0.7725\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 5ms/step - loss: 0.5532 - accuracy: 0.7164 - val_loss: 0.5004 - val_accuracy: 0.7663\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7489 - val_loss: 0.4870 - val_accuracy: 0.7650\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.7542 - val_loss: 0.4831 - val_accuracy: 0.7625\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7572 - val_loss: 0.4827 - val_accuracy: 0.7650\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4990 - accuracy: 0.7564 - val_loss: 0.4812 - val_accuracy: 0.7650\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4970 - accuracy: 0.7592 - val_loss: 0.4813 - val_accuracy: 0.7663\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4946 - accuracy: 0.7604 - val_loss: 0.4809 - val_accuracy: 0.7600\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.7596 - val_loss: 0.4802 - val_accuracy: 0.7613\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.7603 - val_loss: 0.4812 - val_accuracy: 0.7688\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.7618 - val_loss: 0.4811 - val_accuracy: 0.7613\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4899 - accuracy: 0.7636 - val_loss: 0.4801 - val_accuracy: 0.7650\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4891 - accuracy: 0.7628 - val_loss: 0.4811 - val_accuracy: 0.7613\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7657 - val_loss: 0.4813 - val_accuracy: 0.7550\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.7644 - val_loss: 0.4805 - val_accuracy: 0.7525\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.7653 - val_loss: 0.4788 - val_accuracy: 0.7613\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4856 - accuracy: 0.7653 - val_loss: 0.4788 - val_accuracy: 0.7575\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.7660 - val_loss: 0.4802 - val_accuracy: 0.7550\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4838 - accuracy: 0.7679 - val_loss: 0.4798 - val_accuracy: 0.7563\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.7665 - val_loss: 0.4810 - val_accuracy: 0.7625\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7672 - val_loss: 0.4822 - val_accuracy: 0.7550\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1s 5ms/step - loss: 0.5921 - accuracy: 0.6979 - val_loss: 0.5421 - val_accuracy: 0.7412\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5195 - accuracy: 0.7465 - val_loss: 0.5169 - val_accuracy: 0.7638\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5074 - accuracy: 0.7578 - val_loss: 0.5086 - val_accuracy: 0.7600\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7567 - val_loss: 0.5053 - val_accuracy: 0.7613\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4988 - accuracy: 0.7575 - val_loss: 0.5026 - val_accuracy: 0.7638\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7597 - val_loss: 0.5029 - val_accuracy: 0.7650\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4939 - accuracy: 0.7619 - val_loss: 0.5028 - val_accuracy: 0.7625\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.7619 - val_loss: 0.5022 - val_accuracy: 0.7638\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4913 - accuracy: 0.7614 - val_loss: 0.5004 - val_accuracy: 0.7625\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4896 - accuracy: 0.7643 - val_loss: 0.5006 - val_accuracy: 0.7663\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.7657 - val_loss: 0.5000 - val_accuracy: 0.7638\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4879 - accuracy: 0.7664 - val_loss: 0.4988 - val_accuracy: 0.7650\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4861 - accuracy: 0.7675 - val_loss: 0.5017 - val_accuracy: 0.7625\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4858 - accuracy: 0.7665 - val_loss: 0.4999 - val_accuracy: 0.7638\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4843 - accuracy: 0.7675 - val_loss: 0.4988 - val_accuracy: 0.7688\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4834 - accuracy: 0.7676 - val_loss: 0.4997 - val_accuracy: 0.7663\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4827 - accuracy: 0.7675 - val_loss: 0.4989 - val_accuracy: 0.7650\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.4826 - accuracy: 0.7674 - val_loss: 0.4994 - val_accuracy: 0.7625\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7706 - val_loss: 0.4979 - val_accuracy: 0.7688\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.7693 - val_loss: 0.5009 - val_accuracy: 0.7650\n",
            "63/63 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newdata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "05ryfvyLzFVK",
        "outputId": "e354c59b-911c-4817-88b6-c4859869d691"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SVM  KNN  RF  LR  ANN  XGB  y_test\n",
              "0    0    0   0   0    0    0       0\n",
              "1    0    0   0   0    0    0       1\n",
              "2    1    1   1   0    1    1       1\n",
              "3    0    0   0   0    0    1       0\n",
              "4    1    1   1   1    1    1       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86066a9e-05e8-4a96-890f-6f2eb495f983\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SVM</th>\n",
              "      <th>KNN</th>\n",
              "      <th>RF</th>\n",
              "      <th>LR</th>\n",
              "      <th>ANN</th>\n",
              "      <th>XGB</th>\n",
              "      <th>y_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86066a9e-05e8-4a96-890f-6f2eb495f983')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86066a9e-05e8-4a96-890f-6f2eb495f983 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86066a9e-05e8-4a96-890f-6f2eb495f983');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DNN model\n",
        "DNNX = newdata.drop(columns=['y_test']).copy()\n",
        "DNNY = newdata.y_test.copy()\n",
        "DX_train, DX_test, Dy_train, Dy_test = train_test_split(\n",
        "    DNNX, DNNY, test_size=0.2, random_state=123)\n",
        "\n",
        "st = time.time()\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(30, activation='relu', input_shape=DNNX.shape[1:]),\n",
        "    tf.keras.layers.Dense(25, activation='relu'),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dense(15, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(DX_train, Dy_train, epochs=500, batch_size=64, validation_split=0.2)\n",
        "dend = time.time() - st"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4kf97a3yX15",
        "outputId": "007924e8-730a-44fa-a799-4065fabc5733"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "100/100 [==============================] - 2s 5ms/step - loss: 0.6586 - accuracy: 0.7298 - val_loss: 0.6342 - val_accuracy: 0.7556\n",
            "Epoch 2/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6301 - accuracy: 0.7539 - val_loss: 0.6202 - val_accuracy: 0.7631\n",
            "Epoch 3/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6154 - accuracy: 0.7555 - val_loss: 0.6045 - val_accuracy: 0.7688\n",
            "Epoch 4/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6045 - accuracy: 0.7566 - val_loss: 0.5918 - val_accuracy: 0.7681\n",
            "Epoch 5/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5946 - accuracy: 0.7558 - val_loss: 0.5818 - val_accuracy: 0.7681\n",
            "Epoch 6/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5869 - accuracy: 0.7542 - val_loss: 0.5743 - val_accuracy: 0.7681\n",
            "Epoch 7/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5786 - accuracy: 0.7563 - val_loss: 0.5658 - val_accuracy: 0.7694\n",
            "Epoch 8/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5729 - accuracy: 0.7550 - val_loss: 0.5605 - val_accuracy: 0.7681\n",
            "Epoch 9/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5678 - accuracy: 0.7566 - val_loss: 0.5539 - val_accuracy: 0.7669\n",
            "Epoch 10/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5626 - accuracy: 0.7564 - val_loss: 0.5489 - val_accuracy: 0.7688\n",
            "Epoch 11/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5588 - accuracy: 0.7564 - val_loss: 0.5451 - val_accuracy: 0.7669\n",
            "Epoch 12/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5562 - accuracy: 0.7547 - val_loss: 0.5419 - val_accuracy: 0.7681\n",
            "Epoch 13/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5523 - accuracy: 0.7563 - val_loss: 0.5382 - val_accuracy: 0.7688\n",
            "Epoch 14/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5493 - accuracy: 0.7569 - val_loss: 0.5355 - val_accuracy: 0.7688\n",
            "Epoch 15/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5480 - accuracy: 0.7577 - val_loss: 0.5342 - val_accuracy: 0.7681\n",
            "Epoch 16/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5457 - accuracy: 0.7569 - val_loss: 0.5310 - val_accuracy: 0.7688\n",
            "Epoch 17/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5438 - accuracy: 0.7566 - val_loss: 0.5292 - val_accuracy: 0.7675\n",
            "Epoch 18/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5422 - accuracy: 0.7584 - val_loss: 0.5282 - val_accuracy: 0.7681\n",
            "Epoch 19/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5412 - accuracy: 0.7567 - val_loss: 0.5268 - val_accuracy: 0.7681\n",
            "Epoch 20/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5398 - accuracy: 0.7570 - val_loss: 0.5263 - val_accuracy: 0.7675\n",
            "Epoch 21/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5394 - accuracy: 0.7581 - val_loss: 0.5266 - val_accuracy: 0.7725\n",
            "Epoch 22/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5387 - accuracy: 0.7589 - val_loss: 0.5266 - val_accuracy: 0.7675\n",
            "Epoch 23/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5374 - accuracy: 0.7592 - val_loss: 0.5277 - val_accuracy: 0.7694\n",
            "Epoch 24/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5376 - accuracy: 0.7575 - val_loss: 0.5271 - val_accuracy: 0.7669\n",
            "Epoch 25/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5384 - accuracy: 0.7588 - val_loss: 0.5248 - val_accuracy: 0.7688\n",
            "Epoch 26/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5362 - accuracy: 0.7592 - val_loss: 0.5253 - val_accuracy: 0.7656\n",
            "Epoch 27/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5368 - accuracy: 0.7592 - val_loss: 0.5224 - val_accuracy: 0.7675\n",
            "Epoch 28/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5364 - accuracy: 0.7594 - val_loss: 0.5222 - val_accuracy: 0.7675\n",
            "Epoch 29/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5358 - accuracy: 0.7591 - val_loss: 0.5221 - val_accuracy: 0.7650\n",
            "Epoch 30/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.7597 - val_loss: 0.5214 - val_accuracy: 0.7663\n",
            "Epoch 31/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5360 - accuracy: 0.7602 - val_loss: 0.5233 - val_accuracy: 0.7663\n",
            "Epoch 32/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5355 - accuracy: 0.7594 - val_loss: 0.5226 - val_accuracy: 0.7663\n",
            "Epoch 33/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7586 - val_loss: 0.5243 - val_accuracy: 0.7650\n",
            "Epoch 34/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7586 - val_loss: 0.5258 - val_accuracy: 0.7681\n",
            "Epoch 35/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.7589 - val_loss: 0.5216 - val_accuracy: 0.7656\n",
            "Epoch 36/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7605 - val_loss: 0.5234 - val_accuracy: 0.7669\n",
            "Epoch 37/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7603 - val_loss: 0.5212 - val_accuracy: 0.7669\n",
            "Epoch 38/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7594 - val_loss: 0.5229 - val_accuracy: 0.7669\n",
            "Epoch 39/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.7594 - val_loss: 0.5228 - val_accuracy: 0.7669\n",
            "Epoch 40/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7597 - val_loss: 0.5225 - val_accuracy: 0.7650\n",
            "Epoch 41/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7597 - val_loss: 0.5227 - val_accuracy: 0.7669\n",
            "Epoch 42/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7595 - val_loss: 0.5222 - val_accuracy: 0.7694\n",
            "Epoch 43/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7589 - val_loss: 0.5216 - val_accuracy: 0.7656\n",
            "Epoch 44/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7598 - val_loss: 0.5221 - val_accuracy: 0.7656\n",
            "Epoch 45/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7588 - val_loss: 0.5230 - val_accuracy: 0.7681\n",
            "Epoch 46/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5343 - accuracy: 0.7605 - val_loss: 0.5244 - val_accuracy: 0.7656\n",
            "Epoch 47/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7606 - val_loss: 0.5231 - val_accuracy: 0.7669\n",
            "Epoch 48/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7600 - val_loss: 0.5214 - val_accuracy: 0.7656\n",
            "Epoch 49/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7595 - val_loss: 0.5224 - val_accuracy: 0.7663\n",
            "Epoch 50/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7592 - val_loss: 0.5241 - val_accuracy: 0.7656\n",
            "Epoch 51/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7595 - val_loss: 0.5244 - val_accuracy: 0.7675\n",
            "Epoch 52/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7602 - val_loss: 0.5212 - val_accuracy: 0.7669\n",
            "Epoch 53/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7611 - val_loss: 0.5216 - val_accuracy: 0.7656\n",
            "Epoch 54/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7588 - val_loss: 0.5228 - val_accuracy: 0.7669\n",
            "Epoch 55/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7609 - val_loss: 0.5219 - val_accuracy: 0.7675\n",
            "Epoch 56/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7584 - val_loss: 0.5229 - val_accuracy: 0.7675\n",
            "Epoch 57/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7586 - val_loss: 0.5220 - val_accuracy: 0.7669\n",
            "Epoch 58/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7605 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 59/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7602 - val_loss: 0.5235 - val_accuracy: 0.7669\n",
            "Epoch 60/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7608 - val_loss: 0.5221 - val_accuracy: 0.7669\n",
            "Epoch 61/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7602 - val_loss: 0.5225 - val_accuracy: 0.7669\n",
            "Epoch 62/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7611 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 63/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5336 - accuracy: 0.7602 - val_loss: 0.5217 - val_accuracy: 0.7669\n",
            "Epoch 64/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5339 - accuracy: 0.7613 - val_loss: 0.5220 - val_accuracy: 0.7675\n",
            "Epoch 65/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5333 - accuracy: 0.7602 - val_loss: 0.5225 - val_accuracy: 0.7669\n",
            "Epoch 66/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5338 - accuracy: 0.7600 - val_loss: 0.5223 - val_accuracy: 0.7675\n",
            "Epoch 67/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7609 - val_loss: 0.5231 - val_accuracy: 0.7675\n",
            "Epoch 68/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7594 - val_loss: 0.5214 - val_accuracy: 0.7669\n",
            "Epoch 69/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7588 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 70/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7606 - val_loss: 0.5226 - val_accuracy: 0.7675\n",
            "Epoch 71/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7606 - val_loss: 0.5216 - val_accuracy: 0.7669\n",
            "Epoch 72/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7603 - val_loss: 0.5236 - val_accuracy: 0.7669\n",
            "Epoch 73/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7600 - val_loss: 0.5225 - val_accuracy: 0.7675\n",
            "Epoch 74/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7597 - val_loss: 0.5244 - val_accuracy: 0.7719\n",
            "Epoch 75/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7606 - val_loss: 0.5233 - val_accuracy: 0.7669\n",
            "Epoch 76/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5341 - accuracy: 0.7603 - val_loss: 0.5228 - val_accuracy: 0.7669\n",
            "Epoch 77/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7603 - val_loss: 0.5240 - val_accuracy: 0.7663\n",
            "Epoch 78/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7605 - val_loss: 0.5224 - val_accuracy: 0.7669\n",
            "Epoch 79/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5246 - val_accuracy: 0.7700\n",
            "Epoch 80/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7609 - val_loss: 0.5234 - val_accuracy: 0.7675\n",
            "Epoch 81/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7606 - val_loss: 0.5223 - val_accuracy: 0.7669\n",
            "Epoch 82/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7592 - val_loss: 0.5226 - val_accuracy: 0.7669\n",
            "Epoch 83/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7614 - val_loss: 0.5226 - val_accuracy: 0.7675\n",
            "Epoch 84/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7598 - val_loss: 0.5222 - val_accuracy: 0.7675\n",
            "Epoch 85/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7603 - val_loss: 0.5230 - val_accuracy: 0.7675\n",
            "Epoch 86/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7602 - val_loss: 0.5221 - val_accuracy: 0.7675\n",
            "Epoch 87/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7605 - val_loss: 0.5227 - val_accuracy: 0.7656\n",
            "Epoch 88/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7606 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 89/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7605 - val_loss: 0.5226 - val_accuracy: 0.7675\n",
            "Epoch 90/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7608 - val_loss: 0.5234 - val_accuracy: 0.7669\n",
            "Epoch 91/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5229 - val_accuracy: 0.7675\n",
            "Epoch 92/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7589 - val_loss: 0.5227 - val_accuracy: 0.7669\n",
            "Epoch 93/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7609 - val_loss: 0.5243 - val_accuracy: 0.7688\n",
            "Epoch 94/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7605 - val_loss: 0.5222 - val_accuracy: 0.7669\n",
            "Epoch 95/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7602 - val_loss: 0.5221 - val_accuracy: 0.7675\n",
            "Epoch 96/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7606 - val_loss: 0.5224 - val_accuracy: 0.7675\n",
            "Epoch 97/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7609 - val_loss: 0.5241 - val_accuracy: 0.7694\n",
            "Epoch 98/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7603 - val_loss: 0.5239 - val_accuracy: 0.7681\n",
            "Epoch 99/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7602 - val_loss: 0.5227 - val_accuracy: 0.7675\n",
            "Epoch 100/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7595 - val_loss: 0.5230 - val_accuracy: 0.7669\n",
            "Epoch 101/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5331 - accuracy: 0.7608 - val_loss: 0.5228 - val_accuracy: 0.7675\n",
            "Epoch 102/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5330 - accuracy: 0.7595 - val_loss: 0.5244 - val_accuracy: 0.7688\n",
            "Epoch 103/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5334 - accuracy: 0.7611 - val_loss: 0.5230 - val_accuracy: 0.7675\n",
            "Epoch 104/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5231 - val_accuracy: 0.7675\n",
            "Epoch 105/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7616 - val_loss: 0.5223 - val_accuracy: 0.7675\n",
            "Epoch 106/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7609 - val_loss: 0.5235 - val_accuracy: 0.7675\n",
            "Epoch 107/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7608 - val_loss: 0.5228 - val_accuracy: 0.7669\n",
            "Epoch 108/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7609 - val_loss: 0.5249 - val_accuracy: 0.7688\n",
            "Epoch 109/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7594 - val_loss: 0.5227 - val_accuracy: 0.7675\n",
            "Epoch 110/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7606 - val_loss: 0.5230 - val_accuracy: 0.7675\n",
            "Epoch 111/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7600 - val_loss: 0.5230 - val_accuracy: 0.7669\n",
            "Epoch 112/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7613 - val_loss: 0.5230 - val_accuracy: 0.7675\n",
            "Epoch 113/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7606 - val_loss: 0.5231 - val_accuracy: 0.7675\n",
            "Epoch 114/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7606 - val_loss: 0.5238 - val_accuracy: 0.7725\n",
            "Epoch 115/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7609 - val_loss: 0.5229 - val_accuracy: 0.7675\n",
            "Epoch 116/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7611 - val_loss: 0.5228 - val_accuracy: 0.7669\n",
            "Epoch 117/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7589 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 118/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7613 - val_loss: 0.5223 - val_accuracy: 0.7669\n",
            "Epoch 119/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7591 - val_loss: 0.5231 - val_accuracy: 0.7675\n",
            "Epoch 120/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7594 - val_loss: 0.5235 - val_accuracy: 0.7675\n",
            "Epoch 121/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7606 - val_loss: 0.5214 - val_accuracy: 0.7669\n",
            "Epoch 122/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7578 - val_loss: 0.5236 - val_accuracy: 0.7675\n",
            "Epoch 123/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7605 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 124/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7598 - val_loss: 0.5238 - val_accuracy: 0.7675\n",
            "Epoch 125/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7600 - val_loss: 0.5229 - val_accuracy: 0.7675\n",
            "Epoch 126/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5233 - val_accuracy: 0.7669\n",
            "Epoch 127/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7611 - val_loss: 0.5229 - val_accuracy: 0.7675\n",
            "Epoch 128/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7600 - val_loss: 0.5246 - val_accuracy: 0.7713\n",
            "Epoch 129/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7598 - val_loss: 0.5228 - val_accuracy: 0.7675\n",
            "Epoch 130/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7592 - val_loss: 0.5244 - val_accuracy: 0.7713\n",
            "Epoch 131/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.7613 - val_loss: 0.5245 - val_accuracy: 0.7675\n",
            "Epoch 132/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7606 - val_loss: 0.5225 - val_accuracy: 0.7675\n",
            "Epoch 133/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7602 - val_loss: 0.5236 - val_accuracy: 0.7675\n",
            "Epoch 134/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7602 - val_loss: 0.5226 - val_accuracy: 0.7675\n",
            "Epoch 135/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7611 - val_loss: 0.5252 - val_accuracy: 0.7713\n",
            "Epoch 136/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7595 - val_loss: 0.5228 - val_accuracy: 0.7675\n",
            "Epoch 137/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7605 - val_loss: 0.5227 - val_accuracy: 0.7669\n",
            "Epoch 138/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7598 - val_loss: 0.5226 - val_accuracy: 0.7656\n",
            "Epoch 139/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7603 - val_loss: 0.5277 - val_accuracy: 0.7675\n",
            "Epoch 140/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5335 - accuracy: 0.7594 - val_loss: 0.5226 - val_accuracy: 0.7688\n",
            "Epoch 141/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5332 - accuracy: 0.7608 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 142/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5332 - accuracy: 0.7609 - val_loss: 0.5227 - val_accuracy: 0.7688\n",
            "Epoch 143/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7609 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 144/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7613 - val_loss: 0.5233 - val_accuracy: 0.7675\n",
            "Epoch 145/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7603 - val_loss: 0.5229 - val_accuracy: 0.7675\n",
            "Epoch 146/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7611 - val_loss: 0.5261 - val_accuracy: 0.7675\n",
            "Epoch 147/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7616 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 148/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7609 - val_loss: 0.5224 - val_accuracy: 0.7656\n",
            "Epoch 149/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7595 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 150/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7603 - val_loss: 0.5230 - val_accuracy: 0.7675\n",
            "Epoch 151/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7598 - val_loss: 0.5228 - val_accuracy: 0.7656\n",
            "Epoch 152/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7609 - val_loss: 0.5230 - val_accuracy: 0.7700\n",
            "Epoch 153/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7600 - val_loss: 0.5233 - val_accuracy: 0.7675\n",
            "Epoch 154/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7630 - val_loss: 0.5227 - val_accuracy: 0.7669\n",
            "Epoch 155/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7611 - val_loss: 0.5223 - val_accuracy: 0.7675\n",
            "Epoch 156/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7594 - val_loss: 0.5229 - val_accuracy: 0.7675\n",
            "Epoch 157/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7589 - val_loss: 0.5227 - val_accuracy: 0.7669\n",
            "Epoch 158/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7608 - val_loss: 0.5250 - val_accuracy: 0.7719\n",
            "Epoch 159/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7600 - val_loss: 0.5226 - val_accuracy: 0.7669\n",
            "Epoch 160/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7598 - val_loss: 0.5237 - val_accuracy: 0.7669\n",
            "Epoch 161/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7602 - val_loss: 0.5217 - val_accuracy: 0.7669\n",
            "Epoch 162/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7614 - val_loss: 0.5227 - val_accuracy: 0.7688\n",
            "Epoch 163/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7595 - val_loss: 0.5222 - val_accuracy: 0.7675\n",
            "Epoch 164/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7602 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 165/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7611 - val_loss: 0.5236 - val_accuracy: 0.7675\n",
            "Epoch 166/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7603 - val_loss: 0.5223 - val_accuracy: 0.7675\n",
            "Epoch 167/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7602 - val_loss: 0.5238 - val_accuracy: 0.7675\n",
            "Epoch 168/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7608 - val_loss: 0.5228 - val_accuracy: 0.7675\n",
            "Epoch 169/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7613 - val_loss: 0.5237 - val_accuracy: 0.7675\n",
            "Epoch 170/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7595 - val_loss: 0.5224 - val_accuracy: 0.7675\n",
            "Epoch 171/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7597 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 172/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7606 - val_loss: 0.5228 - val_accuracy: 0.7675\n",
            "Epoch 173/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7605 - val_loss: 0.5231 - val_accuracy: 0.7663\n",
            "Epoch 174/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7600 - val_loss: 0.5248 - val_accuracy: 0.7688\n",
            "Epoch 175/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 176/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7606 - val_loss: 0.5229 - val_accuracy: 0.7675\n",
            "Epoch 177/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5329 - accuracy: 0.7602 - val_loss: 0.5242 - val_accuracy: 0.7681\n",
            "Epoch 178/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5329 - accuracy: 0.7609 - val_loss: 0.5222 - val_accuracy: 0.7675\n",
            "Epoch 179/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5330 - accuracy: 0.7600 - val_loss: 0.5230 - val_accuracy: 0.7675\n",
            "Epoch 180/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5331 - accuracy: 0.7602 - val_loss: 0.5233 - val_accuracy: 0.7719\n",
            "Epoch 181/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5329 - accuracy: 0.7595 - val_loss: 0.5227 - val_accuracy: 0.7675\n",
            "Epoch 182/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7608 - val_loss: 0.5230 - val_accuracy: 0.7675\n",
            "Epoch 183/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7600 - val_loss: 0.5233 - val_accuracy: 0.7675\n",
            "Epoch 184/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7600 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 185/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7605 - val_loss: 0.5235 - val_accuracy: 0.7669\n",
            "Epoch 186/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7608 - val_loss: 0.5227 - val_accuracy: 0.7663\n",
            "Epoch 187/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7595 - val_loss: 0.5232 - val_accuracy: 0.7669\n",
            "Epoch 188/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7609 - val_loss: 0.5224 - val_accuracy: 0.7663\n",
            "Epoch 189/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7603 - val_loss: 0.5241 - val_accuracy: 0.7731\n",
            "Epoch 190/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7598 - val_loss: 0.5226 - val_accuracy: 0.7688\n",
            "Epoch 191/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7609 - val_loss: 0.5227 - val_accuracy: 0.7675\n",
            "Epoch 192/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7600 - val_loss: 0.5233 - val_accuracy: 0.7675\n",
            "Epoch 193/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7598 - val_loss: 0.5225 - val_accuracy: 0.7675\n",
            "Epoch 194/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7614 - val_loss: 0.5229 - val_accuracy: 0.7675\n",
            "Epoch 195/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7600 - val_loss: 0.5229 - val_accuracy: 0.7663\n",
            "Epoch 196/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7606 - val_loss: 0.5252 - val_accuracy: 0.7713\n",
            "Epoch 197/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7589 - val_loss: 0.5253 - val_accuracy: 0.7681\n",
            "Epoch 198/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7603 - val_loss: 0.5226 - val_accuracy: 0.7688\n",
            "Epoch 199/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7592 - val_loss: 0.5225 - val_accuracy: 0.7675\n",
            "Epoch 200/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7609 - val_loss: 0.5231 - val_accuracy: 0.7663\n",
            "Epoch 201/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7606 - val_loss: 0.5229 - val_accuracy: 0.7688\n",
            "Epoch 202/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7603 - val_loss: 0.5233 - val_accuracy: 0.7663\n",
            "Epoch 203/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7609 - val_loss: 0.5228 - val_accuracy: 0.7688\n",
            "Epoch 204/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5225 - val_accuracy: 0.7669\n",
            "Epoch 205/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7597 - val_loss: 0.5238 - val_accuracy: 0.7706\n",
            "Epoch 206/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7602 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 207/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7603 - val_loss: 0.5236 - val_accuracy: 0.7675\n",
            "Epoch 208/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7598 - val_loss: 0.5247 - val_accuracy: 0.7688\n",
            "Epoch 209/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5330 - accuracy: 0.7598 - val_loss: 0.5246 - val_accuracy: 0.7669\n",
            "Epoch 210/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7605 - val_loss: 0.5228 - val_accuracy: 0.7663\n",
            "Epoch 211/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7606 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 212/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7620 - val_loss: 0.5224 - val_accuracy: 0.7688\n",
            "Epoch 213/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7606 - val_loss: 0.5226 - val_accuracy: 0.7688\n",
            "Epoch 214/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5328 - accuracy: 0.7602 - val_loss: 0.5230 - val_accuracy: 0.7688\n",
            "Epoch 215/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5327 - accuracy: 0.7608 - val_loss: 0.5235 - val_accuracy: 0.7650\n",
            "Epoch 216/500\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.5328 - accuracy: 0.7609 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 217/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7613 - val_loss: 0.5245 - val_accuracy: 0.7688\n",
            "Epoch 218/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7602 - val_loss: 0.5231 - val_accuracy: 0.7700\n",
            "Epoch 219/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7606 - val_loss: 0.5234 - val_accuracy: 0.7706\n",
            "Epoch 220/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7611 - val_loss: 0.5230 - val_accuracy: 0.7688\n",
            "Epoch 221/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5230 - val_accuracy: 0.7688\n",
            "Epoch 222/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7603 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 223/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7597 - val_loss: 0.5230 - val_accuracy: 0.7688\n",
            "Epoch 224/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7600 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 225/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7598 - val_loss: 0.5238 - val_accuracy: 0.7706\n",
            "Epoch 226/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7606 - val_loss: 0.5228 - val_accuracy: 0.7688\n",
            "Epoch 227/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7597 - val_loss: 0.5228 - val_accuracy: 0.7688\n",
            "Epoch 228/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7608 - val_loss: 0.5228 - val_accuracy: 0.7688\n",
            "Epoch 229/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7602 - val_loss: 0.5247 - val_accuracy: 0.7700\n",
            "Epoch 230/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7597 - val_loss: 0.5230 - val_accuracy: 0.7688\n",
            "Epoch 231/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7597 - val_loss: 0.5242 - val_accuracy: 0.7694\n",
            "Epoch 232/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7609 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 233/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7611 - val_loss: 0.5230 - val_accuracy: 0.7700\n",
            "Epoch 234/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7613 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 235/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7600 - val_loss: 0.5263 - val_accuracy: 0.7675\n",
            "Epoch 236/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7616 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 237/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7614 - val_loss: 0.5243 - val_accuracy: 0.7675\n",
            "Epoch 238/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7602 - val_loss: 0.5230 - val_accuracy: 0.7688\n",
            "Epoch 239/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7606 - val_loss: 0.5231 - val_accuracy: 0.7663\n",
            "Epoch 240/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7616 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 241/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7611 - val_loss: 0.5231 - val_accuracy: 0.7675\n",
            "Epoch 242/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7616 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 243/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7603 - val_loss: 0.5228 - val_accuracy: 0.7688\n",
            "Epoch 244/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7608 - val_loss: 0.5235 - val_accuracy: 0.7700\n",
            "Epoch 245/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7602 - val_loss: 0.5227 - val_accuracy: 0.7688\n",
            "Epoch 246/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7606 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 247/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7605 - val_loss: 0.5244 - val_accuracy: 0.7719\n",
            "Epoch 248/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7600 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 249/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7613 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 250/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5331 - accuracy: 0.7597 - val_loss: 0.5242 - val_accuracy: 0.7688\n",
            "Epoch 251/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5323 - accuracy: 0.7597 - val_loss: 0.5239 - val_accuracy: 0.7663\n",
            "Epoch 252/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5328 - accuracy: 0.7602 - val_loss: 0.5231 - val_accuracy: 0.7663\n",
            "Epoch 253/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5325 - accuracy: 0.7616 - val_loss: 0.5226 - val_accuracy: 0.7688\n",
            "Epoch 254/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5325 - accuracy: 0.7606 - val_loss: 0.5227 - val_accuracy: 0.7688\n",
            "Epoch 255/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7600 - val_loss: 0.5227 - val_accuracy: 0.7688\n",
            "Epoch 256/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7598 - val_loss: 0.5237 - val_accuracy: 0.7675\n",
            "Epoch 257/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7617 - val_loss: 0.5236 - val_accuracy: 0.7675\n",
            "Epoch 258/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7608 - val_loss: 0.5253 - val_accuracy: 0.7688\n",
            "Epoch 259/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7594 - val_loss: 0.5226 - val_accuracy: 0.7688\n",
            "Epoch 260/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7611 - val_loss: 0.5251 - val_accuracy: 0.7700\n",
            "Epoch 261/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7605 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 262/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7605 - val_loss: 0.5233 - val_accuracy: 0.7663\n",
            "Epoch 263/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7602 - val_loss: 0.5244 - val_accuracy: 0.7675\n",
            "Epoch 264/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7613 - val_loss: 0.5230 - val_accuracy: 0.7688\n",
            "Epoch 265/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7602 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 266/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7602 - val_loss: 0.5228 - val_accuracy: 0.7688\n",
            "Epoch 267/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7606 - val_loss: 0.5248 - val_accuracy: 0.7688\n",
            "Epoch 268/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7600 - val_loss: 0.5229 - val_accuracy: 0.7688\n",
            "Epoch 269/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7605 - val_loss: 0.5228 - val_accuracy: 0.7675\n",
            "Epoch 270/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7609 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 271/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7611 - val_loss: 0.5254 - val_accuracy: 0.7675\n",
            "Epoch 272/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7592 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 273/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7603 - val_loss: 0.5227 - val_accuracy: 0.7688\n",
            "Epoch 274/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7605 - val_loss: 0.5252 - val_accuracy: 0.7688\n",
            "Epoch 275/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7619 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 276/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7598 - val_loss: 0.5282 - val_accuracy: 0.7675\n",
            "Epoch 277/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7595 - val_loss: 0.5236 - val_accuracy: 0.7700\n",
            "Epoch 278/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7603 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 279/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7609 - val_loss: 0.5246 - val_accuracy: 0.7688\n",
            "Epoch 280/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7608 - val_loss: 0.5239 - val_accuracy: 0.7675\n",
            "Epoch 281/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7606 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 282/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7608 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 283/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7619 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 284/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7609 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 285/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7611 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 286/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7608 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 287/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5321 - accuracy: 0.7614 - val_loss: 0.5241 - val_accuracy: 0.7688\n",
            "Epoch 288/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5325 - accuracy: 0.7605 - val_loss: 0.5254 - val_accuracy: 0.7713\n",
            "Epoch 289/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5323 - accuracy: 0.7603 - val_loss: 0.5234 - val_accuracy: 0.7675\n",
            "Epoch 290/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7605 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 291/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7605 - val_loss: 0.5238 - val_accuracy: 0.7656\n",
            "Epoch 292/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7606 - val_loss: 0.5225 - val_accuracy: 0.7688\n",
            "Epoch 293/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7613 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 294/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7613 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 295/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7609 - val_loss: 0.5234 - val_accuracy: 0.7675\n",
            "Epoch 296/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7605 - val_loss: 0.5230 - val_accuracy: 0.7688\n",
            "Epoch 297/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7597 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 298/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7600 - val_loss: 0.5250 - val_accuracy: 0.7688\n",
            "Epoch 299/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7603 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 300/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7611 - val_loss: 0.5231 - val_accuracy: 0.7700\n",
            "Epoch 301/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7603 - val_loss: 0.5235 - val_accuracy: 0.7700\n",
            "Epoch 302/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7600 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 303/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7614 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 304/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7597 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 305/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7595 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 306/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7614 - val_loss: 0.5253 - val_accuracy: 0.7688\n",
            "Epoch 307/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7597 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 308/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7603 - val_loss: 0.5244 - val_accuracy: 0.7688\n",
            "Epoch 309/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7606 - val_loss: 0.5220 - val_accuracy: 0.7688\n",
            "Epoch 310/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7603 - val_loss: 0.5243 - val_accuracy: 0.7694\n",
            "Epoch 311/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7603 - val_loss: 0.5240 - val_accuracy: 0.7663\n",
            "Epoch 312/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7608 - val_loss: 0.5231 - val_accuracy: 0.7675\n",
            "Epoch 313/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7602 - val_loss: 0.5254 - val_accuracy: 0.7706\n",
            "Epoch 314/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7606 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 315/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7608 - val_loss: 0.5235 - val_accuracy: 0.7700\n",
            "Epoch 316/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7608 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 317/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7600 - val_loss: 0.5242 - val_accuracy: 0.7675\n",
            "Epoch 318/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7603 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 319/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7595 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 320/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7611 - val_loss: 0.5242 - val_accuracy: 0.7675\n",
            "Epoch 321/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7605 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 322/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7613 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 323/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5328 - accuracy: 0.7603 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 324/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5325 - accuracy: 0.7603 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 325/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7609 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 326/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5322 - accuracy: 0.7609 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 327/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5328 - accuracy: 0.7609 - val_loss: 0.5246 - val_accuracy: 0.7675\n",
            "Epoch 328/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5323 - accuracy: 0.7606 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 329/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7594 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 330/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7598 - val_loss: 0.5234 - val_accuracy: 0.7700\n",
            "Epoch 331/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7595 - val_loss: 0.5226 - val_accuracy: 0.7675\n",
            "Epoch 332/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7600 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 333/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7606 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 334/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7608 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 335/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7603 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 336/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7613 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 337/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7608 - val_loss: 0.5250 - val_accuracy: 0.7688\n",
            "Epoch 338/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7603 - val_loss: 0.5236 - val_accuracy: 0.7675\n",
            "Epoch 339/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7608 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 340/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7608 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 341/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7605 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 342/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7605 - val_loss: 0.5231 - val_accuracy: 0.7700\n",
            "Epoch 343/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7609 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 344/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7602 - val_loss: 0.5245 - val_accuracy: 0.7688\n",
            "Epoch 345/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7605 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 346/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7608 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 347/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7603 - val_loss: 0.5251 - val_accuracy: 0.7694\n",
            "Epoch 348/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5319 - accuracy: 0.7613 - val_loss: 0.5245 - val_accuracy: 0.7669\n",
            "Epoch 349/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7602 - val_loss: 0.5236 - val_accuracy: 0.7675\n",
            "Epoch 350/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7594 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 351/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7614 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 352/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7609 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 353/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7609 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 354/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7609 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 355/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7608 - val_loss: 0.5239 - val_accuracy: 0.7675\n",
            "Epoch 356/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7619 - val_loss: 0.5244 - val_accuracy: 0.7688\n",
            "Epoch 357/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7613 - val_loss: 0.5240 - val_accuracy: 0.7675\n",
            "Epoch 358/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7603 - val_loss: 0.5238 - val_accuracy: 0.7675\n",
            "Epoch 359/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7614 - val_loss: 0.5233 - val_accuracy: 0.7700\n",
            "Epoch 360/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5322 - accuracy: 0.7605 - val_loss: 0.5237 - val_accuracy: 0.7700\n",
            "Epoch 361/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5327 - accuracy: 0.7611 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 362/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5324 - accuracy: 0.7606 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 363/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5320 - accuracy: 0.7609 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 364/500\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.5322 - accuracy: 0.7609 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 365/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7613 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 366/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7605 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 367/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7611 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 368/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7616 - val_loss: 0.5241 - val_accuracy: 0.7719\n",
            "Epoch 369/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7600 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 370/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7595 - val_loss: 0.5239 - val_accuracy: 0.7675\n",
            "Epoch 371/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7606 - val_loss: 0.5242 - val_accuracy: 0.7688\n",
            "Epoch 372/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5332 - accuracy: 0.7605 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 373/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7602 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 374/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7600 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 375/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7606 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 376/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7603 - val_loss: 0.5243 - val_accuracy: 0.7688\n",
            "Epoch 377/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7609 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 378/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7608 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 379/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7611 - val_loss: 0.5251 - val_accuracy: 0.7688\n",
            "Epoch 380/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7595 - val_loss: 0.5241 - val_accuracy: 0.7688\n",
            "Epoch 381/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7595 - val_loss: 0.5244 - val_accuracy: 0.7688\n",
            "Epoch 382/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7606 - val_loss: 0.5241 - val_accuracy: 0.7688\n",
            "Epoch 383/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7603 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 384/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7598 - val_loss: 0.5241 - val_accuracy: 0.7675\n",
            "Epoch 385/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7608 - val_loss: 0.5242 - val_accuracy: 0.7688\n",
            "Epoch 386/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7603 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 387/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5323 - accuracy: 0.7605 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 388/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5320 - accuracy: 0.7613 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 389/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5322 - accuracy: 0.7609 - val_loss: 0.5246 - val_accuracy: 0.7688\n",
            "Epoch 390/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7606 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 391/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7606 - val_loss: 0.5233 - val_accuracy: 0.7675\n",
            "Epoch 392/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7608 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 393/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7609 - val_loss: 0.5237 - val_accuracy: 0.7675\n",
            "Epoch 394/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7608 - val_loss: 0.5245 - val_accuracy: 0.7688\n",
            "Epoch 395/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7606 - val_loss: 0.5238 - val_accuracy: 0.7700\n",
            "Epoch 396/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5322 - accuracy: 0.7606 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 397/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5324 - accuracy: 0.7606 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 398/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5324 - accuracy: 0.7602 - val_loss: 0.5244 - val_accuracy: 0.7688\n",
            "Epoch 399/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5328 - accuracy: 0.7603 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 400/500\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.5325 - accuracy: 0.7594 - val_loss: 0.5238 - val_accuracy: 0.7700\n",
            "Epoch 401/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7613 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 402/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7605 - val_loss: 0.5238 - val_accuracy: 0.7675\n",
            "Epoch 403/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7600 - val_loss: 0.5240 - val_accuracy: 0.7675\n",
            "Epoch 404/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.7600 - val_loss: 0.5245 - val_accuracy: 0.7688\n",
            "Epoch 405/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7606 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 406/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7609 - val_loss: 0.5252 - val_accuracy: 0.7644\n",
            "Epoch 407/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7608 - val_loss: 0.5233 - val_accuracy: 0.7663\n",
            "Epoch 408/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7606 - val_loss: 0.5234 - val_accuracy: 0.7675\n",
            "Epoch 409/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7609 - val_loss: 0.5241 - val_accuracy: 0.7688\n",
            "Epoch 410/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7598 - val_loss: 0.5241 - val_accuracy: 0.7675\n",
            "Epoch 411/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7606 - val_loss: 0.5247 - val_accuracy: 0.7675\n",
            "Epoch 412/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7609 - val_loss: 0.5236 - val_accuracy: 0.7675\n",
            "Epoch 413/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7602 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 414/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7614 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 415/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7603 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 416/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7597 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 417/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7616 - val_loss: 0.5245 - val_accuracy: 0.7675\n",
            "Epoch 418/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7605 - val_loss: 0.5239 - val_accuracy: 0.7663\n",
            "Epoch 419/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7609 - val_loss: 0.5240 - val_accuracy: 0.7675\n",
            "Epoch 420/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7603 - val_loss: 0.5236 - val_accuracy: 0.7675\n",
            "Epoch 421/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7608 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 422/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7613 - val_loss: 0.5254 - val_accuracy: 0.7706\n",
            "Epoch 423/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7592 - val_loss: 0.5254 - val_accuracy: 0.7688\n",
            "Epoch 424/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7603 - val_loss: 0.5243 - val_accuracy: 0.7688\n",
            "Epoch 425/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7606 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 426/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5319 - accuracy: 0.7605 - val_loss: 0.5266 - val_accuracy: 0.7688\n",
            "Epoch 427/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7606 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 428/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7609 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 429/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7606 - val_loss: 0.5251 - val_accuracy: 0.7688\n",
            "Epoch 430/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7606 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 431/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7616 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 432/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5324 - accuracy: 0.7611 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 433/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5323 - accuracy: 0.7608 - val_loss: 0.5244 - val_accuracy: 0.7688\n",
            "Epoch 434/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5324 - accuracy: 0.7600 - val_loss: 0.5231 - val_accuracy: 0.7688\n",
            "Epoch 435/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5328 - accuracy: 0.7606 - val_loss: 0.5237 - val_accuracy: 0.7675\n",
            "Epoch 436/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7602 - val_loss: 0.5240 - val_accuracy: 0.7700\n",
            "Epoch 437/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5323 - accuracy: 0.7609 - val_loss: 0.5237 - val_accuracy: 0.7663\n",
            "Epoch 438/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7603 - val_loss: 0.5252 - val_accuracy: 0.7656\n",
            "Epoch 439/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7603 - val_loss: 0.5242 - val_accuracy: 0.7675\n",
            "Epoch 440/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7614 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 441/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7598 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 442/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7608 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 443/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7603 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 444/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7603 - val_loss: 0.5241 - val_accuracy: 0.7675\n",
            "Epoch 445/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7597 - val_loss: 0.5242 - val_accuracy: 0.7688\n",
            "Epoch 446/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7608 - val_loss: 0.5233 - val_accuracy: 0.7688\n",
            "Epoch 447/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7588 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 448/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7608 - val_loss: 0.5242 - val_accuracy: 0.7688\n",
            "Epoch 449/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7605 - val_loss: 0.5244 - val_accuracy: 0.7688\n",
            "Epoch 450/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7611 - val_loss: 0.5232 - val_accuracy: 0.7688\n",
            "Epoch 451/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7619 - val_loss: 0.5240 - val_accuracy: 0.7706\n",
            "Epoch 452/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7609 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 453/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7613 - val_loss: 0.5239 - val_accuracy: 0.7675\n",
            "Epoch 454/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7602 - val_loss: 0.5242 - val_accuracy: 0.7675\n",
            "Epoch 455/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7608 - val_loss: 0.5242 - val_accuracy: 0.7688\n",
            "Epoch 456/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5319 - accuracy: 0.7602 - val_loss: 0.5245 - val_accuracy: 0.7688\n",
            "Epoch 457/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7606 - val_loss: 0.5242 - val_accuracy: 0.7675\n",
            "Epoch 458/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7609 - val_loss: 0.5237 - val_accuracy: 0.7675\n",
            "Epoch 459/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7606 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 460/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7611 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 461/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7613 - val_loss: 0.5247 - val_accuracy: 0.7688\n",
            "Epoch 462/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7609 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 463/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7602 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 464/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7600 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 465/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5330 - accuracy: 0.7606 - val_loss: 0.5229 - val_accuracy: 0.7688\n",
            "Epoch 466/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7611 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 467/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7600 - val_loss: 0.5244 - val_accuracy: 0.7688\n",
            "Epoch 468/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5322 - accuracy: 0.7611 - val_loss: 0.5234 - val_accuracy: 0.7688\n",
            "Epoch 469/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5325 - accuracy: 0.7614 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 470/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5320 - accuracy: 0.7617 - val_loss: 0.5246 - val_accuracy: 0.7688\n",
            "Epoch 471/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5328 - accuracy: 0.7598 - val_loss: 0.5246 - val_accuracy: 0.7688\n",
            "Epoch 472/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5322 - accuracy: 0.7609 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 473/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 0.7605 - val_loss: 0.5241 - val_accuracy: 0.7688\n",
            "Epoch 474/500\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.5317 - accuracy: 0.7606 - val_loss: 0.5252 - val_accuracy: 0.7688\n",
            "Epoch 475/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7608 - val_loss: 0.5236 - val_accuracy: 0.7688\n",
            "Epoch 476/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7602 - val_loss: 0.5256 - val_accuracy: 0.7675\n",
            "Epoch 477/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7605 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 478/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7611 - val_loss: 0.5235 - val_accuracy: 0.7694\n",
            "Epoch 479/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7597 - val_loss: 0.5240 - val_accuracy: 0.7700\n",
            "Epoch 480/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7608 - val_loss: 0.5239 - val_accuracy: 0.7675\n",
            "Epoch 481/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7600 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 482/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7602 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 483/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7606 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 484/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7608 - val_loss: 0.5235 - val_accuracy: 0.7688\n",
            "Epoch 485/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5320 - accuracy: 0.7609 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 486/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5320 - accuracy: 0.7605 - val_loss: 0.5254 - val_accuracy: 0.7688\n",
            "Epoch 487/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5326 - accuracy: 0.7606 - val_loss: 0.5241 - val_accuracy: 0.7700\n",
            "Epoch 488/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7605 - val_loss: 0.5237 - val_accuracy: 0.7688\n",
            "Epoch 489/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7611 - val_loss: 0.5239 - val_accuracy: 0.7700\n",
            "Epoch 490/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7605 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 491/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7605 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 492/500\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.5319 - accuracy: 0.7613 - val_loss: 0.5241 - val_accuracy: 0.7688\n",
            "Epoch 493/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7603 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 494/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7617 - val_loss: 0.5240 - val_accuracy: 0.7688\n",
            "Epoch 495/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7592 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 496/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7594 - val_loss: 0.5244 - val_accuracy: 0.7675\n",
            "Epoch 497/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5320 - accuracy: 0.7608 - val_loss: 0.5248 - val_accuracy: 0.7675\n",
            "Epoch 498/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7598 - val_loss: 0.5239 - val_accuracy: 0.7688\n",
            "Epoch 499/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7602 - val_loss: 0.5255 - val_accuracy: 0.7688\n",
            "Epoch 500/500\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7611 - val_loss: 0.5260 - val_accuracy: 0.7706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred_DNN = (model.predict(DNNX) > 0.5).astype(\"int32\")\n",
        "DTr = model.evaluate(DX_train, Dy_train,verbose=0)[1]\n",
        "DTe = model.evaluate(DX_test, Dy_test,verbose=0)[1]\n",
        "DTr,DTe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhG0YCbhASH9",
        "outputId": "2e51c552-e833-4a77-d1e7-ae3423193b80"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7628750205039978, 0.7559999823570251)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_DNN = (model.predict(DX_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKbxrS57kOBE",
        "outputId": "ea9dbac0-23b8-460b-952b-cc2c447bf204"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"DNN\":[DTr,DTe]})\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "v_YjlCdOO1JY",
        "outputId": "93716a59-7a27-4e9e-bf99-94f5b9c1426c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        train    test\n",
              "SVM  0.755750  0.7555\n",
              "KNN  0.772250  0.7340\n",
              "RF   0.751750  0.7565\n",
              "LR   0.761500  0.7565\n",
              "ANN  0.764875  0.7505\n",
              "XGB  0.775375  0.7555\n",
              "DNN  0.762875  0.7560"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd283562-3d1f-40ef-acf5-fc5c4534eb6c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.755750</td>\n",
              "      <td>0.7555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>0.772250</td>\n",
              "      <td>0.7340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>0.751750</td>\n",
              "      <td>0.7565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.7565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>0.764875</td>\n",
              "      <td>0.7505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>0.775375</td>\n",
              "      <td>0.7555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>0.762875</td>\n",
              "      <td>0.7560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd283562-3d1f-40ef-acf5-fc5c4534eb6c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd283562-3d1f-40ef-acf5-fc5c4534eb6c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd283562-3d1f-40ef-acf5-fc5c4534eb6c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AutoML Individual and AutoML DNN**"
      ],
      "metadata": {
        "id": "nJaAMLDKKfKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H2O AutoML"
      ],
      "metadata": {
        "id": "EhNt_UkjDKcV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h2o\n",
        "import h2o\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "h2o.init()\n",
        "from h2o.model.segment_models import H2OFrame\n",
        "from h2o.automl import H2OAutoML\n",
        "print(\"All Library Loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "iwMF9ROeDOzF",
        "outputId": "2c8d9f80-3a41-4081-ced2-b1eb1f48e62a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h2o\n",
            "  Downloading h2o-3.40.0.4.tar.gz (177.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.8.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from h2o) (0.18.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.4)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.40.0.4-py2.py3-none-any.whl size=177697886 sha256=43482cd91aa527b5dd2bff45ffc4a9f7142284223a2dbe0aa2179451d624e4c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/f2/b0/5bb4d702a0467e82d77c45088db3eef25114c26b0eec8e7f6a\n",
            "Successfully built h2o\n",
            "Installing collected packages: h2o\n",
            "Successfully installed h2o-3.40.0.4\n",
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.19\" 2023-04-18; OpenJDK Runtime Environment (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1); OpenJDK 64-Bit Server VM (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.10/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpnk3shhba\n",
            "  JVM stdout: /tmp/tmpnk3shhba/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpnk3shhba/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         03 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.40.0.4\n",
              "H2O_cluster_version_age:    2 months and 5 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_mavhl8\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.170 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-1.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-1 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-1 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-1 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table th,\n",
              "#h2o-table-1 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>03 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.40.0.4</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>2 months and 5 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_mavhl8</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.170 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Library Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWdoOLbsF2qs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-QiglXeF_XE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8P1HTOHbugE-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asKywHvIu83f"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5wVvB0pvWHe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train, valid = hdf.split_frame(ratios=[.8], seed=123)\n",
        "#hdf = h2o.H2OFrame(df)\n",
        "#hdf[\"diagnosis\"] = hdf[\"diagnosis\"].asfactor()\n",
        "hy = \"diagnosis\"\n",
        "hx = list(df.columns)\n",
        "hx.remove(hy)\n",
        "hdf  = df.copy()\n",
        "hdf.iloc[:,1:] = StandardScaler().fit_transform(hdf.iloc[:,1:])\n",
        "hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n",
        "hdf.iloc[:,0] = hdf.iloc[:,0].astype('category')\n",
        "train1, valid1 = train_test_split(hdf, test_size=0.2,random_state=123)\n",
        "train = h2o.H2OFrame(train1)\n",
        "valid = h2o.H2OFrame(valid1)\n",
        "train[\"diagnosis\"] = train[\"diagnosis\"].asfactor()\n",
        "valid[\"diagnosis\"] = valid[\"diagnosis\"].asfactor()"
      ],
      "metadata": {
        "id": "NVoFNbYCGxGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436ee19d-4c43-4ac5-fc64-ae9abbe2b2a7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-460708a37676>:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = time.time()\n",
        "aml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\",\n",
        "                nfolds=10, sort_metric='accuracy')\n",
        "aml.train(x = hx, y = hy, training_frame = train,\n",
        "          validation_frame = valid)\n",
        "autoend = time.time() - st"
      ],
      "metadata": {
        "id": "7JNO6XnVHH5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75673a4d-d6be-452b-fb10-31da72c7303b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoML progress: |\n",
            "10:47:02.799: Project: AutoML_1_20230704_104702\n",
            "10:47:02.801: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "10:47:02.807: Setting stopping tolerance adaptively based on the training frame: 0.011180339887498949\n",
            "10:47:02.807: Build control seed: 123\n",
            "10:47:02.808: training frame: Frame key: AutoML_1_20230704_104702_training_py_1_sid_9493    cols: 31    rows: 8000  chunks: 2    size: 1926169  checksum: -5743320669190601360\n",
            "10:47:02.820: validation frame: Frame key: py_2_sid_9493    cols: 31    rows: 2000  chunks: 1    size: 482912  checksum: 7051888715556309816\n",
            "10:47:02.820: leaderboard frame: NULL\n",
            "10:47:02.820: blending frame: NULL\n",
            "10:47:02.820: response column: diagnosis\n",
            "10:47:02.820: fold column: null\n",
            "10:47:02.820: weights column: null\n",
            "10:47:02.869: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "10:47:02.933: AutoML job created: 2023.07.04 10:47:02.734\n",
            "10:47:02.934: AutoML build started: 2023.07.04 10:47:02.933\n",
            "10:47:02.991: AutoML: starting XGBoost_1_AutoML_1_20230704_104702 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "10:47:32.184: New leader: XGBoost_1_AutoML_1_20230704_104702, accuracy: 0.731375\n",
            "10:47:32.190: AutoML: starting GLM_1_AutoML_1_20230704_104702 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "10:47:54.118: AutoML: starting GBM_1_AutoML_1_20230704_104702 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "10:48:25.550: AutoML: starting XGBoost_2_AutoML_1_20230704_104702 model training\n",
            "\n",
            "\n",
            "10:48:41.562: AutoML: starting DRF_1_AutoML_1_20230704_104702 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆ\n",
            "10:49:24.189: AutoML: starting GBM_2_AutoML_1_20230704_104702 model training\n",
            "\n",
            "\n",
            "10:49:41.801: AutoML: starting GBM_3_AutoML_1_20230704_104702 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "10:50:00.446: AutoML: starting GBM_4_AutoML_1_20230704_104702 model training\n",
            "\n",
            "â–ˆ\n",
            "10:50:23.521: AutoML: starting XGBoost_3_AutoML_1_20230704_104702 model training\n",
            "\n",
            "\n",
            "10:50:34.458: AutoML: starting XRT_1_AutoML_1_20230704_104702 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆ\n",
            "10:51:24.497: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "10:51:24.509: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_1_20230704_104702 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "10:51:36.526: AutoML: starting StackedEnsemble_AllModels_1_AutoML_1_20230704_104702 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "\n",
            "10:51:51.168: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "10:51:51.168: AutoML build stopped: 2023.07.04 10:51:51.168\n",
            "10:51:51.168: AutoML build done: built 10 models\n",
            "10:51:51.168: AutoML duration:  4 min 48.235 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lbo606kFH4Zc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lb = aml.leaderboard\n",
        "lb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "gmCXw_D_4y1D",
        "outputId": "83974ac0-0cde-48ce-c481-b65c625c526e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_id                                                   accuracy       auc    logloss     aucpr    mean_per_class_error      rmse       mse\n",
              "-------------------------------------------------------  ----------  --------  ---------  --------  ----------------------  --------  --------\n",
              "XGBoost_1_AutoML_1_20230704_104702                         0.731375  0.789913   0.561354  0.704715                0.281571  0.431458  0.186156\n",
              "XGBoost_2_AutoML_1_20230704_104702                         0.732875  0.786979   0.569302  0.710217                0.292189  0.432906  0.187407\n",
              "GBM_4_AutoML_1_20230704_104702                             0.74325   0.804633   0.521738  0.73211                 0.271387  0.416969  0.173863\n",
              "XGBoost_3_AutoML_1_20230704_104702                         0.746125  0.807007   0.522189  0.733116                0.269031  0.41621   0.173231\n",
              "DRF_1_AutoML_1_20230704_104702                             0.747375  0.804947   0.538154  0.731415                0.271721  0.415924  0.172993\n",
              "XRT_1_AutoML_1_20230704_104702                             0.747875  0.806856   0.550448  0.732292                0.261301  0.415221  0.172409\n",
              "GBM_3_AutoML_1_20230704_104702                             0.750125  0.812317   0.513038  0.737965                0.266742  0.412843  0.170439\n",
              "GBM_2_AutoML_1_20230704_104702                             0.7515    0.813887   0.51051   0.741607                0.261279  0.411848  0.169619\n",
              "GBM_1_AutoML_1_20230704_104702                             0.752625  0.816537   0.506866  0.747142                0.259403  0.409777  0.167917\n",
              "StackedEnsemble_BestOfFamily_1_AutoML_1_20230704_104702    0.760375  0.830465   0.490144  0.765236                0.250482  0.402491  0.161999\n",
              "[10 rows x 8 columns]\n"
            ],
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th>model_id                                               </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>XGBoost_1_AutoML_1_20230704_104702                     </td><td style=\"text-align: right;\">  0.731375</td><td style=\"text-align: right;\">0.789913</td><td style=\"text-align: right;\"> 0.561354</td><td style=\"text-align: right;\">0.704715</td><td style=\"text-align: right;\">              0.281571</td><td style=\"text-align: right;\">0.431458</td><td style=\"text-align: right;\">0.186156</td></tr>\n",
              "<tr><td>XGBoost_2_AutoML_1_20230704_104702                     </td><td style=\"text-align: right;\">  0.732875</td><td style=\"text-align: right;\">0.786979</td><td style=\"text-align: right;\"> 0.569302</td><td style=\"text-align: right;\">0.710217</td><td style=\"text-align: right;\">              0.292189</td><td style=\"text-align: right;\">0.432906</td><td style=\"text-align: right;\">0.187407</td></tr>\n",
              "<tr><td>GBM_4_AutoML_1_20230704_104702                         </td><td style=\"text-align: right;\">  0.74325 </td><td style=\"text-align: right;\">0.804633</td><td style=\"text-align: right;\"> 0.521738</td><td style=\"text-align: right;\">0.73211 </td><td style=\"text-align: right;\">              0.271387</td><td style=\"text-align: right;\">0.416969</td><td style=\"text-align: right;\">0.173863</td></tr>\n",
              "<tr><td>XGBoost_3_AutoML_1_20230704_104702                     </td><td style=\"text-align: right;\">  0.746125</td><td style=\"text-align: right;\">0.807007</td><td style=\"text-align: right;\"> 0.522189</td><td style=\"text-align: right;\">0.733116</td><td style=\"text-align: right;\">              0.269031</td><td style=\"text-align: right;\">0.41621 </td><td style=\"text-align: right;\">0.173231</td></tr>\n",
              "<tr><td>DRF_1_AutoML_1_20230704_104702                         </td><td style=\"text-align: right;\">  0.747375</td><td style=\"text-align: right;\">0.804947</td><td style=\"text-align: right;\"> 0.538154</td><td style=\"text-align: right;\">0.731415</td><td style=\"text-align: right;\">              0.271721</td><td style=\"text-align: right;\">0.415924</td><td style=\"text-align: right;\">0.172993</td></tr>\n",
              "<tr><td>XRT_1_AutoML_1_20230704_104702                         </td><td style=\"text-align: right;\">  0.747875</td><td style=\"text-align: right;\">0.806856</td><td style=\"text-align: right;\"> 0.550448</td><td style=\"text-align: right;\">0.732292</td><td style=\"text-align: right;\">              0.261301</td><td style=\"text-align: right;\">0.415221</td><td style=\"text-align: right;\">0.172409</td></tr>\n",
              "<tr><td>GBM_3_AutoML_1_20230704_104702                         </td><td style=\"text-align: right;\">  0.750125</td><td style=\"text-align: right;\">0.812317</td><td style=\"text-align: right;\"> 0.513038</td><td style=\"text-align: right;\">0.737965</td><td style=\"text-align: right;\">              0.266742</td><td style=\"text-align: right;\">0.412843</td><td style=\"text-align: right;\">0.170439</td></tr>\n",
              "<tr><td>GBM_2_AutoML_1_20230704_104702                         </td><td style=\"text-align: right;\">  0.7515  </td><td style=\"text-align: right;\">0.813887</td><td style=\"text-align: right;\"> 0.51051 </td><td style=\"text-align: right;\">0.741607</td><td style=\"text-align: right;\">              0.261279</td><td style=\"text-align: right;\">0.411848</td><td style=\"text-align: right;\">0.169619</td></tr>\n",
              "<tr><td>GBM_1_AutoML_1_20230704_104702                         </td><td style=\"text-align: right;\">  0.752625</td><td style=\"text-align: right;\">0.816537</td><td style=\"text-align: right;\"> 0.506866</td><td style=\"text-align: right;\">0.747142</td><td style=\"text-align: right;\">              0.259403</td><td style=\"text-align: right;\">0.409777</td><td style=\"text-align: right;\">0.167917</td></tr>\n",
              "<tr><td>StackedEnsemble_BestOfFamily_1_AutoML_1_20230704_104702</td><td style=\"text-align: right;\">  0.760375</td><td style=\"text-align: right;\">0.830465</td><td style=\"text-align: right;\"> 0.490144</td><td style=\"text-align: right;\">0.765236</td><td style=\"text-align: right;\">              0.250482</td><td style=\"text-align: right;\">0.402491</td><td style=\"text-align: right;\">0.161999</td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10 rows x 8 columns]</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "best_model"
      ],
      "metadata": {
        "id": "vlH8zwtisQU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08642eb5-fe65-499a-e3aa-741f28ca7a78"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230704_104702\n",
              "\n",
              "\n",
              "Model Summary: \n",
              "    number_of_trees\n",
              "--  -----------------\n",
              "    30\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.0862762318888866\n",
              "RMSE: 0.2937281598500331\n",
              "LogLoss: 0.2961195502729631\n",
              "Mean Per-Class Error: 0.10560430092631945\n",
              "AUC: 0.9574856020131994\n",
              "AUCPR: 0.9379440756884645\n",
              "Gini: 0.9149712040263989\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40771300310180303\n",
              "       0     1     Error    Rate\n",
              "-----  ----  ----  -------  --------------\n",
              "0      4317  551   0.1132   (551.0/4868.0)\n",
              "1      307   2825  0.098    (307.0/3132.0)\n",
              "Total  4624  3376  0.1072   (858.0/8000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.407713     0.868162  213\n",
              "max f2                       0.291031     0.900738  254\n",
              "max f0point5                 0.608403     0.885993  148\n",
              "max accuracy                 0.44304      0.894625  201\n",
              "max precision                0.993199     1         0\n",
              "max recall                   0.0188069    1         390\n",
              "max specificity              0.993199     1         0\n",
              "max absolute_mcc             0.426081     0.780124  207\n",
              "max min_per_class_accuracy   0.419471     0.89272   209\n",
              "max mean_per_class_accuracy  0.383103     0.894835  222\n",
              "max tns                      0.993199     4868      0\n",
              "max fns                      0.993199     3120      0\n",
              "max fps                      0.0052119    4868      399\n",
              "max tps                      0.0188069    3132      390\n",
              "max tnr                      0.993199     1         0\n",
              "max fnr                      0.993199     0.996169  0\n",
              "max fpr                      0.0052119    1         399\n",
              "max tpr                      0.0188069    1         390\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 39.32 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.983273           2.55428    2.55428            1                0.988143   1                           0.988143            0.0255428       0.0255428                  155.428   155.428            0.0255428\n",
              "2        0.02                        0.974009           2.55428    2.55428            1                0.978908   1                           0.983526            0.0255428       0.0510856                  155.428   155.428            0.0510856\n",
              "3        0.03                        0.965              2.52235    2.54364            0.9875           0.969758   0.995833                    0.978936            0.0252235       0.0763091                  152.235   154.364            0.0761036\n",
              "4        0.04                        0.956618           2.55428    2.5463             1                0.960834   0.996875                    0.974411            0.0255428       0.101852                   155.428   154.63             0.101646\n",
              "5        0.05                        0.948257           2.49042    2.53512            0.975            0.952464   0.9925                      0.970021            0.0249042       0.126756                   149.042   153.512            0.12614\n",
              "6        0.1                         0.891808           2.52235    2.52874            0.9875           0.92015    0.99                        0.945086            0.126117        0.252874                   152.235   152.874            0.25123\n",
              "7        0.15                        0.834331           2.49042    2.51596            0.975            0.864595   0.985                       0.918255            0.124521        0.377395                   149.042   151.596            0.373697\n",
              "8        0.2                         0.778986           2.40741    2.48883            0.9425           0.807305   0.974375                    0.890518            0.12037         0.497765                   140.741   148.883            0.489343\n",
              "9        0.3                         0.626419           2.20626    2.39464            0.86375          0.704981   0.9375                      0.828672            0.220626        0.718391                   120.626   139.464            0.687577\n",
              "10       0.4                         0.439384           1.57727    2.19029            0.6175           0.53251    0.8575                      0.754632            0.157727        0.876117                   57.7267   119.029            0.782445\n",
              "11       0.5                         0.291733           0.740741   1.90038            0.29             0.362017   0.744                       0.676109            0.0740741       0.950192                   -25.9259  90.0383            0.739838\n",
              "12       0.6                         0.186727           0.322478   1.6374             0.12625          0.235522   0.641042                    0.602678            0.0322478       0.982439                   -67.7522  63.7399            0.628495\n",
              "13       0.7                         0.118291           0.0957854  1.41717            0.0375           0.14983    0.554821                    0.537985            0.00957854      0.992018                   -90.4215  41.7168            0.479898\n",
              "14       0.8                         0.0704665          0.0510856  1.24641            0.02             0.0932805  0.487969                    0.482397            0.00510856      0.997126                   -94.8914  24.6408            0.323955\n",
              "15       0.9                         0.0350014          0.0159642  1.10969            0.00625          0.0520768  0.434444                    0.434584            0.00159642      0.998723                   -98.4036  10.9692            0.16224\n",
              "16       1                           0.00378296         0.0127714  1                  0.005            0.0206824  0.3915                      0.393194            0.00127714      1                          -98.7229  0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.18840019637319533\n",
              "RMSE: 0.4340509144941355\n",
              "LogLoss: 0.5694655292500879\n",
              "Mean Per-Class Error: 0.29928818116277384\n",
              "AUC: 0.7805786689418005\n",
              "AUCPR: 0.7132335417937792\n",
              "Gini: 0.561157337883601\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24615386625130972\n",
              "       0    1     Error    Rate\n",
              "-----  ---  ----  -------  --------------\n",
              "0      731  486   0.3993   (486.0/1217.0)\n",
              "1      156  627   0.1992   (156.0/783.0)\n",
              "Total  887  1113  0.321    (642.0/2000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.246154     0.661392  271\n",
              "max f2                       0.0822697    0.783171  350\n",
              "max f0point5                 0.628715     0.680179  134\n",
              "max accuracy                 0.628715     0.7385    134\n",
              "max precision                0.996239     1         0\n",
              "max recall                   0.00914458   1         397\n",
              "max specificity              0.996239     1         0\n",
              "max absolute_mcc             0.628715     0.435211  134\n",
              "max min_per_class_accuracy   0.354121     0.699872  226\n",
              "max mean_per_class_accuracy  0.552612     0.706514  157\n",
              "max tns                      0.996239     1217      0\n",
              "max fns                      0.996239     782       0\n",
              "max fps                      0.00487721   1217      399\n",
              "max tps                      0.00914458   783       397\n",
              "max tnr                      0.996239     1         0\n",
              "max fnr                      0.996239     0.998723  0\n",
              "max fpr                      0.00487721   1         399\n",
              "max tpr                      0.00914458   1         397\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.46 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.977798           2.55428   2.55428            1                0.985284   1                           0.985284            0.0255428       0.0255428                  155.428   155.428            0.0255428\n",
              "2        0.02                        0.964573           2.55428   2.55428            1                0.972878   1                           0.979081            0.0255428       0.0510856                  155.428   155.428            0.0510856\n",
              "3        0.03                        0.954787           2.29885   2.46914            0.9              0.958362   0.966667                    0.972175            0.0229885       0.0740741                  129.885   146.914            0.0724307\n",
              "4        0.04                        0.943778           2.17114   2.39464            0.85             0.950531   0.9375                      0.966764            0.0217114       0.0957854                  117.114   139.464            0.091677\n",
              "5        0.05                        0.928851           2.04342   2.32439            0.8              0.935592   0.91                        0.960529            0.0204342       0.11622                    104.342   132.439            0.108824\n",
              "6        0.1                         0.874916           1.94125   2.13282            0.76             0.902028   0.835                       0.931279            0.0970626       0.213282                   94.1252   113.282            0.186166\n",
              "7        0.15                        0.800584           1.96679   2.07748            0.77             0.837897   0.813333                    0.900151            0.0983397       0.311622                   96.6794   107.748            0.265607\n",
              "8        0.2                         0.727813           1.76245   1.99872            0.69             0.76954    0.7825                      0.867499            0.0881226       0.399745                   76.2452   99.8723            0.328257\n",
              "9        0.3                         0.572579           1.45594   1.81779            0.57             0.649716   0.711667                    0.794904            0.145594        0.545338                   45.5939   81.7795            0.403186\n",
              "10       0.4                         0.427932           0.970626  1.606              0.38             0.50131    0.62875                     0.721506            0.0970626       0.642401                   -2.93742  60.6003            0.398358\n",
              "11       0.5                         0.303219           0.91954   1.46871            0.36             0.363956   0.575                       0.649996            0.091954        0.734355                   -8.04598  46.871             0.385136\n",
              "12       0.6                         0.208844           0.881226  1.3708             0.345            0.252344   0.536667                    0.58372             0.0881226       0.822478                   -11.8774  37.0796            0.365617\n",
              "13       0.7                         0.130075           0.702427  1.27531            0.275            0.165416   0.499286                    0.523963            0.0702427       0.89272                    -29.7573  27.5315            0.316714\n",
              "14       0.8                         0.0777253          0.523627  1.18135            0.205            0.10211    0.4625                      0.471231            0.0523627       0.945083                   -47.6373  18.1354            0.238427\n",
              "15       0.9                         0.0354974          0.332056  1.08699            0.13             0.0543392  0.425556                    0.42491             0.0332056       0.978289                   -66.7944  8.69874            0.128658\n",
              "16       1                           0.00410937         0.217114  1                  0.085            0.0213709  0.3915                      0.384556            0.0217114       1                          -78.2886  0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.18615605059426837\n",
              "RMSE: 0.43145805195206216\n",
              "LogLoss: 0.5613540167599764\n",
              "Mean Per-Class Error: 0.28157062936622623\n",
              "AUC: 0.7899126007045779\n",
              "AUCPR: 0.704715036081932\n",
              "Gini: 0.5798252014091558\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3172192946076393\n",
              "       0     1     Error    Rate\n",
              "-----  ----  ----  -------  ---------------\n",
              "0      3297  1571  0.3227   (1571.0/4868.0)\n",
              "1      753   2379  0.2404   (753.0/3132.0)\n",
              "Total  4050  3950  0.2905   (2324.0/8000.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.317219     0.671844  248\n",
              "max f2                       0.0903366    0.790684  347\n",
              "max f0point5                 0.562613     0.661942  158\n",
              "max accuracy                 0.548578     0.731375  163\n",
              "max precision                0.984683     0.941176  2\n",
              "max recall                   0.00676566   1         398\n",
              "max specificity              0.991555     0.999795  0\n",
              "max absolute_mcc             0.395143     0.434769  217\n",
              "max min_per_class_accuracy   0.369361     0.71871   227\n",
              "max mean_per_class_accuracy  0.395143     0.720713  217\n",
              "max tns                      0.991555     4867      0\n",
              "max fns                      0.991555     3120      0\n",
              "max fps                      0.00360703   4868      399\n",
              "max tps                      0.00676566   3132      398\n",
              "max tnr                      0.991555     0.999795  0\n",
              "max fnr                      0.991555     0.996169  0\n",
              "max fpr                      0.00360703   1         399\n",
              "max tpr                      0.00676566   1         398\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.96 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.01                        0.976782           2.29885   2.29885            0.9              0.98354    0.9                         0.98354             0.0229885       0.0229885                  129.885   129.885            0.0213451\n",
              "2        0.02                        0.96877            2.36271   2.33078            0.925            0.972395   0.9125                      0.977967            0.0236271       0.0466156                  136.271   133.078            0.0437397\n",
              "3        0.03                        0.958395           2.26692   2.30949            0.8875           0.963849   0.904167                    0.973261            0.0226692       0.0692848                  126.692   130.949            0.0645601\n",
              "4        0.04                        0.947065           2.26692   2.29885            0.8875           0.952408   0.9                         0.968048            0.0226692       0.091954                   126.692   129.885            0.0853805\n",
              "5        0.05                        0.93624            2.23499   2.28608            0.875            0.941784   0.895                       0.962795            0.0223499       0.114304                   123.499   128.608            0.105676\n",
              "6        0.1                         0.879107           1.94764   2.11686            0.7625           0.908667   0.82875                     0.935731            0.0973819       0.211686                   94.7637   111.686            0.183543\n",
              "7        0.15                        0.814872           1.81354   2.01575            0.71             0.846586   0.789167                    0.906016            0.0906769       0.302363                   81.3538   101.575            0.250391\n",
              "8        0.2                         0.743607           1.67305   1.93008            0.655            0.781333   0.755625                    0.874845            0.0836526       0.386015                   67.3052   93.0077            0.305695\n",
              "9        0.3                         0.588081           1.47829   1.77948            0.57875          0.666473   0.696667                    0.805388            0.147829        0.533844                   47.8289   77.9481            0.384296\n",
              "10       0.4                         0.443764           1.28033   1.65469            0.50125          0.512584   0.647813                    0.732187            0.128033        0.661877                   28.0332   65.4693            0.430365\n",
              "11       0.5                         0.306942           1.01533   1.52682            0.3975           0.37215    0.59775                     0.66018             0.101533        0.76341                    1.53257   52.682             0.432884\n",
              "12       0.6                         0.202824           0.766284  1.40006            0.3              0.251989   0.548125                    0.592148            0.0766284       0.840038                   -23.3716  40.0064            0.394475\n",
              "13       0.7                         0.130317           0.664112  1.29493            0.26             0.163729   0.506964                    0.530945            0.0664112       0.90645                    -33.5888  29.4928            0.339276\n",
              "14       0.8                         0.0758979          0.472542  1.19213            0.185            0.101729   0.466719                    0.477293            0.0472542       0.953704                   -52.7458  19.213             0.252594\n",
              "15       0.9                         0.0371696          0.322478  1.0955             0.12625          0.0557451  0.428889                    0.430455            0.0322478       0.985951                   -67.7522  9.55016            0.141251\n",
              "16       1                           0.00228818         0.140485  1                  0.055            0.0215713  0.3915                      0.389566            0.0140485       1                          -85.9515  0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                         mean      sd         cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid    cv_6_valid    cv_7_valid    cv_8_valid    cv_9_valid    cv_10_valid\n",
              "-----------------------  --------  ---------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  -------------\n",
              "accuracy                 0.70825   0.0371661  0.7275        0.74375       0.74          0.70375       0.61875       0.6975        0.7375        0.705         0.72375       0.685\n",
              "auc                      0.789564  0.0191126  0.791147      0.797844      0.78883       0.801228      0.766344      0.776762      0.831018      0.78962       0.788937      0.763911\n",
              "err                      0.29175   0.0371661  0.2725        0.25625       0.26          0.29625       0.38125       0.3025        0.2625        0.295         0.27625       0.315\n",
              "err_count                233.4     29.7329    218           205           208           237           305           242           210           236           221           252\n",
              "f0point5                 0.629483  0.0312905  0.641696      0.648259      0.655488      0.60241       0.570673      0.637743      0.675287      0.638809      0.629467      0.595\n",
              "f1                       0.679528  0.0206461  0.669697      0.678179      0.673981      0.669456      0.664466      0.689744      0.728682      0.692708      0.674521      0.653846\n",
              "f2                       0.740414  0.0352846  0.700253      0.710994      0.693548      0.753296      0.795155      0.750977      0.791246      0.756542      0.726523      0.72561\n",
              "lift_top_group           2.34294   0.336948   1.96078       2.38095       2.63158       2.73973       2.42424       1.78042       2.08333       2.12121       2.67559       2.63158\n",
              "logloss                  0.561354  0.0306372  0.563873      0.538003      0.554789      0.533056      0.596285      0.5928        0.506254      0.576416      0.552059      0.600005\n",
              "max_per_class_error      0.354375  0.0925364  0.277778      0.265306      0.292763      0.364173      0.589362      0.37581       0.336207      0.365957      0.301397      0.375\n",
              "mcc                      0.437511  0.0391816  0.443246      0.471221      0.459964      0.441607      0.358681      0.419569      0.498854      0.43528       0.450001      0.396688\n",
              "mean_per_class_accuracy  0.721276  0.0247419  0.726496      0.741853      0.733659      0.728872      0.662895      0.711205      0.751539      0.720052      0.732245      0.703947\n",
              "mean_per_class_error     0.278724  0.0247419  0.273504      0.258147      0.266341      0.271128      0.337105      0.288795      0.248461      0.279948      0.267755      0.296053\n",
              "mse                      0.186156  0.0112054  0.185587      0.175314      0.181778      0.177739      0.200074      0.198868      0.167041      0.191238      0.184604      0.199318\n",
              "pr_auc                   0.706007  0.0296863  0.684173      0.702552      0.70519       0.707094      0.706578      0.687794      0.782989      0.702086      0.709587      0.672029\n",
              "precision                0.600636  0.0398683  0.624294      0.629738      0.643713      0.564706      0.521589      0.607223      0.643836      0.607306      0.602632      0.561321\n",
              "r2                       0.21708   0.0442381  0.214258      0.245779      0.228448      0.233138      0.174422      0.184293      0.314283      0.210882      0.2113        0.153998\n",
              "recall                   0.789357  0.0619735  0.722222      0.734694      0.707237      0.821918      0.915152      0.79822       0.839286      0.806061      0.765886      0.782895\n",
              "rmse                     0.431281  0.0130248  0.430798      0.418705      0.426354      0.421592      0.447296      0.445946      0.408706      0.437308      0.429656      0.446451\n",
              "specificity              0.653196  0.0999643  0.730769      0.749012      0.760081      0.635827      0.410638      0.62419       0.663793      0.634043      0.698603      0.625\n",
              "\n",
              "Scoring History: \n",
              "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
              "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
              "    2023-07-04 10:47:29  26.923 sec  0                  0.5              0.693147            0.5             0.3915             1                0.6085                           0.5                0.693147              0.5               0.3915               1                  0.6085\n",
              "    2023-07-04 10:47:30  27.311 sec  5                  0.387143         0.469449            0.8686          0.815761           2.52235          0.211                            0.420583           0.532308              0.797085          0.718904             2.17114            0.2715\n",
              "    2023-07-04 10:47:30  27.574 sec  10                 0.35995          0.410703            0.896726        0.85392            2.55428          0.182                            0.420456           0.531574              0.795512          0.722968             2.42656            0.2735\n",
              "    2023-07-04 10:47:30  27.873 sec  15                 0.340863         0.374872            0.917331        0.881451           2.55428          0.15525                          0.425685           0.546093              0.788397          0.717152             2.42656            0.3105\n",
              "    2023-07-04 10:47:31  28.159 sec  20                 0.322802         0.343603            0.934676        0.906835           2.55428          0.135625                         0.428904           0.556092              0.784902          0.714457             2.55428            0.2845\n",
              "    2023-07-04 10:47:31  28.494 sec  25                 0.30702          0.31811             0.947853        0.924756           2.55428          0.118625                         0.431961           0.564559              0.781737          0.712846             2.55428            0.2935\n",
              "    2023-07-04 10:47:31  28.892 sec  30                 0.293728         0.29612             0.957486        0.937944           2.55428          0.10725                          0.434051           0.569466              0.780579          0.713234             2.55428            0.321\n",
              "\n",
              "Variable Importances: \n",
              "variable                 relative_importance    scaled_importance    percentage\n",
              "-----------------------  ---------------------  -------------------  --------------------\n",
              "area_se                  1904.8582763671875     1.0                  0.17710163529899145\n",
              "perimeter_worst          670.3985595703125      0.35194143726474486  0.06232940406906372\n",
              "smoothness_worst         490.8770751953125      0.25769742625235037  0.0456386356016325\n",
              "texture_worst            471.06378173828125     0.24729597344987847  0.04379652130082945\n",
              "radius_se                406.78167724609375     0.2135495759935901   0.03781997912587105\n",
              "symmetry_se              383.16064453125        0.2011491612184331   0.03562384539080497\n",
              "area_worst               379.52288818359375     0.19923943575865039  0.03528562988890534\n",
              "texture_se               376.7027893066406      0.19775895875312144  0.035023434990203606\n",
              "radius_worst             356.6051025390625      0.18720820701640586  0.03315487960399761\n",
              "symmetry_mean            341.8689880371094      0.17947213830999437  0.031784809185306774\n",
              "---                      ---                    ---                  ---\n",
              "compactness_mean         257.6020812988281      0.13523425049243495  0.02395020691064367\n",
              "smoothness_se            247.90921020507812     0.13014575062133926  0.023049025262253914\n",
              "fractal_dimension_worst  239.37608337402344     0.12566608568409865  0.02225566927627705\n",
              "perimeter_mean           235.8315887451172      0.1238053201495277   0.021926124657196535\n",
              "fractal_dimension_mean   229.67420959472656     0.12057285964221189  0.021353650615311495\n",
              "compactness_worst        214.68325805664062     0.11270300826057754  0.019959887066064025\n",
              "concave points_mean      202.2340850830078      0.10616752311289768  0.01880244195893765\n",
              "radius_mean              184.11338806152344     0.09665463848189883  0.01711769453437711\n",
              "area_mean                175.94384765625        0.09236584676094528  0.01635814250713946\n",
              "compactness_se           169.79820251464844     0.08913954629657578  0.015786759418933725\n",
              "[30 rows x 4 columns]\n",
              "\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ],
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230704_104702\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-2.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-2 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-2 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-2 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table th,\n",
              "#h2o-table-2 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>number_of_trees</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>30.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.0862762318888866\n",
              "RMSE: 0.2937281598500331\n",
              "LogLoss: 0.2961195502729631\n",
              "Mean Per-Class Error: 0.10560430092631945\n",
              "AUC: 0.9574856020131994\n",
              "AUCPR: 0.9379440756884645\n",
              "Gini: 0.9149712040263989</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-3.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-3 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-3 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-3 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table th,\n",
              "#h2o-table-3 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40771300310180303</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>4317.0</td>\n",
              "<td>551.0</td>\n",
              "<td>0.1132</td>\n",
              "<td> (551.0/4868.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>307.0</td>\n",
              "<td>2825.0</td>\n",
              "<td>0.098</td>\n",
              "<td> (307.0/3132.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>4624.0</td>\n",
              "<td>3376.0</td>\n",
              "<td>0.1072</td>\n",
              "<td> (858.0/8000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-4.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-4 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-4 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-4 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table th,\n",
              "#h2o-table-4 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-4 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-4\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4077130</td>\n",
              "<td>0.8681623</td>\n",
              "<td>213.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.2910308</td>\n",
              "<td>0.9007375</td>\n",
              "<td>254.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6084025</td>\n",
              "<td>0.8859932</td>\n",
              "<td>148.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.4430403</td>\n",
              "<td>0.894625</td>\n",
              "<td>201.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0188069</td>\n",
              "<td>1.0</td>\n",
              "<td>390.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4260813</td>\n",
              "<td>0.7801245</td>\n",
              "<td>207.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.4194713</td>\n",
              "<td>0.8927203</td>\n",
              "<td>209.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3831034</td>\n",
              "<td>0.8948353</td>\n",
              "<td>222.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9931994</td>\n",
              "<td>4868.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9931994</td>\n",
              "<td>3120.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0052119</td>\n",
              "<td>4868.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0188069</td>\n",
              "<td>3132.0</td>\n",
              "<td>390.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9931994</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9931994</td>\n",
              "<td>0.9961686</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0052119</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0188069</td>\n",
              "<td>1.0</td>\n",
              "<td>390.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-5.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-5 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-5 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-5 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table th,\n",
              "#h2o-table-5 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 39.32 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9832729</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9881432</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9881432</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0255428</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0255428</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9740088</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9789083</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9835258</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0510856</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0510856</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9650004</td>\n",
              "<td>2.5223499</td>\n",
              "<td>2.5436356</td>\n",
              "<td>0.9875</td>\n",
              "<td>0.9697575</td>\n",
              "<td>0.9958333</td>\n",
              "<td>0.9789364</td>\n",
              "<td>0.0252235</td>\n",
              "<td>0.0763091</td>\n",
              "<td>152.2349936</td>\n",
              "<td>154.3635590</td>\n",
              "<td>0.0761036</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9566181</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5462963</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9608343</td>\n",
              "<td>0.996875</td>\n",
              "<td>0.9744108</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.1018519</td>\n",
              "<td>155.4278416</td>\n",
              "<td>154.6296296</td>\n",
              "<td>0.1016464</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9482571</td>\n",
              "<td>2.4904215</td>\n",
              "<td>2.5351213</td>\n",
              "<td>0.975</td>\n",
              "<td>0.9524639</td>\n",
              "<td>0.9925</td>\n",
              "<td>0.9700214</td>\n",
              "<td>0.0249042</td>\n",
              "<td>0.1267561</td>\n",
              "<td>149.0421456</td>\n",
              "<td>153.5121328</td>\n",
              "<td>0.1261398</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8918075</td>\n",
              "<td>2.5223499</td>\n",
              "<td>2.5287356</td>\n",
              "<td>0.9875</td>\n",
              "<td>0.9201499</td>\n",
              "<td>0.99</td>\n",
              "<td>0.9450857</td>\n",
              "<td>0.1261175</td>\n",
              "<td>0.2528736</td>\n",
              "<td>152.2349936</td>\n",
              "<td>152.8735632</td>\n",
              "<td>0.2512302</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8343306</td>\n",
              "<td>2.4904215</td>\n",
              "<td>2.5159642</td>\n",
              "<td>0.975</td>\n",
              "<td>0.8645949</td>\n",
              "<td>0.985</td>\n",
              "<td>0.9182554</td>\n",
              "<td>0.1245211</td>\n",
              "<td>0.3773946</td>\n",
              "<td>149.0421456</td>\n",
              "<td>151.5964240</td>\n",
              "<td>0.3736970</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7789860</td>\n",
              "<td>2.4074074</td>\n",
              "<td>2.4888250</td>\n",
              "<td>0.9425</td>\n",
              "<td>0.8073055</td>\n",
              "<td>0.974375</td>\n",
              "<td>0.8905179</td>\n",
              "<td>0.1203704</td>\n",
              "<td>0.4977650</td>\n",
              "<td>140.7407407</td>\n",
              "<td>148.8825032</td>\n",
              "<td>0.4893427</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.6264191</td>\n",
              "<td>2.2062580</td>\n",
              "<td>2.3946360</td>\n",
              "<td>0.86375</td>\n",
              "<td>0.7049811</td>\n",
              "<td>0.9375</td>\n",
              "<td>0.8286723</td>\n",
              "<td>0.2206258</td>\n",
              "<td>0.7183908</td>\n",
              "<td>120.6257982</td>\n",
              "<td>139.4636015</td>\n",
              "<td>0.6875773</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4393838</td>\n",
              "<td>1.5772669</td>\n",
              "<td>2.1902937</td>\n",
              "<td>0.6175</td>\n",
              "<td>0.5325101</td>\n",
              "<td>0.8575</td>\n",
              "<td>0.7546318</td>\n",
              "<td>0.1577267</td>\n",
              "<td>0.8761175</td>\n",
              "<td>57.7266922</td>\n",
              "<td>119.0293742</td>\n",
              "<td>0.7824445</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.2917330</td>\n",
              "<td>0.7407407</td>\n",
              "<td>1.9003831</td>\n",
              "<td>0.29</td>\n",
              "<td>0.3620169</td>\n",
              "<td>0.744</td>\n",
              "<td>0.6761088</td>\n",
              "<td>0.0740741</td>\n",
              "<td>0.9501916</td>\n",
              "<td>-25.9259259</td>\n",
              "<td>90.0383142</td>\n",
              "<td>0.7398382</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.1867268</td>\n",
              "<td>0.3224777</td>\n",
              "<td>1.6373989</td>\n",
              "<td>0.12625</td>\n",
              "<td>0.2355222</td>\n",
              "<td>0.6410417</td>\n",
              "<td>0.6026777</td>\n",
              "<td>0.0322478</td>\n",
              "<td>0.9824393</td>\n",
              "<td>-67.7522350</td>\n",
              "<td>63.7398893</td>\n",
              "<td>0.6284952</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1182911</td>\n",
              "<td>0.0957854</td>\n",
              "<td>1.4171684</td>\n",
              "<td>0.0375</td>\n",
              "<td>0.1498300</td>\n",
              "<td>0.5548214</td>\n",
              "<td>0.5379852</td>\n",
              "<td>0.0095785</td>\n",
              "<td>0.9920179</td>\n",
              "<td>-90.4214559</td>\n",
              "<td>41.7168400</td>\n",
              "<td>0.4798979</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0704665</td>\n",
              "<td>0.0510856</td>\n",
              "<td>1.2464080</td>\n",
              "<td>0.02</td>\n",
              "<td>0.0932805</td>\n",
              "<td>0.4879688</td>\n",
              "<td>0.4823971</td>\n",
              "<td>0.0051086</td>\n",
              "<td>0.9971264</td>\n",
              "<td>-94.8914432</td>\n",
              "<td>24.6408046</td>\n",
              "<td>0.3239547</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0350014</td>\n",
              "<td>0.0159642</td>\n",
              "<td>1.1096921</td>\n",
              "<td>0.00625</td>\n",
              "<td>0.0520768</td>\n",
              "<td>0.4344444</td>\n",
              "<td>0.4345837</td>\n",
              "<td>0.0015964</td>\n",
              "<td>0.9987229</td>\n",
              "<td>-98.4035760</td>\n",
              "<td>10.9692068</td>\n",
              "<td>0.1622397</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0037830</td>\n",
              "<td>0.0127714</td>\n",
              "<td>1.0</td>\n",
              "<td>0.005</td>\n",
              "<td>0.0206824</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3931936</td>\n",
              "<td>0.0012771</td>\n",
              "<td>1.0</td>\n",
              "<td>-98.7228608</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.18840019637319533\n",
              "RMSE: 0.4340509144941355\n",
              "LogLoss: 0.5694655292500879\n",
              "Mean Per-Class Error: 0.29928818116277384\n",
              "AUC: 0.7805786689418005\n",
              "AUCPR: 0.7132335417937792\n",
              "Gini: 0.561157337883601</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-6.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-6 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-6 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-6 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table th,\n",
              "#h2o-table-6 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24615386625130972</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>731.0</td>\n",
              "<td>486.0</td>\n",
              "<td>0.3993</td>\n",
              "<td> (486.0/1217.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>156.0</td>\n",
              "<td>627.0</td>\n",
              "<td>0.1992</td>\n",
              "<td> (156.0/783.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>887.0</td>\n",
              "<td>1113.0</td>\n",
              "<td>0.321</td>\n",
              "<td> (642.0/2000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-7.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-7 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-7 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-7 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table th,\n",
              "#h2o-table-7 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-7 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-7\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.2461539</td>\n",
              "<td>0.6613924</td>\n",
              "<td>271.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.0822697</td>\n",
              "<td>0.7831708</td>\n",
              "<td>350.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.6801786</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.7385</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0091446</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.6287153</td>\n",
              "<td>0.4352113</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3541214</td>\n",
              "<td>0.6998723</td>\n",
              "<td>226.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.5526123</td>\n",
              "<td>0.7065135</td>\n",
              "<td>157.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1217.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9962388</td>\n",
              "<td>782.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0048772</td>\n",
              "<td>1217.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0091446</td>\n",
              "<td>783.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9962388</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9962388</td>\n",
              "<td>0.9987229</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0048772</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0091446</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-8.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-8 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-8 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-8 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table th,\n",
              "#h2o-table-8 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-8 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-8\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.46 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9777978</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9852836</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9852836</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0255428</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0255428</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9645726</td>\n",
              "<td>2.5542784</td>\n",
              "<td>2.5542784</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9728781</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9790808</td>\n",
              "<td>0.0255428</td>\n",
              "<td>0.0510856</td>\n",
              "<td>155.4278416</td>\n",
              "<td>155.4278416</td>\n",
              "<td>0.0510856</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9547869</td>\n",
              "<td>2.2988506</td>\n",
              "<td>2.4691358</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9583625</td>\n",
              "<td>0.9666667</td>\n",
              "<td>0.9721747</td>\n",
              "<td>0.0229885</td>\n",
              "<td>0.0740741</td>\n",
              "<td>129.8850575</td>\n",
              "<td>146.9135802</td>\n",
              "<td>0.0724307</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9437785</td>\n",
              "<td>2.1711367</td>\n",
              "<td>2.3946360</td>\n",
              "<td>0.85</td>\n",
              "<td>0.9505307</td>\n",
              "<td>0.9375</td>\n",
              "<td>0.9667637</td>\n",
              "<td>0.0217114</td>\n",
              "<td>0.0957854</td>\n",
              "<td>117.1136654</td>\n",
              "<td>139.4636015</td>\n",
              "<td>0.0916770</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9288514</td>\n",
              "<td>2.0434227</td>\n",
              "<td>2.3243934</td>\n",
              "<td>0.8</td>\n",
              "<td>0.9355922</td>\n",
              "<td>0.91</td>\n",
              "<td>0.9605294</td>\n",
              "<td>0.0204342</td>\n",
              "<td>0.1162197</td>\n",
              "<td>104.3422733</td>\n",
              "<td>132.4393359</td>\n",
              "<td>0.1088244</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8749162</td>\n",
              "<td>1.9412516</td>\n",
              "<td>2.1328225</td>\n",
              "<td>0.76</td>\n",
              "<td>0.9020278</td>\n",
              "<td>0.835</td>\n",
              "<td>0.9312786</td>\n",
              "<td>0.0970626</td>\n",
              "<td>0.2132822</td>\n",
              "<td>94.1251596</td>\n",
              "<td>113.2822478</td>\n",
              "<td>0.1861664</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8005845</td>\n",
              "<td>1.9667944</td>\n",
              "<td>2.0774798</td>\n",
              "<td>0.77</td>\n",
              "<td>0.8378969</td>\n",
              "<td>0.8133333</td>\n",
              "<td>0.9001514</td>\n",
              "<td>0.0983397</td>\n",
              "<td>0.3116220</td>\n",
              "<td>96.6794381</td>\n",
              "<td>107.7479779</td>\n",
              "<td>0.2656072</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7278127</td>\n",
              "<td>1.7624521</td>\n",
              "<td>1.9987229</td>\n",
              "<td>0.69</td>\n",
              "<td>0.7695403</td>\n",
              "<td>0.7825</td>\n",
              "<td>0.8674986</td>\n",
              "<td>0.0881226</td>\n",
              "<td>0.3997446</td>\n",
              "<td>76.2452107</td>\n",
              "<td>99.8722861</td>\n",
              "<td>0.3282573</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.5725785</td>\n",
              "<td>1.4559387</td>\n",
              "<td>1.8177948</td>\n",
              "<td>0.57</td>\n",
              "<td>0.6497156</td>\n",
              "<td>0.7116667</td>\n",
              "<td>0.7949043</td>\n",
              "<td>0.1455939</td>\n",
              "<td>0.5453384</td>\n",
              "<td>45.5938697</td>\n",
              "<td>81.7794806</td>\n",
              "<td>0.4031856</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4279322</td>\n",
              "<td>0.9706258</td>\n",
              "<td>1.6060026</td>\n",
              "<td>0.38</td>\n",
              "<td>0.5013098</td>\n",
              "<td>0.62875</td>\n",
              "<td>0.7215057</td>\n",
              "<td>0.0970626</td>\n",
              "<td>0.6424010</td>\n",
              "<td>-2.9374202</td>\n",
              "<td>60.6002554</td>\n",
              "<td>0.3983583</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3032194</td>\n",
              "<td>0.9195402</td>\n",
              "<td>1.4687101</td>\n",
              "<td>0.36</td>\n",
              "<td>0.3639555</td>\n",
              "<td>0.575</td>\n",
              "<td>0.6499956</td>\n",
              "<td>0.0919540</td>\n",
              "<td>0.7343550</td>\n",
              "<td>-8.0459770</td>\n",
              "<td>46.8710089</td>\n",
              "<td>0.3851357</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.2088440</td>\n",
              "<td>0.8812261</td>\n",
              "<td>1.3707961</td>\n",
              "<td>0.345</td>\n",
              "<td>0.2523442</td>\n",
              "<td>0.5366667</td>\n",
              "<td>0.5837204</td>\n",
              "<td>0.0881226</td>\n",
              "<td>0.8224777</td>\n",
              "<td>-11.8773946</td>\n",
              "<td>37.0796083</td>\n",
              "<td>0.3656165</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1300754</td>\n",
              "<td>0.7024266</td>\n",
              "<td>1.2753147</td>\n",
              "<td>0.275</td>\n",
              "<td>0.1654164</td>\n",
              "<td>0.4992857</td>\n",
              "<td>0.5239627</td>\n",
              "<td>0.0702427</td>\n",
              "<td>0.8927203</td>\n",
              "<td>-29.7573436</td>\n",
              "<td>27.5314724</td>\n",
              "<td>0.3167137</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0777253</td>\n",
              "<td>0.5236271</td>\n",
              "<td>1.1813538</td>\n",
              "<td>0.205</td>\n",
              "<td>0.1021104</td>\n",
              "<td>0.4625</td>\n",
              "<td>0.4712311</td>\n",
              "<td>0.0523627</td>\n",
              "<td>0.9450830</td>\n",
              "<td>-47.6372925</td>\n",
              "<td>18.1353768</td>\n",
              "<td>0.2384273</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0354974</td>\n",
              "<td>0.3320562</td>\n",
              "<td>1.0869874</td>\n",
              "<td>0.13</td>\n",
              "<td>0.0543392</td>\n",
              "<td>0.4255556</td>\n",
              "<td>0.4249098</td>\n",
              "<td>0.0332056</td>\n",
              "<td>0.9782886</td>\n",
              "<td>-66.7943806</td>\n",
              "<td>8.6987371</td>\n",
              "<td>0.1286584</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0041094</td>\n",
              "<td>0.2171137</td>\n",
              "<td>1.0</td>\n",
              "<td>0.085</td>\n",
              "<td>0.0213709</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3845559</td>\n",
              "<td>0.0217114</td>\n",
              "<td>1.0</td>\n",
              "<td>-78.2886335</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.18615605059426837\n",
              "RMSE: 0.43145805195206216\n",
              "LogLoss: 0.5613540167599764\n",
              "Mean Per-Class Error: 0.28157062936622623\n",
              "AUC: 0.7899126007045779\n",
              "AUCPR: 0.704715036081932\n",
              "Gini: 0.5798252014091558</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-9.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-9 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-9 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-9 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table th,\n",
              "#h2o-table-9 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-9 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-9\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3172192946076393</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>3297.0</td>\n",
              "<td>1571.0</td>\n",
              "<td>0.3227</td>\n",
              "<td> (1571.0/4868.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>753.0</td>\n",
              "<td>2379.0</td>\n",
              "<td>0.2404</td>\n",
              "<td> (753.0/3132.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>4050.0</td>\n",
              "<td>3950.0</td>\n",
              "<td>0.2905</td>\n",
              "<td> (2324.0/8000.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-10.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-10 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-10 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-10 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table th,\n",
              "#h2o-table-10 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-10 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-10\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3172193</td>\n",
              "<td>0.6718441</td>\n",
              "<td>248.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.0903366</td>\n",
              "<td>0.7906840</td>\n",
              "<td>347.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.5626134</td>\n",
              "<td>0.6619421</td>\n",
              "<td>158.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.5485779</td>\n",
              "<td>0.731375</td>\n",
              "<td>163.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9846833</td>\n",
              "<td>0.9411765</td>\n",
              "<td>2.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0067657</td>\n",
              "<td>1.0</td>\n",
              "<td>398.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9997946</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.3951432</td>\n",
              "<td>0.4347693</td>\n",
              "<td>217.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3693605</td>\n",
              "<td>0.7187101</td>\n",
              "<td>227.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3951432</td>\n",
              "<td>0.7207126</td>\n",
              "<td>217.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9915547</td>\n",
              "<td>4867.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9915547</td>\n",
              "<td>3120.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0036070</td>\n",
              "<td>4868.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0067657</td>\n",
              "<td>3132.0</td>\n",
              "<td>398.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9997946</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9915547</td>\n",
              "<td>0.9961686</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0036070</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0067657</td>\n",
              "<td>1.0</td>\n",
              "<td>398.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-11.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-11 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-11 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-11 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table th,\n",
              "#h2o-table-11 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-11 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-11\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 39.15 %, avg score: 38.96 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.01</td>\n",
              "<td>0.9767816</td>\n",
              "<td>2.2988506</td>\n",
              "<td>2.2988506</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9835398</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9835398</td>\n",
              "<td>0.0229885</td>\n",
              "<td>0.0229885</td>\n",
              "<td>129.8850575</td>\n",
              "<td>129.8850575</td>\n",
              "<td>0.0213451</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.02</td>\n",
              "<td>0.9687700</td>\n",
              "<td>2.3627075</td>\n",
              "<td>2.3307791</td>\n",
              "<td>0.925</td>\n",
              "<td>0.9723946</td>\n",
              "<td>0.9125</td>\n",
              "<td>0.9779672</td>\n",
              "<td>0.0236271</td>\n",
              "<td>0.0466156</td>\n",
              "<td>136.2707535</td>\n",
              "<td>133.0779055</td>\n",
              "<td>0.0437397</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.03</td>\n",
              "<td>0.9583953</td>\n",
              "<td>2.2669221</td>\n",
              "<td>2.3094934</td>\n",
              "<td>0.8875</td>\n",
              "<td>0.9638492</td>\n",
              "<td>0.9041667</td>\n",
              "<td>0.9732612</td>\n",
              "<td>0.0226692</td>\n",
              "<td>0.0692848</td>\n",
              "<td>126.6922095</td>\n",
              "<td>130.9493401</td>\n",
              "<td>0.0645601</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.04</td>\n",
              "<td>0.9470655</td>\n",
              "<td>2.2669221</td>\n",
              "<td>2.2988506</td>\n",
              "<td>0.8875</td>\n",
              "<td>0.9524081</td>\n",
              "<td>0.9</td>\n",
              "<td>0.9680479</td>\n",
              "<td>0.0226692</td>\n",
              "<td>0.0919540</td>\n",
              "<td>126.6922095</td>\n",
              "<td>129.8850575</td>\n",
              "<td>0.0853805</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.05</td>\n",
              "<td>0.9362398</td>\n",
              "<td>2.2349936</td>\n",
              "<td>2.2860792</td>\n",
              "<td>0.875</td>\n",
              "<td>0.9417836</td>\n",
              "<td>0.895</td>\n",
              "<td>0.9627951</td>\n",
              "<td>0.0223499</td>\n",
              "<td>0.1143040</td>\n",
              "<td>123.4993614</td>\n",
              "<td>128.6079183</td>\n",
              "<td>0.1056762</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1</td>\n",
              "<td>0.8791068</td>\n",
              "<td>1.9476373</td>\n",
              "<td>2.1168582</td>\n",
              "<td>0.7625</td>\n",
              "<td>0.9086671</td>\n",
              "<td>0.82875</td>\n",
              "<td>0.9357311</td>\n",
              "<td>0.0973819</td>\n",
              "<td>0.2116858</td>\n",
              "<td>94.7637292</td>\n",
              "<td>111.6858238</td>\n",
              "<td>0.1835428</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.15</td>\n",
              "<td>0.8148719</td>\n",
              "<td>1.8135377</td>\n",
              "<td>2.0157514</td>\n",
              "<td>0.71</td>\n",
              "<td>0.8465859</td>\n",
              "<td>0.7891667</td>\n",
              "<td>0.9060160</td>\n",
              "<td>0.0906769</td>\n",
              "<td>0.3023627</td>\n",
              "<td>81.3537676</td>\n",
              "<td>101.5751384</td>\n",
              "<td>0.2503906</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.7436068</td>\n",
              "<td>1.6730524</td>\n",
              "<td>1.9300766</td>\n",
              "<td>0.655</td>\n",
              "<td>0.7813334</td>\n",
              "<td>0.755625</td>\n",
              "<td>0.8748454</td>\n",
              "<td>0.0836526</td>\n",
              "<td>0.3860153</td>\n",
              "<td>67.3052363</td>\n",
              "<td>93.0076628</td>\n",
              "<td>0.3056949</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3</td>\n",
              "<td>0.5880806</td>\n",
              "<td>1.4782886</td>\n",
              "<td>1.7794806</td>\n",
              "<td>0.57875</td>\n",
              "<td>0.6664730</td>\n",
              "<td>0.6966667</td>\n",
              "<td>0.8053879</td>\n",
              "<td>0.1478289</td>\n",
              "<td>0.5338442</td>\n",
              "<td>47.8288633</td>\n",
              "<td>77.9480630</td>\n",
              "<td>0.3842961</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4437644</td>\n",
              "<td>1.2803321</td>\n",
              "<td>1.6546935</td>\n",
              "<td>0.50125</td>\n",
              "<td>0.5125842</td>\n",
              "<td>0.6478125</td>\n",
              "<td>0.7321870</td>\n",
              "<td>0.1280332</td>\n",
              "<td>0.6618774</td>\n",
              "<td>28.0332056</td>\n",
              "<td>65.4693487</td>\n",
              "<td>0.4303655</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3069417</td>\n",
              "<td>1.0153257</td>\n",
              "<td>1.5268199</td>\n",
              "<td>0.3975</td>\n",
              "<td>0.3721498</td>\n",
              "<td>0.59775</td>\n",
              "<td>0.6601796</td>\n",
              "<td>0.1015326</td>\n",
              "<td>0.7634100</td>\n",
              "<td>1.5325670</td>\n",
              "<td>52.6819923</td>\n",
              "<td>0.4328841</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.2028235</td>\n",
              "<td>0.7662835</td>\n",
              "<td>1.4000639</td>\n",
              "<td>0.3</td>\n",
              "<td>0.2519894</td>\n",
              "<td>0.548125</td>\n",
              "<td>0.5921479</td>\n",
              "<td>0.0766284</td>\n",
              "<td>0.8400383</td>\n",
              "<td>-23.3716475</td>\n",
              "<td>40.0063857</td>\n",
              "<td>0.3944755</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7</td>\n",
              "<td>0.1303173</td>\n",
              "<td>0.6641124</td>\n",
              "<td>1.2949279</td>\n",
              "<td>0.26</td>\n",
              "<td>0.1637290</td>\n",
              "<td>0.5069643</td>\n",
              "<td>0.5309452</td>\n",
              "<td>0.0664112</td>\n",
              "<td>0.9064496</td>\n",
              "<td>-33.5887612</td>\n",
              "<td>29.4927933</td>\n",
              "<td>0.3392762</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0758979</td>\n",
              "<td>0.4725415</td>\n",
              "<td>1.1921296</td>\n",
              "<td>0.185</td>\n",
              "<td>0.1017293</td>\n",
              "<td>0.4667188</td>\n",
              "<td>0.4772932</td>\n",
              "<td>0.0472542</td>\n",
              "<td>0.9537037</td>\n",
              "<td>-52.7458493</td>\n",
              "<td>19.2129630</td>\n",
              "<td>0.2525944</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9</td>\n",
              "<td>0.0371696</td>\n",
              "<td>0.3224777</td>\n",
              "<td>1.0955016</td>\n",
              "<td>0.12625</td>\n",
              "<td>0.0557451</td>\n",
              "<td>0.4288889</td>\n",
              "<td>0.4304545</td>\n",
              "<td>0.0322478</td>\n",
              "<td>0.9859515</td>\n",
              "<td>-67.7522350</td>\n",
              "<td>9.5501632</td>\n",
              "<td>0.1412514</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0022882</td>\n",
              "<td>0.1404853</td>\n",
              "<td>1.0</td>\n",
              "<td>0.055</td>\n",
              "<td>0.0215713</td>\n",
              "<td>0.3915</td>\n",
              "<td>0.3895662</td>\n",
              "<td>0.0140485</td>\n",
              "<td>1.0</td>\n",
              "<td>-85.9514687</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-12.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-12 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-12 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-12 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table th,\n",
              "#h2o-table-12 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-12 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-12\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th>\n",
              "<th>cv_6_valid</th>\n",
              "<th>cv_7_valid</th>\n",
              "<th>cv_8_valid</th>\n",
              "<th>cv_9_valid</th>\n",
              "<th>cv_10_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.70825</td>\n",
              "<td>0.0371661</td>\n",
              "<td>0.7275</td>\n",
              "<td>0.74375</td>\n",
              "<td>0.74</td>\n",
              "<td>0.70375</td>\n",
              "<td>0.61875</td>\n",
              "<td>0.6975</td>\n",
              "<td>0.7375</td>\n",
              "<td>0.705</td>\n",
              "<td>0.72375</td>\n",
              "<td>0.685</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.7895642</td>\n",
              "<td>0.0191126</td>\n",
              "<td>0.7911474</td>\n",
              "<td>0.7978442</td>\n",
              "<td>0.7888304</td>\n",
              "<td>0.8012283</td>\n",
              "<td>0.7663443</td>\n",
              "<td>0.7767623</td>\n",
              "<td>0.8310178</td>\n",
              "<td>0.7896196</td>\n",
              "<td>0.7889371</td>\n",
              "<td>0.7639107</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.29175</td>\n",
              "<td>0.0371661</td>\n",
              "<td>0.2725</td>\n",
              "<td>0.25625</td>\n",
              "<td>0.26</td>\n",
              "<td>0.29625</td>\n",
              "<td>0.38125</td>\n",
              "<td>0.3025</td>\n",
              "<td>0.2625</td>\n",
              "<td>0.295</td>\n",
              "<td>0.27625</td>\n",
              "<td>0.315</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>233.4</td>\n",
              "<td>29.732885</td>\n",
              "<td>218.0</td>\n",
              "<td>205.0</td>\n",
              "<td>208.0</td>\n",
              "<td>237.0</td>\n",
              "<td>305.0</td>\n",
              "<td>242.0</td>\n",
              "<td>210.0</td>\n",
              "<td>236.0</td>\n",
              "<td>221.0</td>\n",
              "<td>252.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.6294831</td>\n",
              "<td>0.0312905</td>\n",
              "<td>0.6416957</td>\n",
              "<td>0.6482593</td>\n",
              "<td>0.6554878</td>\n",
              "<td>0.6024097</td>\n",
              "<td>0.5706727</td>\n",
              "<td>0.637743</td>\n",
              "<td>0.6752874</td>\n",
              "<td>0.6388088</td>\n",
              "<td>0.6294667</td>\n",
              "<td>0.595</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.6795281</td>\n",
              "<td>0.0206461</td>\n",
              "<td>0.669697</td>\n",
              "<td>0.6781790</td>\n",
              "<td>0.6739812</td>\n",
              "<td>0.6694561</td>\n",
              "<td>0.6644664</td>\n",
              "<td>0.6897436</td>\n",
              "<td>0.7286822</td>\n",
              "<td>0.6927083</td>\n",
              "<td>0.6745213</td>\n",
              "<td>0.6538461</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.7404144</td>\n",
              "<td>0.0352846</td>\n",
              "<td>0.7002535</td>\n",
              "<td>0.7109941</td>\n",
              "<td>0.6935484</td>\n",
              "<td>0.7532957</td>\n",
              "<td>0.7951554</td>\n",
              "<td>0.7509771</td>\n",
              "<td>0.7912458</td>\n",
              "<td>0.7565416</td>\n",
              "<td>0.7265229</td>\n",
              "<td>0.7256098</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.3429408</td>\n",
              "<td>0.3369483</td>\n",
              "<td>1.9607843</td>\n",
              "<td>2.3809524</td>\n",
              "<td>2.631579</td>\n",
              "<td>2.739726</td>\n",
              "<td>2.4242425</td>\n",
              "<td>1.7804154</td>\n",
              "<td>2.0833333</td>\n",
              "<td>2.121212</td>\n",
              "<td>2.6755853</td>\n",
              "<td>2.631579</td></tr>\n",
              "<tr><td>logloss</td>\n",
              "<td>0.5613540</td>\n",
              "<td>0.0306372</td>\n",
              "<td>0.5638731</td>\n",
              "<td>0.5380028</td>\n",
              "<td>0.5547891</td>\n",
              "<td>0.5330562</td>\n",
              "<td>0.5962847</td>\n",
              "<td>0.5928004</td>\n",
              "<td>0.5062535</td>\n",
              "<td>0.576416</td>\n",
              "<td>0.5520594</td>\n",
              "<td>0.600005</td></tr>\n",
              "<tr><td>max_per_class_error</td>\n",
              "<td>0.3543753</td>\n",
              "<td>0.0925364</td>\n",
              "<td>0.2777778</td>\n",
              "<td>0.2653061</td>\n",
              "<td>0.2927631</td>\n",
              "<td>0.3641732</td>\n",
              "<td>0.5893617</td>\n",
              "<td>0.3758099</td>\n",
              "<td>0.3362069</td>\n",
              "<td>0.3659574</td>\n",
              "<td>0.3013972</td>\n",
              "<td>0.375</td></tr>\n",
              "<tr><td>mcc</td>\n",
              "<td>0.4375112</td>\n",
              "<td>0.0391816</td>\n",
              "<td>0.4432463</td>\n",
              "<td>0.4712214</td>\n",
              "<td>0.4599636</td>\n",
              "<td>0.4416075</td>\n",
              "<td>0.3586807</td>\n",
              "<td>0.4195689</td>\n",
              "<td>0.4988538</td>\n",
              "<td>0.4352803</td>\n",
              "<td>0.4500011</td>\n",
              "<td>0.3966878</td></tr>\n",
              "<tr><td>mean_per_class_accuracy</td>\n",
              "<td>0.7212762</td>\n",
              "<td>0.0247419</td>\n",
              "<td>0.7264957</td>\n",
              "<td>0.7418529</td>\n",
              "<td>0.7336587</td>\n",
              "<td>0.7288723</td>\n",
              "<td>0.6628949</td>\n",
              "<td>0.7112048</td>\n",
              "<td>0.7515394</td>\n",
              "<td>0.7200516</td>\n",
              "<td>0.7322445</td>\n",
              "<td>0.7039474</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.2787238</td>\n",
              "<td>0.0247419</td>\n",
              "<td>0.2735043</td>\n",
              "<td>0.2581471</td>\n",
              "<td>0.2663413</td>\n",
              "<td>0.2711277</td>\n",
              "<td>0.3371051</td>\n",
              "<td>0.2887952</td>\n",
              "<td>0.2484606</td>\n",
              "<td>0.2799484</td>\n",
              "<td>0.2677555</td>\n",
              "<td>0.2960526</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.1861560</td>\n",
              "<td>0.0112054</td>\n",
              "<td>0.1855874</td>\n",
              "<td>0.1753139</td>\n",
              "<td>0.1817776</td>\n",
              "<td>0.1777394</td>\n",
              "<td>0.2000736</td>\n",
              "<td>0.1988681</td>\n",
              "<td>0.1670406</td>\n",
              "<td>0.1912379</td>\n",
              "<td>0.1846039</td>\n",
              "<td>0.1993182</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.706007</td>\n",
              "<td>0.0296863</td>\n",
              "<td>0.6841726</td>\n",
              "<td>0.7025523</td>\n",
              "<td>0.7051897</td>\n",
              "<td>0.7070935</td>\n",
              "<td>0.7065780</td>\n",
              "<td>0.6877939</td>\n",
              "<td>0.7829888</td>\n",
              "<td>0.7020859</td>\n",
              "<td>0.7095867</td>\n",
              "<td>0.6720285</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.6006356</td>\n",
              "<td>0.0398683</td>\n",
              "<td>0.6242938</td>\n",
              "<td>0.6297376</td>\n",
              "<td>0.6437126</td>\n",
              "<td>0.5647059</td>\n",
              "<td>0.5215889</td>\n",
              "<td>0.6072234</td>\n",
              "<td>0.6438356</td>\n",
              "<td>0.6073059</td>\n",
              "<td>0.6026316</td>\n",
              "<td>0.5613208</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.2170801</td>\n",
              "<td>0.0442381</td>\n",
              "<td>0.2142580</td>\n",
              "<td>0.2457793</td>\n",
              "<td>0.2284481</td>\n",
              "<td>0.2331382</td>\n",
              "<td>0.1744221</td>\n",
              "<td>0.1842931</td>\n",
              "<td>0.3142833</td>\n",
              "<td>0.2108816</td>\n",
              "<td>0.2113000</td>\n",
              "<td>0.1539976</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.7893569</td>\n",
              "<td>0.0619735</td>\n",
              "<td>0.7222222</td>\n",
              "<td>0.7346939</td>\n",
              "<td>0.7072368</td>\n",
              "<td>0.8219178</td>\n",
              "<td>0.9151515</td>\n",
              "<td>0.7982196</td>\n",
              "<td>0.8392857</td>\n",
              "<td>0.8060606</td>\n",
              "<td>0.7658863</td>\n",
              "<td>0.7828947</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.4312811</td>\n",
              "<td>0.0130248</td>\n",
              "<td>0.4307985</td>\n",
              "<td>0.4187050</td>\n",
              "<td>0.4263539</td>\n",
              "<td>0.4215915</td>\n",
              "<td>0.4472959</td>\n",
              "<td>0.4459462</td>\n",
              "<td>0.408706</td>\n",
              "<td>0.4373076</td>\n",
              "<td>0.4296555</td>\n",
              "<td>0.4464506</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.6531956</td>\n",
              "<td>0.0999643</td>\n",
              "<td>0.7307692</td>\n",
              "<td>0.7490119</td>\n",
              "<td>0.7600806</td>\n",
              "<td>0.6358268</td>\n",
              "<td>0.4106383</td>\n",
              "<td>0.6241901</td>\n",
              "<td>0.6637931</td>\n",
              "<td>0.6340426</td>\n",
              "<td>0.6986028</td>\n",
              "<td>0.625</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-13.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-13 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-13 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-13 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table th,\n",
              "#h2o-table-13 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-13 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-13\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Scoring History: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>timestamp</th>\n",
              "<th>duration</th>\n",
              "<th>number_of_trees</th>\n",
              "<th>training_rmse</th>\n",
              "<th>training_logloss</th>\n",
              "<th>training_auc</th>\n",
              "<th>training_pr_auc</th>\n",
              "<th>training_lift</th>\n",
              "<th>training_classification_error</th>\n",
              "<th>validation_rmse</th>\n",
              "<th>validation_logloss</th>\n",
              "<th>validation_auc</th>\n",
              "<th>validation_pr_auc</th>\n",
              "<th>validation_lift</th>\n",
              "<th>validation_classification_error</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>2023-07-04 10:47:29</td>\n",
              "<td>26.923 sec</td>\n",
              "<td>0.0</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3915000</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6085</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3915000</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6085</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:47:30</td>\n",
              "<td>27.311 sec</td>\n",
              "<td>5.0</td>\n",
              "<td>0.3871426</td>\n",
              "<td>0.4694493</td>\n",
              "<td>0.8686003</td>\n",
              "<td>0.8157606</td>\n",
              "<td>2.5223499</td>\n",
              "<td>0.211</td>\n",
              "<td>0.4205830</td>\n",
              "<td>0.5323082</td>\n",
              "<td>0.7970855</td>\n",
              "<td>0.7189038</td>\n",
              "<td>2.1711367</td>\n",
              "<td>0.2715</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:47:30</td>\n",
              "<td>27.574 sec</td>\n",
              "<td>10.0</td>\n",
              "<td>0.3599499</td>\n",
              "<td>0.4107030</td>\n",
              "<td>0.8967256</td>\n",
              "<td>0.8539203</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.182</td>\n",
              "<td>0.4204562</td>\n",
              "<td>0.5315744</td>\n",
              "<td>0.7955124</td>\n",
              "<td>0.7229683</td>\n",
              "<td>2.4265645</td>\n",
              "<td>0.2735</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:47:30</td>\n",
              "<td>27.873 sec</td>\n",
              "<td>15.0</td>\n",
              "<td>0.3408629</td>\n",
              "<td>0.3748717</td>\n",
              "<td>0.9173308</td>\n",
              "<td>0.8814513</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.15525</td>\n",
              "<td>0.4256850</td>\n",
              "<td>0.5460932</td>\n",
              "<td>0.7883968</td>\n",
              "<td>0.7171523</td>\n",
              "<td>2.4265645</td>\n",
              "<td>0.3105</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:47:31</td>\n",
              "<td>28.159 sec</td>\n",
              "<td>20.0</td>\n",
              "<td>0.3228023</td>\n",
              "<td>0.3436028</td>\n",
              "<td>0.9346762</td>\n",
              "<td>0.9068350</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.135625</td>\n",
              "<td>0.4289042</td>\n",
              "<td>0.5560917</td>\n",
              "<td>0.7849017</td>\n",
              "<td>0.7144570</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.2845</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:47:31</td>\n",
              "<td>28.494 sec</td>\n",
              "<td>25.0</td>\n",
              "<td>0.3070198</td>\n",
              "<td>0.3181104</td>\n",
              "<td>0.9478527</td>\n",
              "<td>0.9247560</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.118625</td>\n",
              "<td>0.4319605</td>\n",
              "<td>0.5645589</td>\n",
              "<td>0.7817372</td>\n",
              "<td>0.7128463</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.2935</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:47:31</td>\n",
              "<td>28.892 sec</td>\n",
              "<td>30.0</td>\n",
              "<td>0.2937282</td>\n",
              "<td>0.2961196</td>\n",
              "<td>0.9574856</td>\n",
              "<td>0.9379441</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.10725</td>\n",
              "<td>0.4340509</td>\n",
              "<td>0.5694655</td>\n",
              "<td>0.7805787</td>\n",
              "<td>0.7132335</td>\n",
              "<td>2.5542784</td>\n",
              "<td>0.321</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-14.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-14 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-14 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-14 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table th,\n",
              "#h2o-table-14 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-14 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-14\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Variable Importances: </caption>\n",
              "    <thead><tr><th>variable</th>\n",
              "<th>relative_importance</th>\n",
              "<th>scaled_importance</th>\n",
              "<th>percentage</th></tr></thead>\n",
              "    <tbody><tr><td>area_se</td>\n",
              "<td>1904.8582764</td>\n",
              "<td>1.0</td>\n",
              "<td>0.1771016</td></tr>\n",
              "<tr><td>perimeter_worst</td>\n",
              "<td>670.3985596</td>\n",
              "<td>0.3519414</td>\n",
              "<td>0.0623294</td></tr>\n",
              "<tr><td>smoothness_worst</td>\n",
              "<td>490.8770752</td>\n",
              "<td>0.2576974</td>\n",
              "<td>0.0456386</td></tr>\n",
              "<tr><td>texture_worst</td>\n",
              "<td>471.0637817</td>\n",
              "<td>0.2472960</td>\n",
              "<td>0.0437965</td></tr>\n",
              "<tr><td>radius_se</td>\n",
              "<td>406.7816772</td>\n",
              "<td>0.2135496</td>\n",
              "<td>0.0378200</td></tr>\n",
              "<tr><td>symmetry_se</td>\n",
              "<td>383.1606445</td>\n",
              "<td>0.2011492</td>\n",
              "<td>0.0356238</td></tr>\n",
              "<tr><td>area_worst</td>\n",
              "<td>379.5228882</td>\n",
              "<td>0.1992394</td>\n",
              "<td>0.0352856</td></tr>\n",
              "<tr><td>texture_se</td>\n",
              "<td>376.7027893</td>\n",
              "<td>0.1977590</td>\n",
              "<td>0.0350234</td></tr>\n",
              "<tr><td>radius_worst</td>\n",
              "<td>356.6051025</td>\n",
              "<td>0.1872082</td>\n",
              "<td>0.0331549</td></tr>\n",
              "<tr><td>symmetry_mean</td>\n",
              "<td>341.8689880</td>\n",
              "<td>0.1794721</td>\n",
              "<td>0.0317848</td></tr>\n",
              "<tr><td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td></tr>\n",
              "<tr><td>compactness_mean</td>\n",
              "<td>257.6020813</td>\n",
              "<td>0.1352343</td>\n",
              "<td>0.0239502</td></tr>\n",
              "<tr><td>smoothness_se</td>\n",
              "<td>247.9092102</td>\n",
              "<td>0.1301458</td>\n",
              "<td>0.0230490</td></tr>\n",
              "<tr><td>fractal_dimension_worst</td>\n",
              "<td>239.3760834</td>\n",
              "<td>0.1256661</td>\n",
              "<td>0.0222557</td></tr>\n",
              "<tr><td>perimeter_mean</td>\n",
              "<td>235.8315887</td>\n",
              "<td>0.1238053</td>\n",
              "<td>0.0219261</td></tr>\n",
              "<tr><td>fractal_dimension_mean</td>\n",
              "<td>229.6742096</td>\n",
              "<td>0.1205729</td>\n",
              "<td>0.0213537</td></tr>\n",
              "<tr><td>compactness_worst</td>\n",
              "<td>214.6832581</td>\n",
              "<td>0.1127030</td>\n",
              "<td>0.0199599</td></tr>\n",
              "<tr><td>concave points_mean</td>\n",
              "<td>202.2340851</td>\n",
              "<td>0.1061675</td>\n",
              "<td>0.0188024</td></tr>\n",
              "<tr><td>radius_mean</td>\n",
              "<td>184.1133881</td>\n",
              "<td>0.0966546</td>\n",
              "<td>0.0171177</td></tr>\n",
              "<tr><td>area_mean</td>\n",
              "<td>175.9438477</td>\n",
              "<td>0.0923658</td>\n",
              "<td>0.0163581</td></tr>\n",
              "<tr><td>compactness_se</td>\n",
              "<td>169.7982025</td>\n",
              "<td>0.0891395</td>\n",
              "<td>0.0157868</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "<pre style='font-size: smaller; margin-bottom: 1em;'>[30 rows x 4 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.model_performance(train).accuracy()"
      ],
      "metadata": {
        "id": "G_PyYFNMs97_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3262008a-38fe-4aec-b2c4-901594b760e2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.44304026911656064, 0.894625]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "HATr  = best_model.model_performance(train)\n",
        "HATe  = best_model.model_performance(valid)"
      ],
      "metadata": {
        "id": "jKquKjVVJBL9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_h2o = pd.DataFrame(h2o.as_list(best_model.predict(valid)))['predict']\n",
        "y_test_h2o = np.array(valid1['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-caBWGPlp8P",
        "outputId": "5497e9d9-dbe2-4314-a804-8004c5b82adb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SFOLD DATA AUTOML\n",
        "#strain, svalid = shdf.split_frame(ratios=[.8], seed=123)\n",
        "shdf  = newdata.copy()\n",
        "#shdf['y_test'] = shdf['y_test'].replace(0,\"B\")\n",
        "#shdf['y_test'] = shdf['y_test'].replace(1,\"M\")\n",
        "shy = \"y_test\"\n",
        "shx = list(shdf.columns)\n",
        "shx.remove(shy)\n",
        "\n",
        "shdf.iloc[:,0:6] = StandardScaler().fit_transform(shdf.iloc[:,0:6])\n",
        "#shdf.iloc[:,-1] = LabelEncoder().fit_transform(shdf.iloc[:,-1])\n",
        "strain1, svalid1 = train_test_split(shdf, test_size=0.2,random_state=123)\n",
        "strain = h2o.H2OFrame(strain1)\n",
        "svalid = h2o.H2OFrame(svalid1)\n",
        "strain[\"y_test\"] = strain[\"y_test\"].asfactor()\n",
        "svalid[\"y_test\"] = svalid[\"y_test\"].asfactor()\n",
        "\n",
        "st = time.time()\n",
        "saml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\", nfolds=10, sort_metric='accuracy')\n",
        "saml.train(x = shx, y = shy, training_frame = strain, validation_frame = svalid)\n",
        "sautoend = time.time() - st\n",
        "sbest_model = saml.get_best_model()\n",
        "sHATr  = sbest_model.model_performance(strain)\n",
        "sHATe  = sbest_model.model_performance(svalid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMDqylVK7svS",
        "outputId": "8b0aed56-980d-4cc4-dc9e-04ab5e49d3ef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "AutoML progress: |\n",
            "10:51:58.461: Project: AutoML_2_20230704_105158\n",
            "10:51:58.461: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "10:51:58.462: Setting stopping tolerance adaptively based on the training frame: 0.011180339887498949\n",
            "10:51:58.462: Build control seed: 123\n",
            "10:51:58.462: training frame: Frame key: AutoML_2_20230704_105158_training_py_11_sid_9493    cols: 7    rows: 8000  chunks: 1    size: 98174  checksum: 691450733884769904\n",
            "10:51:58.463: validation frame: Frame key: py_12_sid_9493    cols: 7    rows: 2000  chunks: 1    size: 25424  checksum: 6791181805634214736\n",
            "10:51:58.463: leaderboard frame: NULL\n",
            "10:51:58.463: blending frame: NULL\n",
            "10:51:58.463: response column: y_test\n",
            "10:51:58.463: fold column: null\n",
            "10:51:58.463: weights column: null\n",
            "10:51:58.463: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "10:51:58.465: AutoML job created: 2023.07.04 10:51:58.461\n",
            "10:51:58.465: AutoML build started: 2023.07.04 10:51:58.465\n",
            "10:51:58.466: AutoML: starting XGBoost_1_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆ\n",
            "10:52:04.754: New leader: XGBoost_1_AutoML_2_20230704_105158, accuracy: 0.758\n",
            "10:52:04.754: AutoML: starting GLM_1_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "10:52:10.120: New leader: GLM_1_AutoML_2_20230704_105158, accuracy: 0.75625\n",
            "10:52:10.121: AutoML: starting GBM_1_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆ\n",
            "10:52:14.808: AutoML: starting XGBoost_2_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆ\n",
            "10:52:19.126: AutoML: starting DRF_1_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "10:52:25.863: AutoML: starting GBM_2_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆ\n",
            "10:52:30.124: AutoML: starting GBM_3_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆ\n",
            "10:52:35.289: AutoML: starting GBM_4_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆ\n",
            "10:52:39.909: AutoML: starting XGBoost_3_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "10:52:43.209: AutoML: starting XRT_1_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆ\n",
            "10:52:49.708: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "10:52:49.709: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆ\n",
            "10:53:00.691: AutoML: starting StackedEnsemble_AllModels_1_AutoML_2_20230704_105158 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "\n",
            "10:53:14.456: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "10:53:14.456: AutoML build stopped: 2023.07.04 10:53:14.456\n",
            "10:53:14.456: AutoML build done: built 10 models\n",
            "10:53:14.456: AutoML duration:  1 min 15.991 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_sh2o = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))['predict']\n",
        "y_test_sh2o = np.array(svalid1['y_test']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivMaaDNNmYY2",
        "outputId": "439b5f13-a88e-4aea-fbf5-92783db81465"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glm prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "nJrcI3Gn8we2",
        "outputId": "57262a60-6910-4437-f0c7-817e6f3a2d85"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 7 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]\n",
        "\n",
        "label = [\"ArtificialNeuralNetwork\", 'DeepNeuralNetwork',\n",
        "         'KNearestNeighborsClassifier', 'LogisticRegression',\n",
        "         'RandomForestClassifier', 'SupportVectorClassifier',\n",
        "         'XGBoost', type(best_model).__name__, type(sbest_model).__name__ ]\n",
        "\n",
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"DNN\":[DTr,DTe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"H_OD\":[HATr.accuracy()[0][1],HATe.accuracy()[0][1]],\n",
        "    \"H_SOD\":[sHATr.accuracy()[0][1],sHATe.accuracy()[0][1]]\n",
        "    })\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc['Model'] = label\n",
        "\n",
        "acc = acc[['Model', 'train', 'test']]\n",
        "acc['avg'] = round((acc['train'] + acc['test'])/2, 6)\n",
        "acc[acc[\"avg\"] == acc[\"avg\"].max()]\n",
        "acc['BestModel'] = 0\n",
        "for i in range(len(acc)):\n",
        "  if acc['avg'][i] >= 90 and acc['avg'][i] < acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"good\"\n",
        "  elif acc['avg'][i] == acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"best\"\n",
        "  else:\n",
        "    acc.iloc[i,-1] = \"not good\"\n",
        "\n",
        "acc[\"Precision\"] = np.zeros(len(acc))\n",
        "acc[\"Recall\"]    = np.zeros(len(acc))\n",
        "acc[\"F1_Score\"]  = np.zeros(len(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ba6kRO-eI60S"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ANNn = []\n",
        "y_pred_DNNn = []\n",
        "for i in range(len(y_pred_ANN)):\n",
        "  y_pred_ANNn.append(y_pred_ANN[i][0])\n",
        "  y_pred_DNNn.append(y_pred_DNN[i][0])"
      ],
      "metadata": {
        "id": "S7pf8G6ao4oF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [np.array(y_pred_ANNn), np.array(y_pred_DNNn), y_pred_knn,\n",
        "        y_pred_lr, y_pred_rf,\n",
        "        y_pred_svm, y_pred_xgb, y_pred_h2o, y_pred_sh2o]\n",
        "\n",
        "tes  = [y_test_indi_ML, np.array(Dy_test), y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_indi_ML, y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_h2o.copy(), y_test_sh2o.copy()]"
      ],
      "metadata": {
        "id": "D2n2VnW1jKp4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pred)):\n",
        "  p,r,f,_ = precision_recall_fscore_support(tes[i], pred[i],\n",
        "                                            average='macro')\n",
        "  acc.iloc[i,5]= p\n",
        "  acc.iloc[i,6]= r\n",
        "  acc.iloc[i,7]= f\n",
        "  p = 0\n",
        "  r = 0\n",
        "  f = 0\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "2DlSQ29moN9T",
        "outputId": "aef1eb32-d26e-4193-ce80-a455049385d0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               Model     train    test       avg BestModel  \\\n",
              "ANN          ArtificialNeuralNetwork  0.764875  0.7505  0.757688  not good   \n",
              "DNN                DeepNeuralNetwork  0.762875  0.7560  0.759438  not good   \n",
              "KNN      KNearestNeighborsClassifier  0.772250  0.7340  0.753125  not good   \n",
              "LR                LogisticRegression  0.761500  0.7565  0.759000  not good   \n",
              "RF            RandomForestClassifier  0.751750  0.7565  0.754125  not good   \n",
              "SVM          SupportVectorClassifier  0.755750  0.7555  0.755625  not good   \n",
              "XGB                          XGBoost  0.775375  0.7555  0.765438  not good   \n",
              "H_OD             H2OXGBoostEstimator  0.894625  0.7385  0.816563      best   \n",
              "H_SOD  H2OGeneralizedLinearEstimator  0.757625  0.7605  0.759062  not good   \n",
              "\n",
              "       Precision    Recall  F1_Score  \n",
              "ANN     0.743555  0.720978  0.727157  \n",
              "DNN     0.747419  0.730507  0.735880  \n",
              "KNN     0.723671  0.705142  0.710304  \n",
              "LR      0.752195  0.725225  0.732152  \n",
              "RF      0.748737  0.729779  0.735544  \n",
              "SVM     0.752952  0.722126  0.729413  \n",
              "XGB     0.747051  0.729641  0.735101  \n",
              "H_OD    0.691755  0.698796  0.676574  \n",
              "H_SOD   0.729287  0.739078  0.730558  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7dffb91f-2dc6-40ad-8c13-9bc8d761b0c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>avg</th>\n",
              "      <th>BestModel</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>ArtificialNeuralNetwork</td>\n",
              "      <td>0.764875</td>\n",
              "      <td>0.7505</td>\n",
              "      <td>0.757688</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.743555</td>\n",
              "      <td>0.720978</td>\n",
              "      <td>0.727157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>DeepNeuralNetwork</td>\n",
              "      <td>0.762875</td>\n",
              "      <td>0.7560</td>\n",
              "      <td>0.759438</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.730507</td>\n",
              "      <td>0.735880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>KNearestNeighborsClassifier</td>\n",
              "      <td>0.772250</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.753125</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.723671</td>\n",
              "      <td>0.705142</td>\n",
              "      <td>0.710304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.7565</td>\n",
              "      <td>0.759000</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.752195</td>\n",
              "      <td>0.725225</td>\n",
              "      <td>0.732152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.751750</td>\n",
              "      <td>0.7565</td>\n",
              "      <td>0.754125</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.748737</td>\n",
              "      <td>0.729779</td>\n",
              "      <td>0.735544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>SupportVectorClassifier</td>\n",
              "      <td>0.755750</td>\n",
              "      <td>0.7555</td>\n",
              "      <td>0.755625</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.752952</td>\n",
              "      <td>0.722126</td>\n",
              "      <td>0.729413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.775375</td>\n",
              "      <td>0.7555</td>\n",
              "      <td>0.765438</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.747051</td>\n",
              "      <td>0.729641</td>\n",
              "      <td>0.735101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_OD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.7385</td>\n",
              "      <td>0.816563</td>\n",
              "      <td>best</td>\n",
              "      <td>0.691755</td>\n",
              "      <td>0.698796</td>\n",
              "      <td>0.676574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_SOD</th>\n",
              "      <td>H2OGeneralizedLinearEstimator</td>\n",
              "      <td>0.757625</td>\n",
              "      <td>0.7605</td>\n",
              "      <td>0.759062</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.729287</td>\n",
              "      <td>0.739078</td>\n",
              "      <td>0.730558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7dffb91f-2dc6-40ad-8c13-9bc8d761b0c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7dffb91f-2dc6-40ad-8c13-9bc8d761b0c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7dffb91f-2dc6-40ad-8c13-9bc8d761b0c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ru4oH6-YzGPw",
        "outputId": "d7b8845f-4a8f-4b51-8a99-4e5a24cf1348"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 9 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve"
      ],
      "metadata": {
        "id": "Qma6hWrmDBvf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]"
      ],
      "metadata": {
        "id": "03rwpiYpoAGs"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()"
      ],
      "metadata": {
        "id": "x5wrzn-qPaXw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "C80ESe__xB5x"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
        "\n",
        "y_pred = ANNmodel.predict(X_test).ravel()\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[0]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = model.predict(DX_test).ravel()\n",
        "y_test = Dy_test.copy()\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[1]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = knn.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[2]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = lr.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[3]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = rf.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[4]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = svm.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[5]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = xgb.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[6]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(best_model.predict(valid)))\n",
        "y_test = h2o.as_list(valid['diagnosis'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(best_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))\n",
        "y_test = h2o.as_list(svalid['y_test'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(sbest_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('AUC ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "4Bi4CMUJm6yH",
        "outputId": "b5fff250-8c6d-4a22-b040-815105a1051f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 1ms/step\n",
            "63/63 [==============================] - 0s 1ms/step\n",
            "xgboost prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "glm prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tm = [aend,dend,kend,lend,rend,send,xend,autoend,sautoend]"
      ],
      "metadata": {
        "id": "6vUubKKMxLIP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['time'] = 0"
      ],
      "metadata": {
        "id": "_1wKoAe9wIPn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['time'] = tm"
      ],
      "metadata": {
        "id": "P-K86caJxhvX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Kl5SnYY3R1S4",
        "outputId": "bc7f35b8-a550-48ae-c0e5-1bdf8b71264b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               Model     train    test       avg BestModel  \\\n",
              "ANN          ArtificialNeuralNetwork  0.764875  0.7505  0.757688  not good   \n",
              "DNN                DeepNeuralNetwork  0.762875  0.7560  0.759438  not good   \n",
              "KNN      KNearestNeighborsClassifier  0.772250  0.7340  0.753125  not good   \n",
              "LR                LogisticRegression  0.761500  0.7565  0.759000  not good   \n",
              "RF            RandomForestClassifier  0.751750  0.7565  0.754125  not good   \n",
              "SVM          SupportVectorClassifier  0.755750  0.7555  0.755625  not good   \n",
              "XGB                          XGBoost  0.775375  0.7555  0.765438  not good   \n",
              "H_OD             H2OXGBoostEstimator  0.894625  0.7385  0.816563      best   \n",
              "H_SOD  H2OGeneralizedLinearEstimator  0.757625  0.7605  0.759062  not good   \n",
              "\n",
              "       Precision    Recall  F1_Score        time  \n",
              "ANN     0.743555  0.720978  0.727157    5.387012  \n",
              "DNN     0.747419  0.730507  0.735880  203.481221  \n",
              "KNN     0.723671  0.705142  0.710304    0.002926  \n",
              "LR      0.752195  0.725225  0.732152    0.187616  \n",
              "RF      0.748737  0.729779  0.735544    1.855545  \n",
              "SVM     0.752952  0.722126  0.729413   19.026791  \n",
              "XGB     0.747051  0.729641  0.735101   12.631472  \n",
              "H_OD    0.691755  0.698796  0.676574  291.899301  \n",
              "H_SOD   0.729287  0.739078  0.730558   77.810254  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99923863-ad65-4f58-8aed-9773662fac7f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>avg</th>\n",
              "      <th>BestModel</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_Score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>ArtificialNeuralNetwork</td>\n",
              "      <td>0.764875</td>\n",
              "      <td>0.7505</td>\n",
              "      <td>0.757688</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.743555</td>\n",
              "      <td>0.720978</td>\n",
              "      <td>0.727157</td>\n",
              "      <td>5.387012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>DeepNeuralNetwork</td>\n",
              "      <td>0.762875</td>\n",
              "      <td>0.7560</td>\n",
              "      <td>0.759438</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.730507</td>\n",
              "      <td>0.735880</td>\n",
              "      <td>203.481221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>KNearestNeighborsClassifier</td>\n",
              "      <td>0.772250</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.753125</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.723671</td>\n",
              "      <td>0.705142</td>\n",
              "      <td>0.710304</td>\n",
              "      <td>0.002926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.7565</td>\n",
              "      <td>0.759000</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.752195</td>\n",
              "      <td>0.725225</td>\n",
              "      <td>0.732152</td>\n",
              "      <td>0.187616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.751750</td>\n",
              "      <td>0.7565</td>\n",
              "      <td>0.754125</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.748737</td>\n",
              "      <td>0.729779</td>\n",
              "      <td>0.735544</td>\n",
              "      <td>1.855545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>SupportVectorClassifier</td>\n",
              "      <td>0.755750</td>\n",
              "      <td>0.7555</td>\n",
              "      <td>0.755625</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.752952</td>\n",
              "      <td>0.722126</td>\n",
              "      <td>0.729413</td>\n",
              "      <td>19.026791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.775375</td>\n",
              "      <td>0.7555</td>\n",
              "      <td>0.765438</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.747051</td>\n",
              "      <td>0.729641</td>\n",
              "      <td>0.735101</td>\n",
              "      <td>12.631472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_OD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.7385</td>\n",
              "      <td>0.816563</td>\n",
              "      <td>best</td>\n",
              "      <td>0.691755</td>\n",
              "      <td>0.698796</td>\n",
              "      <td>0.676574</td>\n",
              "      <td>291.899301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_SOD</th>\n",
              "      <td>H2OGeneralizedLinearEstimator</td>\n",
              "      <td>0.757625</td>\n",
              "      <td>0.7605</td>\n",
              "      <td>0.759062</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.729287</td>\n",
              "      <td>0.739078</td>\n",
              "      <td>0.730558</td>\n",
              "      <td>77.810254</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99923863-ad65-4f58-8aed-9773662fac7f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-99923863-ad65-4f58-8aed-9773662fac7f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-99923863-ad65-4f58-8aed-9773662fac7f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}