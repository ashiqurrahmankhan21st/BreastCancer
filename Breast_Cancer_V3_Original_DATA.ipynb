{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashiqurrahmankhan21st/BreastCancer/blob/main/Breast_Cancer_V3_Original_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "id": "EUMYU_d6_J-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "#import keras_metrics\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#original data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/data.csv\")\n",
        "del df['id']\n",
        "df"
      ],
      "metadata": {
        "id": "Ntdqq4T_aGXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "a92c5b21-15fa-4185-e26f-c460377102aa"
      },
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0           M        17.99         10.38          122.80     1001.0   \n",
              "1           M        20.57         17.77          132.90     1326.0   \n",
              "2           M        19.69         21.25          130.00     1203.0   \n",
              "3           M        11.42         20.38           77.58      386.1   \n",
              "4           M        20.29         14.34          135.10     1297.0   \n",
              "..        ...          ...           ...             ...        ...   \n",
              "564         M        21.56         22.39          142.00     1479.0   \n",
              "565         M        20.13         28.25          131.20     1261.0   \n",
              "566         M        16.60         28.08          108.30      858.1   \n",
              "567         M        20.60         29.33          140.10     1265.0   \n",
              "568         B         7.76         24.54           47.92      181.0   \n",
              "\n",
              "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0            0.11840           0.27760         0.30010              0.14710   \n",
              "1            0.08474           0.07864         0.08690              0.07017   \n",
              "2            0.10960           0.15990         0.19740              0.12790   \n",
              "3            0.14250           0.28390         0.24140              0.10520   \n",
              "4            0.10030           0.13280         0.19800              0.10430   \n",
              "..               ...               ...             ...                  ...   \n",
              "564          0.11100           0.11590         0.24390              0.13890   \n",
              "565          0.09780           0.10340         0.14400              0.09791   \n",
              "566          0.08455           0.10230         0.09251              0.05302   \n",
              "567          0.11780           0.27700         0.35140              0.15200   \n",
              "568          0.05263           0.04362         0.00000              0.00000   \n",
              "\n",
              "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
              "0           0.2419  ...        25.380          17.33           184.60   \n",
              "1           0.1812  ...        24.990          23.41           158.80   \n",
              "2           0.2069  ...        23.570          25.53           152.50   \n",
              "3           0.2597  ...        14.910          26.50            98.87   \n",
              "4           0.1809  ...        22.540          16.67           152.20   \n",
              "..             ...  ...           ...            ...              ...   \n",
              "564         0.1726  ...        25.450          26.40           166.10   \n",
              "565         0.1752  ...        23.690          38.25           155.00   \n",
              "566         0.1590  ...        18.980          34.12           126.70   \n",
              "567         0.2397  ...        25.740          39.42           184.60   \n",
              "568         0.1587  ...         9.456          30.37            59.16   \n",
              "\n",
              "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "0        2019.0           0.16220            0.66560           0.7119   \n",
              "1        1956.0           0.12380            0.18660           0.2416   \n",
              "2        1709.0           0.14440            0.42450           0.4504   \n",
              "3         567.7           0.20980            0.86630           0.6869   \n",
              "4        1575.0           0.13740            0.20500           0.4000   \n",
              "..          ...               ...                ...              ...   \n",
              "564      2027.0           0.14100            0.21130           0.4107   \n",
              "565      1731.0           0.11660            0.19220           0.3215   \n",
              "566      1124.0           0.11390            0.30940           0.3403   \n",
              "567      1821.0           0.16500            0.86810           0.9387   \n",
              "568       268.6           0.08996            0.06444           0.0000   \n",
              "\n",
              "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
              "0                  0.2654          0.4601                  0.11890  \n",
              "1                  0.1860          0.2750                  0.08902  \n",
              "2                  0.2430          0.3613                  0.08758  \n",
              "3                  0.2575          0.6638                  0.17300  \n",
              "4                  0.1625          0.2364                  0.07678  \n",
              "..                    ...             ...                      ...  \n",
              "564                0.2216          0.2060                  0.07115  \n",
              "565                0.1628          0.2572                  0.06637  \n",
              "566                0.1418          0.2218                  0.07820  \n",
              "567                0.2650          0.4087                  0.12400  \n",
              "568                0.0000          0.2871                  0.07039  \n",
              "\n",
              "[569 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af2f08be-0cf8-4e72-98da-d6530e46597c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>...</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>...</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>...</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>...</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>...</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>...</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>...</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>...</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>...</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>...</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af2f08be-0cf8-4e72-98da-d6530e46597c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af2f08be-0cf8-4e72-98da-d6530e46597c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af2f08be-0cf8-4e72-98da-d6530e46597c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 375
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()"
      ],
      "metadata": {
        "id": "4cimS259ti6F"
      },
      "execution_count": 376,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diagnosis'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv9sG-P8M1Fe",
        "outputId": "d2cd1351-af7b-46bd-f664-52ac903183d8"
      },
      "execution_count": 377,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    357\n",
              "M    212\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 377
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()\n",
        "print(\"Original data : \",df.shape)\n",
        "print(\"tarin         : \",X_train.shape)\n",
        "print(\"test          : \",X_test.shape[0])\n",
        "#print(\"validation    : \",X_val.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpyVwv13CTut",
        "outputId": "17eba2d9-75b8-472f-a8d9-247310ef95b2"
      },
      "execution_count": 378,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data :  (569, 31)\n",
            "tarin         :  (455, 30)\n",
            "test          :  114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "svm = SVC(C=0.1, gamma='auto', kernel = 'rbf',probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "STr = svm.score(X_train, y_train)\n",
        "STe = svm.score(X_test, y_test)\n",
        "y_pred_svm = svm.predict(X_test)"
      ],
      "metadata": {
        "id": "wnRYFOkyEEZ9"
      },
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANN\n",
        "tf.random.set_seed(123)\n",
        "ANNmodel = Sequential()\n",
        "ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "ANNmodel.add(Dense(10, activation='relu'))\n",
        "ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "ATr = ANNmodel.evaluate(X_train,y_train,verbose=0)[1]\n",
        "ATe = ANNmodel.evaluate(X_test,y_test,verbose=0)[1]\n",
        "y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZX7DbN-OIyu",
        "outputId": "1e1befa8-234b-46f5-dfee-8a04ea5afe1b"
      },
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 81ms/step - loss: 0.6155 - accuracy: 0.7335 - val_loss: 0.6424 - val_accuracy: 0.6087\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5529 - accuracy: 0.8166 - val_loss: 0.6019 - val_accuracy: 0.6957\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5017 - accuracy: 0.8533 - val_loss: 0.5668 - val_accuracy: 0.7391\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4572 - accuracy: 0.8973 - val_loss: 0.5349 - val_accuracy: 0.7609\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.4181 - accuracy: 0.9120 - val_loss: 0.5061 - val_accuracy: 0.7609\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3850 - accuracy: 0.9169 - val_loss: 0.4793 - val_accuracy: 0.7609\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3548 - accuracy: 0.9218 - val_loss: 0.4555 - val_accuracy: 0.7826\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3291 - accuracy: 0.9242 - val_loss: 0.4342 - val_accuracy: 0.7826\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3060 - accuracy: 0.9267 - val_loss: 0.4149 - val_accuracy: 0.7826\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2851 - accuracy: 0.9315 - val_loss: 0.3976 - val_accuracy: 0.8043\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2666 - accuracy: 0.9315 - val_loss: 0.3819 - val_accuracy: 0.8043\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2494 - accuracy: 0.9413 - val_loss: 0.3677 - val_accuracy: 0.8478\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2346 - accuracy: 0.9413 - val_loss: 0.3545 - val_accuracy: 0.8696\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2208 - accuracy: 0.9462 - val_loss: 0.3424 - val_accuracy: 0.8696\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2083 - accuracy: 0.9462 - val_loss: 0.3312 - val_accuracy: 0.8913\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1973 - accuracy: 0.9462 - val_loss: 0.3210 - val_accuracy: 0.9130\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1868 - accuracy: 0.9462 - val_loss: 0.3122 - val_accuracy: 0.9130\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1773 - accuracy: 0.9462 - val_loss: 0.3042 - val_accuracy: 0.9130\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1686 - accuracy: 0.9535 - val_loss: 0.2969 - val_accuracy: 0.9130\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1607 - accuracy: 0.9560 - val_loss: 0.2901 - val_accuracy: 0.9130\n",
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "params = {\n",
        "            'objective':'binary:logistic',\n",
        "            'max_depth': 7,\n",
        "            'alpha': 10,\n",
        "            'learning_rate': 1,\n",
        "            'n_estimators':100\n",
        "        }\n",
        "xgb = XGBClassifier(objective='binary:logistic',max_depth= 7,alpha= 10,learning_rate= 1,n_estimators=100)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "XTr = accuracy_score(y_train, xgb.predict(X_train))\n",
        "XTe = accuracy_score(y_test, xgb.predict(X_test))\n",
        "XTr,XTe"
      ],
      "metadata": {
        "id": "PCP82YZ9RjgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c073e92d-84e0-4d63-edd6-494663a37232"
      },
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.989010989010989, 0.9824561403508771)"
            ]
          },
          "metadata": {},
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "# Define the range of n_neighbors values to test\n",
        "n_neighbors_values = [1,3, 5, 7, 9, 11]\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_n_neighbors = None\n",
        "\n",
        "for n_neighbors in n_neighbors_values:\n",
        "    print(\"Number of Neighbors:\", n_neighbors)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, knn.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "    print('KNN model train accuracy score: {0:0.4f}'.format(train_accuracy))\n",
        "    print('KNN model test accuracy score: {0:0.4f}'.format(test_accuracy))\n",
        "    print()\n",
        "\n",
        "    # Check if the current test accuracy is better than the previous best\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_n_neighbors = n_neighbors\n",
        "print(\"best neighbours: \", best_n_neighbors)\n",
        "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "knn.fit(X_train, y_train)\n",
        "KTr = accuracy_score(y_train, knn.predict(X_train))\n",
        "KTe = accuracy_score(y_test, knn.predict(X_test))\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "KTr,KTe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y_6b33A1imy",
        "outputId": "442d6b53-37a1-4c36-bac8-3eb8cc40c7e8"
      },
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Neighbors: 1\n",
            "KNN model train accuracy score: 1.0000\n",
            "KNN model test accuracy score: 0.9737\n",
            "\n",
            "Number of Neighbors: 3\n",
            "KNN model train accuracy score: 0.9802\n",
            "KNN model test accuracy score: 0.9912\n",
            "\n",
            "Number of Neighbors: 5\n",
            "KNN model train accuracy score: 0.9692\n",
            "KNN model test accuracy score: 0.9825\n",
            "\n",
            "Number of Neighbors: 7\n",
            "KNN model train accuracy score: 0.9692\n",
            "KNN model test accuracy score: 0.9737\n",
            "\n",
            "Number of Neighbors: 9\n",
            "KNN model train accuracy score: 0.9692\n",
            "KNN model test accuracy score: 0.9825\n",
            "\n",
            "Number of Neighbors: 11\n",
            "KNN model train accuracy score: 0.9714\n",
            "KNN model test accuracy score: 0.9825\n",
            "\n",
            "best neighbours:  3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9802197802197802, 0.9912280701754386)"
            ]
          },
          "metadata": {},
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RF\n",
        "rf = RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123)\n",
        "rf.fit(X_train, y_train)\n",
        "RTr = accuracy_score(y_train, rf.predict(X_train))\n",
        "RTe = accuracy_score(y_test, rf.predict(X_test))\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "RTr,RTe"
      ],
      "metadata": {
        "id": "k_kyk_E92bSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d36dbe-a164-4003-89d0-2261a1e503f3"
      },
      "execution_count": 383,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9758241758241758, 0.9912280701754386)"
            ]
          },
          "metadata": {},
          "execution_count": 383
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LR\n",
        "lr = LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123)\n",
        "lr.fit(X_train, y_train)\n",
        "LTr = accuracy_score(y_train, lr.predict(X_train))\n",
        "LTe = accuracy_score(y_test, lr.predict(X_test))\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "LTr,LTe"
      ],
      "metadata": {
        "id": "OWkZmwL23Fm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ce6198-32ae-46d1-b5d3-626a19469899"
      },
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9802197802197802, 0.9736842105263158)"
            ]
          },
          "metadata": {},
          "execution_count": 384
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E9_l6r5f0w5n"
      },
      "execution_count": 384,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CVal(ML):\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/data.csv\")\n",
        "  del df['id']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "    ML.fit(X_train, y_train)\n",
        "    y_pred_ML = ML.predict(X_test)\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ML)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]+1\n",
        "\n",
        "  y_pred_final = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_final.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "XqaJZ1Mb0xAe"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CValANN():\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/data.csv\")\n",
        "  del df['id']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "    tf.random.set_seed(123)\n",
        "    ANNmodel = Sequential()\n",
        "    ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    ANNmodel.add(Dense(10, activation='relu'))\n",
        "    ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "    ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "    ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "    y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ANN)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]\n",
        "\n",
        "  y_pred_fina = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_fina.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  y_pred_final = []\n",
        "\n",
        "  for i in range(len(y_pred_fina)):\n",
        "    y_pred_final.append(y_pred_fina[i][0])\n",
        "\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "jRa_a7EY0y6B"
      },
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = pd.DataFrame({\n",
        "    \"SVM\": CVal(SVC(C=0.1, gamma='auto', kernel = 'rbf'))\n",
        "})\n",
        "newdata[\"KNN\"] = CVal(KNeighborsClassifier(n_neighbors=3))\n",
        "newdata[\"RF\"]  = CVal(RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123))\n",
        "newdata['LR']  = CVal(LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123))\n",
        "newdata[\"ANN\"] = CValANN()\n",
        "newdata[\"XGB\"] = CVal(XGBClassifier(objective='binary:logistic',max_depth= 7,alpha= 10,learning_rate= 1,n_estimators=100))\n",
        "newdata['y_test'] = encoder.fit_transform(df['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CGgdO1V0zA9",
        "outputId": "6e498d17-bcc1-440e-92e4-8a7ced5a9dd1"
      },
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 78ms/step - loss: 0.5581 - accuracy: 0.8337 - val_loss: 0.4827 - val_accuracy: 0.9130\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.5025 - accuracy: 0.9120 - val_loss: 0.4274 - val_accuracy: 0.9348\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4509 - accuracy: 0.9340 - val_loss: 0.3771 - val_accuracy: 0.9565\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4022 - accuracy: 0.9389 - val_loss: 0.3315 - val_accuracy: 0.9783\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3578 - accuracy: 0.9438 - val_loss: 0.2905 - val_accuracy: 0.9783\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3178 - accuracy: 0.9511 - val_loss: 0.2550 - val_accuracy: 0.9783\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2852 - accuracy: 0.9535 - val_loss: 0.2254 - val_accuracy: 0.9783\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2563 - accuracy: 0.9584 - val_loss: 0.2013 - val_accuracy: 0.9783\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2327 - accuracy: 0.9609 - val_loss: 0.1814 - val_accuracy: 0.9783\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2123 - accuracy: 0.9633 - val_loss: 0.1649 - val_accuracy: 0.9783\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1950 - accuracy: 0.9633 - val_loss: 0.1512 - val_accuracy: 0.9783\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1804 - accuracy: 0.9658 - val_loss: 0.1400 - val_accuracy: 0.9783\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1679 - accuracy: 0.9682 - val_loss: 0.1308 - val_accuracy: 0.9783\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1575 - accuracy: 0.9682 - val_loss: 0.1231 - val_accuracy: 0.9783\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1483 - accuracy: 0.9682 - val_loss: 0.1164 - val_accuracy: 0.9783\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1404 - accuracy: 0.9682 - val_loss: 0.1108 - val_accuracy: 0.9783\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1335 - accuracy: 0.9707 - val_loss: 0.1064 - val_accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1273 - accuracy: 0.9707 - val_loss: 0.1022 - val_accuracy: 0.9783\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1220 - accuracy: 0.9731 - val_loss: 0.0989 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1172 - accuracy: 0.9731 - val_loss: 0.0965 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 70ms/step - loss: 0.7042 - accuracy: 0.5428 - val_loss: 0.6791 - val_accuracy: 0.5870\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6297 - accuracy: 0.5966 - val_loss: 0.6071 - val_accuracy: 0.6522\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.5710 - accuracy: 0.6455 - val_loss: 0.5484 - val_accuracy: 0.6522\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5230 - accuracy: 0.7066 - val_loss: 0.4985 - val_accuracy: 0.7174\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4839 - accuracy: 0.7482 - val_loss: 0.4533 - val_accuracy: 0.7609\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4497 - accuracy: 0.7726 - val_loss: 0.4120 - val_accuracy: 0.8696\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4192 - accuracy: 0.8142 - val_loss: 0.3730 - val_accuracy: 0.9130\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3887 - accuracy: 0.8582 - val_loss: 0.3338 - val_accuracy: 0.9348\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3600 - accuracy: 0.8826 - val_loss: 0.2996 - val_accuracy: 0.9565\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3316 - accuracy: 0.9022 - val_loss: 0.2702 - val_accuracy: 0.9565\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3045 - accuracy: 0.9095 - val_loss: 0.2453 - val_accuracy: 0.9565\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2769 - accuracy: 0.9193 - val_loss: 0.2248 - val_accuracy: 0.9565\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2505 - accuracy: 0.9364 - val_loss: 0.2072 - val_accuracy: 0.9565\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2254 - accuracy: 0.9462 - val_loss: 0.1923 - val_accuracy: 0.9565\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2048 - accuracy: 0.9487 - val_loss: 0.1816 - val_accuracy: 0.9565\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1878 - accuracy: 0.9511 - val_loss: 0.1729 - val_accuracy: 0.9565\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1737 - accuracy: 0.9584 - val_loss: 0.1652 - val_accuracy: 0.9565\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1615 - accuracy: 0.9633 - val_loss: 0.1580 - val_accuracy: 0.9565\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1512 - accuracy: 0.9707 - val_loss: 0.1516 - val_accuracy: 0.9565\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1424 - accuracy: 0.9707 - val_loss: 0.1461 - val_accuracy: 0.9565\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 71ms/step - loss: 0.6590 - accuracy: 0.6235 - val_loss: 0.4794 - val_accuracy: 0.8478\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.5859 - accuracy: 0.6748 - val_loss: 0.4310 - val_accuracy: 0.8913\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5311 - accuracy: 0.7555 - val_loss: 0.3891 - val_accuracy: 0.8696\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4833 - accuracy: 0.7922 - val_loss: 0.3507 - val_accuracy: 0.9130\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4431 - accuracy: 0.8533 - val_loss: 0.3161 - val_accuracy: 0.9348\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4076 - accuracy: 0.8826 - val_loss: 0.2871 - val_accuracy: 0.9348\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3764 - accuracy: 0.9144 - val_loss: 0.2613 - val_accuracy: 0.9348\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3490 - accuracy: 0.9169 - val_loss: 0.2394 - val_accuracy: 0.9565\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3247 - accuracy: 0.9267 - val_loss: 0.2200 - val_accuracy: 0.9565\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3018 - accuracy: 0.9291 - val_loss: 0.2034 - val_accuracy: 0.9565\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2815 - accuracy: 0.9315 - val_loss: 0.1884 - val_accuracy: 0.9783\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2636 - accuracy: 0.9340 - val_loss: 0.1751 - val_accuracy: 0.9783\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2465 - accuracy: 0.9340 - val_loss: 0.1638 - val_accuracy: 0.9783\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2314 - accuracy: 0.9413 - val_loss: 0.1538 - val_accuracy: 0.9783\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2171 - accuracy: 0.9413 - val_loss: 0.1454 - val_accuracy: 0.9783\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2040 - accuracy: 0.9438 - val_loss: 0.1379 - val_accuracy: 0.9783\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1920 - accuracy: 0.9462 - val_loss: 0.1309 - val_accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1810 - accuracy: 0.9487 - val_loss: 0.1249 - val_accuracy: 0.9783\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1710 - accuracy: 0.9487 - val_loss: 0.1202 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1620 - accuracy: 0.9511 - val_loss: 0.1163 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 71ms/step - loss: 1.0661 - accuracy: 0.1711 - val_loss: 1.1264 - val_accuracy: 0.0435\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.9506 - accuracy: 0.2152 - val_loss: 0.9898 - val_accuracy: 0.0652\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.8500 - accuracy: 0.2665 - val_loss: 0.8777 - val_accuracy: 0.2174\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.7660 - accuracy: 0.3863 - val_loss: 0.7855 - val_accuracy: 0.3913\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.6918 - accuracy: 0.5575 - val_loss: 0.7060 - val_accuracy: 0.5870\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.6295 - accuracy: 0.6993 - val_loss: 0.6382 - val_accuracy: 0.6739\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.5759 - accuracy: 0.7751 - val_loss: 0.5806 - val_accuracy: 0.7391\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.5296 - accuracy: 0.8362 - val_loss: 0.5316 - val_accuracy: 0.9130\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4881 - accuracy: 0.8753 - val_loss: 0.4882 - val_accuracy: 0.9130\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.4513 - accuracy: 0.8924 - val_loss: 0.4492 - val_accuracy: 0.9130\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4190 - accuracy: 0.9022 - val_loss: 0.4140 - val_accuracy: 0.9348\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3888 - accuracy: 0.9095 - val_loss: 0.3831 - val_accuracy: 0.9565\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3615 - accuracy: 0.9144 - val_loss: 0.3546 - val_accuracy: 0.9565\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.3364 - accuracy: 0.9218 - val_loss: 0.3286 - val_accuracy: 0.9565\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3141 - accuracy: 0.9218 - val_loss: 0.3056 - val_accuracy: 0.9565\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2935 - accuracy: 0.9267 - val_loss: 0.2850 - val_accuracy: 0.9783\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2758 - accuracy: 0.9315 - val_loss: 0.2660 - val_accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2594 - accuracy: 0.9340 - val_loss: 0.2488 - val_accuracy: 0.9783\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2444 - accuracy: 0.9413 - val_loss: 0.2336 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2305 - accuracy: 0.9462 - val_loss: 0.2203 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 104ms/step - loss: 0.5734 - accuracy: 0.7976 - val_loss: 0.5469 - val_accuracy: 0.9130\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5237 - accuracy: 0.8902 - val_loss: 0.5064 - val_accuracy: 0.9565\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.4806 - accuracy: 0.9244 - val_loss: 0.4705 - val_accuracy: 0.9565\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4410 - accuracy: 0.9390 - val_loss: 0.4364 - val_accuracy: 0.9565\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.4041 - accuracy: 0.9415 - val_loss: 0.4031 - val_accuracy: 0.9783\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.3697 - accuracy: 0.9463 - val_loss: 0.3702 - val_accuracy: 0.9783\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3382 - accuracy: 0.9488 - val_loss: 0.3407 - val_accuracy: 0.9565\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.3097 - accuracy: 0.9512 - val_loss: 0.3140 - val_accuracy: 0.9565\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.2845 - accuracy: 0.9512 - val_loss: 0.2902 - val_accuracy: 0.9565\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2621 - accuracy: 0.9537 - val_loss: 0.2682 - val_accuracy: 0.9565\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2420 - accuracy: 0.9537 - val_loss: 0.2484 - val_accuracy: 0.9565\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.2240 - accuracy: 0.9585 - val_loss: 0.2308 - val_accuracy: 0.9565\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.2084 - accuracy: 0.9585 - val_loss: 0.2150 - val_accuracy: 0.9565\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1944 - accuracy: 0.9610 - val_loss: 0.2012 - val_accuracy: 0.9565\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.1819 - accuracy: 0.9634 - val_loss: 0.1892 - val_accuracy: 0.9565\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1708 - accuracy: 0.9634 - val_loss: 0.1784 - val_accuracy: 0.9565\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.1610 - accuracy: 0.9659 - val_loss: 0.1691 - val_accuracy: 0.9565\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.1522 - accuracy: 0.9659 - val_loss: 0.1609 - val_accuracy: 0.9565\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1445 - accuracy: 0.9683 - val_loss: 0.1535 - val_accuracy: 0.9565\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1376 - accuracy: 0.9683 - val_loss: 0.1472 - val_accuracy: 0.9565\n",
            "4/4 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newdata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "05ryfvyLzFVK",
        "outputId": "81167b5f-e989-4389-cd72-76f4c47b1593"
      },
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SVM  KNN  RF  LR  ANN  XGB  y_test\n",
              "0    1    1   1   1    1    0       1\n",
              "1    1    1   1   1    1    1       1\n",
              "2    1    1   1   1    1    1       1\n",
              "3    1    1   1   1    1    1       1\n",
              "4    1    1   1   1    1    0       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81b49e46-68ba-4d13-93a9-da9be7bee8c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SVM</th>\n",
              "      <th>KNN</th>\n",
              "      <th>RF</th>\n",
              "      <th>LR</th>\n",
              "      <th>ANN</th>\n",
              "      <th>XGB</th>\n",
              "      <th>y_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81b49e46-68ba-4d13-93a9-da9be7bee8c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81b49e46-68ba-4d13-93a9-da9be7bee8c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81b49e46-68ba-4d13-93a9-da9be7bee8c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 388
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DNN model\n",
        "DNNX = newdata.drop(columns=['y_test']).copy()\n",
        "DNNY = newdata.y_test.copy()\n",
        "DX_train, DX_test, Dy_train, Dy_test = train_test_split(\n",
        "    DNNX, DNNY, test_size=0.2, random_state=123)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(30, activation='relu', input_shape=DNNX.shape[1:]),\n",
        "    tf.keras.layers.Dense(25, activation='relu'),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dense(15, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(DX_train, Dy_train, epochs=500, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4kf97a3yX15",
        "outputId": "bea17aae-bb4a-417c-d334-098f3015ccd3"
      },
      "execution_count": 389,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 2s 54ms/step - loss: 0.6592 - accuracy: 0.6868 - val_loss: 0.6388 - val_accuracy: 0.8791\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.6359 - accuracy: 0.8819 - val_loss: 0.6118 - val_accuracy: 0.8791\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.6128 - accuracy: 0.8819 - val_loss: 0.5846 - val_accuracy: 0.8791\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5895 - accuracy: 0.8819 - val_loss: 0.5594 - val_accuracy: 0.8791\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.5688 - accuracy: 0.8819 - val_loss: 0.5375 - val_accuracy: 0.8791\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.5517 - accuracy: 0.8819 - val_loss: 0.5225 - val_accuracy: 0.8791\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.5397 - accuracy: 0.8819 - val_loss: 0.5145 - val_accuracy: 0.8791\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5336 - accuracy: 0.8819 - val_loss: 0.5101 - val_accuracy: 0.8791\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5268 - accuracy: 0.8929 - val_loss: 0.5053 - val_accuracy: 0.8791\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.5198 - accuracy: 0.8956 - val_loss: 0.4995 - val_accuracy: 0.8791\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.5117 - accuracy: 0.9038 - val_loss: 0.4935 - val_accuracy: 0.8791\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.5045 - accuracy: 0.9011 - val_loss: 0.4880 - val_accuracy: 0.8791\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4989 - accuracy: 0.9038 - val_loss: 0.4826 - val_accuracy: 0.8791\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4933 - accuracy: 0.9203 - val_loss: 0.4793 - val_accuracy: 0.8901\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4886 - accuracy: 0.9368 - val_loss: 0.4761 - val_accuracy: 0.8901\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4854 - accuracy: 0.9396 - val_loss: 0.4723 - val_accuracy: 0.8901\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4820 - accuracy: 0.9396 - val_loss: 0.4665 - val_accuracy: 0.8901\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.4796 - accuracy: 0.9396 - val_loss: 0.4643 - val_accuracy: 0.8901\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4774 - accuracy: 0.9396 - val_loss: 0.4629 - val_accuracy: 0.8901\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4751 - accuracy: 0.9451 - val_loss: 0.4574 - val_accuracy: 0.8901\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4725 - accuracy: 0.9423 - val_loss: 0.4558 - val_accuracy: 0.8901\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4703 - accuracy: 0.9451 - val_loss: 0.4539 - val_accuracy: 0.8901\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4679 - accuracy: 0.9451 - val_loss: 0.4527 - val_accuracy: 0.8901\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4664 - accuracy: 0.9451 - val_loss: 0.4548 - val_accuracy: 0.8901\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4641 - accuracy: 0.9451 - val_loss: 0.4515 - val_accuracy: 0.8901\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4617 - accuracy: 0.9451 - val_loss: 0.4455 - val_accuracy: 0.8901\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4598 - accuracy: 0.9478 - val_loss: 0.4437 - val_accuracy: 0.8901\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4580 - accuracy: 0.9478 - val_loss: 0.4402 - val_accuracy: 0.8901\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4558 - accuracy: 0.9478 - val_loss: 0.4404 - val_accuracy: 0.8901\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4537 - accuracy: 0.9478 - val_loss: 0.4393 - val_accuracy: 0.8901\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4519 - accuracy: 0.9478 - val_loss: 0.4380 - val_accuracy: 0.8901\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4501 - accuracy: 0.9478 - val_loss: 0.4363 - val_accuracy: 0.8901\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4478 - accuracy: 0.9478 - val_loss: 0.4329 - val_accuracy: 0.8901\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4460 - accuracy: 0.9478 - val_loss: 0.4288 - val_accuracy: 0.9121\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4444 - accuracy: 0.9478 - val_loss: 0.4302 - val_accuracy: 0.9121\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4422 - accuracy: 0.9478 - val_loss: 0.4298 - val_accuracy: 0.9011\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.4402 - accuracy: 0.9478 - val_loss: 0.4260 - val_accuracy: 0.9121\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4381 - accuracy: 0.9478 - val_loss: 0.4244 - val_accuracy: 0.9121\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4363 - accuracy: 0.9478 - val_loss: 0.4240 - val_accuracy: 0.9121\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4347 - accuracy: 0.9478 - val_loss: 0.4246 - val_accuracy: 0.9121\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4330 - accuracy: 0.9478 - val_loss: 0.4205 - val_accuracy: 0.9121\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4307 - accuracy: 0.9478 - val_loss: 0.4213 - val_accuracy: 0.9121\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4291 - accuracy: 0.9478 - val_loss: 0.4218 - val_accuracy: 0.9121\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4269 - accuracy: 0.9478 - val_loss: 0.4179 - val_accuracy: 0.9121\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4252 - accuracy: 0.9478 - val_loss: 0.4166 - val_accuracy: 0.9121\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4233 - accuracy: 0.9478 - val_loss: 0.4154 - val_accuracy: 0.9121\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4214 - accuracy: 0.9505 - val_loss: 0.4154 - val_accuracy: 0.9121\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4194 - accuracy: 0.9505 - val_loss: 0.4161 - val_accuracy: 0.9121\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4178 - accuracy: 0.9505 - val_loss: 0.4144 - val_accuracy: 0.9121\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4160 - accuracy: 0.9505 - val_loss: 0.4131 - val_accuracy: 0.9121\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4144 - accuracy: 0.9533 - val_loss: 0.4123 - val_accuracy: 0.9121\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4126 - accuracy: 0.9533 - val_loss: 0.4113 - val_accuracy: 0.9231\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4103 - accuracy: 0.9533 - val_loss: 0.4103 - val_accuracy: 0.9121\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4075 - accuracy: 0.9533 - val_loss: 0.4091 - val_accuracy: 0.9231\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4051 - accuracy: 0.9560 - val_loss: 0.4066 - val_accuracy: 0.9231\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4015 - accuracy: 0.9533 - val_loss: 0.4064 - val_accuracy: 0.9231\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3983 - accuracy: 0.9560 - val_loss: 0.4027 - val_accuracy: 0.9231\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3955 - accuracy: 0.9560 - val_loss: 0.4010 - val_accuracy: 0.9231\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.3920 - accuracy: 0.9560 - val_loss: 0.3986 - val_accuracy: 0.9231\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.3892 - accuracy: 0.9560 - val_loss: 0.3936 - val_accuracy: 0.9231\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.3846 - accuracy: 0.9560 - val_loss: 0.3937 - val_accuracy: 0.9231\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.3813 - accuracy: 0.9560 - val_loss: 0.3948 - val_accuracy: 0.9231\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3774 - accuracy: 0.9560 - val_loss: 0.3890 - val_accuracy: 0.9231\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3740 - accuracy: 0.9560 - val_loss: 0.3858 - val_accuracy: 0.9231\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3703 - accuracy: 0.9560 - val_loss: 0.3851 - val_accuracy: 0.9231\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3665 - accuracy: 0.9560 - val_loss: 0.3836 - val_accuracy: 0.9231\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3624 - accuracy: 0.9560 - val_loss: 0.3789 - val_accuracy: 0.9231\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3600 - accuracy: 0.9560 - val_loss: 0.3757 - val_accuracy: 0.9231\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3554 - accuracy: 0.9560 - val_loss: 0.3755 - val_accuracy: 0.9231\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.3517 - accuracy: 0.9560 - val_loss: 0.3757 - val_accuracy: 0.9231\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3487 - accuracy: 0.9560 - val_loss: 0.3721 - val_accuracy: 0.9231\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3458 - accuracy: 0.9505 - val_loss: 0.3665 - val_accuracy: 0.9231\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.3410 - accuracy: 0.9505 - val_loss: 0.3660 - val_accuracy: 0.9231\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.3374 - accuracy: 0.9560 - val_loss: 0.3659 - val_accuracy: 0.9231\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3335 - accuracy: 0.9560 - val_loss: 0.3622 - val_accuracy: 0.9231\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3299 - accuracy: 0.9560 - val_loss: 0.3595 - val_accuracy: 0.9231\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3261 - accuracy: 0.9505 - val_loss: 0.3558 - val_accuracy: 0.9231\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.3228 - accuracy: 0.9560 - val_loss: 0.3549 - val_accuracy: 0.9231\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3192 - accuracy: 0.9560 - val_loss: 0.3529 - val_accuracy: 0.9231\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.3148 - accuracy: 0.9560 - val_loss: 0.3484 - val_accuracy: 0.9231\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3115 - accuracy: 0.9560 - val_loss: 0.3459 - val_accuracy: 0.9231\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3080 - accuracy: 0.9560 - val_loss: 0.3447 - val_accuracy: 0.9231\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3035 - accuracy: 0.9560 - val_loss: 0.3420 - val_accuracy: 0.9231\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.3003 - accuracy: 0.9505 - val_loss: 0.3368 - val_accuracy: 0.9231\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.2962 - accuracy: 0.9560 - val_loss: 0.3328 - val_accuracy: 0.9231\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2867 - accuracy: 0.9533 - val_loss: 0.3301 - val_accuracy: 0.9231\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2764 - accuracy: 0.9560 - val_loss: 0.3232 - val_accuracy: 0.9231\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.2665 - accuracy: 0.9533 - val_loss: 0.3092 - val_accuracy: 0.9231\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2529 - accuracy: 0.9560 - val_loss: 0.3067 - val_accuracy: 0.9231\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.2398 - accuracy: 0.9560 - val_loss: 0.2943 - val_accuracy: 0.9231\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.2262 - accuracy: 0.9560 - val_loss: 0.2860 - val_accuracy: 0.9231\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.2126 - accuracy: 0.9560 - val_loss: 0.2782 - val_accuracy: 0.9231\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1984 - accuracy: 0.9560 - val_loss: 0.2738 - val_accuracy: 0.9231\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1876 - accuracy: 0.9560 - val_loss: 0.2715 - val_accuracy: 0.9231\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1769 - accuracy: 0.9560 - val_loss: 0.2785 - val_accuracy: 0.9231\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1723 - accuracy: 0.9560 - val_loss: 0.2886 - val_accuracy: 0.9231\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1675 - accuracy: 0.9533 - val_loss: 0.2827 - val_accuracy: 0.9231\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1648 - accuracy: 0.9560 - val_loss: 0.2886 - val_accuracy: 0.9231\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1632 - accuracy: 0.9560 - val_loss: 0.2930 - val_accuracy: 0.9231\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1665 - accuracy: 0.9533 - val_loss: 0.2950 - val_accuracy: 0.9231\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1625 - accuracy: 0.9505 - val_loss: 0.2946 - val_accuracy: 0.9231\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1620 - accuracy: 0.9560 - val_loss: 0.2933 - val_accuracy: 0.9231\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1599 - accuracy: 0.9560 - val_loss: 0.2955 - val_accuracy: 0.9231\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1624 - accuracy: 0.9560 - val_loss: 0.2982 - val_accuracy: 0.9231\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1606 - accuracy: 0.9560 - val_loss: 0.2962 - val_accuracy: 0.9231\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1592 - accuracy: 0.9560 - val_loss: 0.2985 - val_accuracy: 0.9231\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1586 - accuracy: 0.9560 - val_loss: 0.3001 - val_accuracy: 0.9231\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1581 - accuracy: 0.9560 - val_loss: 0.2994 - val_accuracy: 0.9231\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1589 - accuracy: 0.9560 - val_loss: 0.2990 - val_accuracy: 0.9231\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1573 - accuracy: 0.9533 - val_loss: 0.2986 - val_accuracy: 0.9231\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1569 - accuracy: 0.9533 - val_loss: 0.2991 - val_accuracy: 0.9231\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1571 - accuracy: 0.9533 - val_loss: 0.2995 - val_accuracy: 0.9231\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1569 - accuracy: 0.9560 - val_loss: 0.2998 - val_accuracy: 0.9231\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1575 - accuracy: 0.9560 - val_loss: 0.3001 - val_accuracy: 0.9231\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1559 - accuracy: 0.9560 - val_loss: 0.2985 - val_accuracy: 0.9231\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1574 - accuracy: 0.9560 - val_loss: 0.2991 - val_accuracy: 0.9231\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1555 - accuracy: 0.9533 - val_loss: 0.2987 - val_accuracy: 0.9231\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1558 - accuracy: 0.9560 - val_loss: 0.2979 - val_accuracy: 0.9231\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1561 - accuracy: 0.9533 - val_loss: 0.2976 - val_accuracy: 0.9231\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1549 - accuracy: 0.9533 - val_loss: 0.2991 - val_accuracy: 0.9231\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1558 - accuracy: 0.9560 - val_loss: 0.3014 - val_accuracy: 0.9231\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1547 - accuracy: 0.9560 - val_loss: 0.2999 - val_accuracy: 0.9231\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1549 - accuracy: 0.9533 - val_loss: 0.2984 - val_accuracy: 0.9231\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1540 - accuracy: 0.9560 - val_loss: 0.2995 - val_accuracy: 0.9231\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1544 - accuracy: 0.9560 - val_loss: 0.3007 - val_accuracy: 0.9231\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1538 - accuracy: 0.9560 - val_loss: 0.3014 - val_accuracy: 0.9231\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1541 - accuracy: 0.9560 - val_loss: 0.3028 - val_accuracy: 0.9231\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1529 - accuracy: 0.9560 - val_loss: 0.3037 - val_accuracy: 0.9231\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1543 - accuracy: 0.9560 - val_loss: 0.3032 - val_accuracy: 0.9231\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1535 - accuracy: 0.9533 - val_loss: 0.3016 - val_accuracy: 0.9231\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1526 - accuracy: 0.9560 - val_loss: 0.3031 - val_accuracy: 0.9231\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1528 - accuracy: 0.9560 - val_loss: 0.3033 - val_accuracy: 0.9231\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1528 - accuracy: 0.9560 - val_loss: 0.3042 - val_accuracy: 0.9231\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1524 - accuracy: 0.9560 - val_loss: 0.3048 - val_accuracy: 0.9231\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1538 - accuracy: 0.9505 - val_loss: 0.3051 - val_accuracy: 0.9231\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1515 - accuracy: 0.9560 - val_loss: 0.3042 - val_accuracy: 0.9231\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1517 - accuracy: 0.9560 - val_loss: 0.3051 - val_accuracy: 0.9231\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1515 - accuracy: 0.9560 - val_loss: 0.3061 - val_accuracy: 0.9231\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1508 - accuracy: 0.9560 - val_loss: 0.3057 - val_accuracy: 0.9231\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1516 - accuracy: 0.9560 - val_loss: 0.3051 - val_accuracy: 0.9231\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1510 - accuracy: 0.9560 - val_loss: 0.3065 - val_accuracy: 0.9231\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1501 - accuracy: 0.9560 - val_loss: 0.3102 - val_accuracy: 0.9231\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1502 - accuracy: 0.9533 - val_loss: 0.3116 - val_accuracy: 0.9231\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1500 - accuracy: 0.9560 - val_loss: 0.3096 - val_accuracy: 0.9231\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1486 - accuracy: 0.9533 - val_loss: 0.3072 - val_accuracy: 0.9231\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1519 - accuracy: 0.9560 - val_loss: 0.3067 - val_accuracy: 0.9231\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1479 - accuracy: 0.9560 - val_loss: 0.3082 - val_accuracy: 0.9231\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1485 - accuracy: 0.9560 - val_loss: 0.3116 - val_accuracy: 0.9231\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1474 - accuracy: 0.9560 - val_loss: 0.3113 - val_accuracy: 0.9231\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1469 - accuracy: 0.9560 - val_loss: 0.3126 - val_accuracy: 0.9231\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1470 - accuracy: 0.9533 - val_loss: 0.3110 - val_accuracy: 0.9231\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1474 - accuracy: 0.9560 - val_loss: 0.3133 - val_accuracy: 0.9231\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1471 - accuracy: 0.9533 - val_loss: 0.3150 - val_accuracy: 0.9231\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1472 - accuracy: 0.9505 - val_loss: 0.3135 - val_accuracy: 0.9231\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1461 - accuracy: 0.9560 - val_loss: 0.3143 - val_accuracy: 0.9231\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1465 - accuracy: 0.9560 - val_loss: 0.3165 - val_accuracy: 0.9231\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1463 - accuracy: 0.9560 - val_loss: 0.3167 - val_accuracy: 0.9231\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1459 - accuracy: 0.9505 - val_loss: 0.3203 - val_accuracy: 0.9231\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1456 - accuracy: 0.9505 - val_loss: 0.3203 - val_accuracy: 0.9231\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1454 - accuracy: 0.9505 - val_loss: 0.3219 - val_accuracy: 0.9231\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1454 - accuracy: 0.9560 - val_loss: 0.3209 - val_accuracy: 0.9231\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1465 - accuracy: 0.9560 - val_loss: 0.3199 - val_accuracy: 0.9231\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1451 - accuracy: 0.9533 - val_loss: 0.3194 - val_accuracy: 0.9231\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1464 - accuracy: 0.9560 - val_loss: 0.3213 - val_accuracy: 0.9231\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1443 - accuracy: 0.9533 - val_loss: 0.3186 - val_accuracy: 0.9231\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1440 - accuracy: 0.9588 - val_loss: 0.3181 - val_accuracy: 0.9231\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1452 - accuracy: 0.9615 - val_loss: 0.3205 - val_accuracy: 0.9231\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1445 - accuracy: 0.9560 - val_loss: 0.3218 - val_accuracy: 0.9231\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1437 - accuracy: 0.9615 - val_loss: 0.3204 - val_accuracy: 0.9231\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1447 - accuracy: 0.9615 - val_loss: 0.3227 - val_accuracy: 0.9231\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1449 - accuracy: 0.9560 - val_loss: 0.3269 - val_accuracy: 0.9231\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1441 - accuracy: 0.9588 - val_loss: 0.3232 - val_accuracy: 0.9231\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1441 - accuracy: 0.9560 - val_loss: 0.3226 - val_accuracy: 0.9231\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1456 - accuracy: 0.9588 - val_loss: 0.3238 - val_accuracy: 0.9231\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1452 - accuracy: 0.9560 - val_loss: 0.3240 - val_accuracy: 0.9231\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1431 - accuracy: 0.9615 - val_loss: 0.3247 - val_accuracy: 0.9231\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1427 - accuracy: 0.9615 - val_loss: 0.3249 - val_accuracy: 0.9231\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1426 - accuracy: 0.9588 - val_loss: 0.3260 - val_accuracy: 0.9231\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1431 - accuracy: 0.9615 - val_loss: 0.3258 - val_accuracy: 0.9231\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1448 - accuracy: 0.9588 - val_loss: 0.3263 - val_accuracy: 0.9231\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1426 - accuracy: 0.9615 - val_loss: 0.3253 - val_accuracy: 0.9231\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1440 - accuracy: 0.9560 - val_loss: 0.3284 - val_accuracy: 0.9231\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1425 - accuracy: 0.9588 - val_loss: 0.3269 - val_accuracy: 0.9231\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1421 - accuracy: 0.9615 - val_loss: 0.3267 - val_accuracy: 0.9231\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1420 - accuracy: 0.9615 - val_loss: 0.3291 - val_accuracy: 0.9231\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1421 - accuracy: 0.9615 - val_loss: 0.3294 - val_accuracy: 0.9231\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1418 - accuracy: 0.9588 - val_loss: 0.3290 - val_accuracy: 0.9231\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1425 - accuracy: 0.9615 - val_loss: 0.3309 - val_accuracy: 0.9231\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1412 - accuracy: 0.9615 - val_loss: 0.3278 - val_accuracy: 0.9231\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1418 - accuracy: 0.9588 - val_loss: 0.3277 - val_accuracy: 0.9231\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1413 - accuracy: 0.9615 - val_loss: 0.3293 - val_accuracy: 0.9231\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1420 - accuracy: 0.9588 - val_loss: 0.3303 - val_accuracy: 0.9231\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1413 - accuracy: 0.9588 - val_loss: 0.3323 - val_accuracy: 0.9231\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1414 - accuracy: 0.9588 - val_loss: 0.3320 - val_accuracy: 0.9231\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1422 - accuracy: 0.9588 - val_loss: 0.3340 - val_accuracy: 0.9231\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1432 - accuracy: 0.9560 - val_loss: 0.3288 - val_accuracy: 0.9231\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1434 - accuracy: 0.9560 - val_loss: 0.3352 - val_accuracy: 0.9231\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1412 - accuracy: 0.9615 - val_loss: 0.3341 - val_accuracy: 0.9231\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1408 - accuracy: 0.9615 - val_loss: 0.3305 - val_accuracy: 0.9231\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1416 - accuracy: 0.9588 - val_loss: 0.3331 - val_accuracy: 0.9231\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1420 - accuracy: 0.9560 - val_loss: 0.3342 - val_accuracy: 0.9231\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1411 - accuracy: 0.9588 - val_loss: 0.3358 - val_accuracy: 0.9231\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1411 - accuracy: 0.9615 - val_loss: 0.3354 - val_accuracy: 0.9231\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.3350 - val_accuracy: 0.9231\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1424 - accuracy: 0.9560 - val_loss: 0.3350 - val_accuracy: 0.9231\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1406 - accuracy: 0.9615 - val_loss: 0.3344 - val_accuracy: 0.9231\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1405 - accuracy: 0.9588 - val_loss: 0.3354 - val_accuracy: 0.9231\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1415 - accuracy: 0.9615 - val_loss: 0.3381 - val_accuracy: 0.9231\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1417 - accuracy: 0.9615 - val_loss: 0.3357 - val_accuracy: 0.9231\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1406 - accuracy: 0.9615 - val_loss: 0.3342 - val_accuracy: 0.9231\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1432 - accuracy: 0.9615 - val_loss: 0.3363 - val_accuracy: 0.9231\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1402 - accuracy: 0.9615 - val_loss: 0.3334 - val_accuracy: 0.9231\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1413 - accuracy: 0.9615 - val_loss: 0.3353 - val_accuracy: 0.9231\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1415 - accuracy: 0.9615 - val_loss: 0.3363 - val_accuracy: 0.9231\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1409 - accuracy: 0.9588 - val_loss: 0.3393 - val_accuracy: 0.9231\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1402 - accuracy: 0.9615 - val_loss: 0.3423 - val_accuracy: 0.9231\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1408 - accuracy: 0.9615 - val_loss: 0.3411 - val_accuracy: 0.9231\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1415 - accuracy: 0.9588 - val_loss: 0.3350 - val_accuracy: 0.9231\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1414 - accuracy: 0.9615 - val_loss: 0.3358 - val_accuracy: 0.9231\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1410 - accuracy: 0.9615 - val_loss: 0.3382 - val_accuracy: 0.9231\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1411 - accuracy: 0.9560 - val_loss: 0.3410 - val_accuracy: 0.9231\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1421 - accuracy: 0.9615 - val_loss: 0.3412 - val_accuracy: 0.9231\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1408 - accuracy: 0.9588 - val_loss: 0.3391 - val_accuracy: 0.9231\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1404 - accuracy: 0.9560 - val_loss: 0.3380 - val_accuracy: 0.9231\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1401 - accuracy: 0.9588 - val_loss: 0.3362 - val_accuracy: 0.9231\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1404 - accuracy: 0.9533 - val_loss: 0.3383 - val_accuracy: 0.9231\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1404 - accuracy: 0.9560 - val_loss: 0.3413 - val_accuracy: 0.9231\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1400 - accuracy: 0.9588 - val_loss: 0.3390 - val_accuracy: 0.9231\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1405 - accuracy: 0.9615 - val_loss: 0.3399 - val_accuracy: 0.9231\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1408 - accuracy: 0.9560 - val_loss: 0.3445 - val_accuracy: 0.9231\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1400 - accuracy: 0.9615 - val_loss: 0.3451 - val_accuracy: 0.9231\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1394 - accuracy: 0.9615 - val_loss: 0.3425 - val_accuracy: 0.9231\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1402 - accuracy: 0.9615 - val_loss: 0.3425 - val_accuracy: 0.9231\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1395 - accuracy: 0.9615 - val_loss: 0.3433 - val_accuracy: 0.9231\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1394 - accuracy: 0.9615 - val_loss: 0.3435 - val_accuracy: 0.9231\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1397 - accuracy: 0.9615 - val_loss: 0.3458 - val_accuracy: 0.9231\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1397 - accuracy: 0.9560 - val_loss: 0.3443 - val_accuracy: 0.9231\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1415 - accuracy: 0.9615 - val_loss: 0.3444 - val_accuracy: 0.9231\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1396 - accuracy: 0.9588 - val_loss: 0.3411 - val_accuracy: 0.9231\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1412 - accuracy: 0.9560 - val_loss: 0.3414 - val_accuracy: 0.9231\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1399 - accuracy: 0.9615 - val_loss: 0.3417 - val_accuracy: 0.9231\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1413 - accuracy: 0.9588 - val_loss: 0.3461 - val_accuracy: 0.9231\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1411 - accuracy: 0.9615 - val_loss: 0.3475 - val_accuracy: 0.9231\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1395 - accuracy: 0.9615 - val_loss: 0.3454 - val_accuracy: 0.9231\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1390 - accuracy: 0.9588 - val_loss: 0.3440 - val_accuracy: 0.9231\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1412 - accuracy: 0.9615 - val_loss: 0.3424 - val_accuracy: 0.9231\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1405 - accuracy: 0.9588 - val_loss: 0.3454 - val_accuracy: 0.9231\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1391 - accuracy: 0.9588 - val_loss: 0.3441 - val_accuracy: 0.9231\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1402 - accuracy: 0.9560 - val_loss: 0.3416 - val_accuracy: 0.9231\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1405 - accuracy: 0.9615 - val_loss: 0.3434 - val_accuracy: 0.9231\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1397 - accuracy: 0.9588 - val_loss: 0.3433 - val_accuracy: 0.9231\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1387 - accuracy: 0.9615 - val_loss: 0.3450 - val_accuracy: 0.9231\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1395 - accuracy: 0.9588 - val_loss: 0.3446 - val_accuracy: 0.9231\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.3493 - val_accuracy: 0.9231\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1392 - accuracy: 0.9615 - val_loss: 0.3476 - val_accuracy: 0.9231\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1400 - accuracy: 0.9615 - val_loss: 0.3448 - val_accuracy: 0.9231\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1397 - accuracy: 0.9615 - val_loss: 0.3456 - val_accuracy: 0.9231\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1390 - accuracy: 0.9588 - val_loss: 0.3479 - val_accuracy: 0.9231\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1391 - accuracy: 0.9588 - val_loss: 0.3481 - val_accuracy: 0.9231\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1398 - accuracy: 0.9588 - val_loss: 0.3467 - val_accuracy: 0.9231\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1389 - accuracy: 0.9615 - val_loss: 0.3471 - val_accuracy: 0.9231\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1389 - accuracy: 0.9588 - val_loss: 0.3464 - val_accuracy: 0.9231\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1394 - accuracy: 0.9615 - val_loss: 0.3465 - val_accuracy: 0.9231\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1393 - accuracy: 0.9588 - val_loss: 0.3471 - val_accuracy: 0.9231\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1389 - accuracy: 0.9615 - val_loss: 0.3467 - val_accuracy: 0.9231\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1389 - accuracy: 0.9588 - val_loss: 0.3462 - val_accuracy: 0.9231\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3466 - val_accuracy: 0.9231\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1397 - accuracy: 0.9588 - val_loss: 0.3500 - val_accuracy: 0.9231\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1394 - accuracy: 0.9588 - val_loss: 0.3521 - val_accuracy: 0.9231\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1388 - accuracy: 0.9615 - val_loss: 0.3494 - val_accuracy: 0.9231\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1387 - accuracy: 0.9615 - val_loss: 0.3474 - val_accuracy: 0.9231\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1387 - accuracy: 0.9615 - val_loss: 0.3499 - val_accuracy: 0.9231\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3500 - val_accuracy: 0.9231\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1390 - accuracy: 0.9615 - val_loss: 0.3503 - val_accuracy: 0.9231\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1401 - accuracy: 0.9588 - val_loss: 0.3549 - val_accuracy: 0.9231\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3522 - val_accuracy: 0.9231\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1388 - accuracy: 0.9615 - val_loss: 0.3496 - val_accuracy: 0.9231\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1385 - accuracy: 0.9588 - val_loss: 0.3491 - val_accuracy: 0.9231\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1391 - accuracy: 0.9588 - val_loss: 0.3492 - val_accuracy: 0.9231\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3494 - val_accuracy: 0.9231\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3511 - val_accuracy: 0.9231\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1394 - accuracy: 0.9560 - val_loss: 0.3515 - val_accuracy: 0.9231\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3536 - val_accuracy: 0.9231\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1389 - accuracy: 0.9588 - val_loss: 0.3556 - val_accuracy: 0.9231\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1390 - accuracy: 0.9588 - val_loss: 0.3541 - val_accuracy: 0.9231\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1389 - accuracy: 0.9615 - val_loss: 0.3562 - val_accuracy: 0.9231\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3556 - val_accuracy: 0.9231\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1390 - accuracy: 0.9560 - val_loss: 0.3523 - val_accuracy: 0.9231\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1387 - accuracy: 0.9615 - val_loss: 0.3547 - val_accuracy: 0.9231\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3568 - val_accuracy: 0.9231\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1389 - accuracy: 0.9588 - val_loss: 0.3525 - val_accuracy: 0.9231\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1387 - accuracy: 0.9615 - val_loss: 0.3518 - val_accuracy: 0.9231\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1395 - accuracy: 0.9588 - val_loss: 0.3512 - val_accuracy: 0.9231\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1389 - accuracy: 0.9588 - val_loss: 0.3531 - val_accuracy: 0.9231\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1394 - accuracy: 0.9588 - val_loss: 0.3527 - val_accuracy: 0.9231\n",
            "Epoch 296/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1387 - accuracy: 0.9588 - val_loss: 0.3528 - val_accuracy: 0.9231\n",
            "Epoch 297/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1395 - accuracy: 0.9560 - val_loss: 0.3548 - val_accuracy: 0.9231\n",
            "Epoch 298/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1390 - accuracy: 0.9615 - val_loss: 0.3540 - val_accuracy: 0.9231\n",
            "Epoch 299/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3511 - val_accuracy: 0.9231\n",
            "Epoch 300/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1387 - accuracy: 0.9588 - val_loss: 0.3502 - val_accuracy: 0.9231\n",
            "Epoch 301/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1388 - accuracy: 0.9615 - val_loss: 0.3536 - val_accuracy: 0.9231\n",
            "Epoch 302/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1381 - accuracy: 0.9588 - val_loss: 0.3569 - val_accuracy: 0.9231\n",
            "Epoch 303/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3580 - val_accuracy: 0.9231\n",
            "Epoch 304/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1397 - accuracy: 0.9588 - val_loss: 0.3562 - val_accuracy: 0.9231\n",
            "Epoch 305/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1392 - accuracy: 0.9588 - val_loss: 0.3588 - val_accuracy: 0.9231\n",
            "Epoch 306/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1384 - accuracy: 0.9588 - val_loss: 0.3558 - val_accuracy: 0.9231\n",
            "Epoch 307/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3555 - val_accuracy: 0.9231\n",
            "Epoch 308/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3565 - val_accuracy: 0.9231\n",
            "Epoch 309/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1393 - accuracy: 0.9560 - val_loss: 0.3560 - val_accuracy: 0.9231\n",
            "Epoch 310/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3574 - val_accuracy: 0.9231\n",
            "Epoch 311/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1398 - accuracy: 0.9615 - val_loss: 0.3542 - val_accuracy: 0.9231\n",
            "Epoch 312/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1392 - accuracy: 0.9588 - val_loss: 0.3562 - val_accuracy: 0.9231\n",
            "Epoch 313/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1398 - accuracy: 0.9615 - val_loss: 0.3554 - val_accuracy: 0.9231\n",
            "Epoch 314/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1399 - accuracy: 0.9588 - val_loss: 0.3535 - val_accuracy: 0.9231\n",
            "Epoch 315/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1386 - accuracy: 0.9643 - val_loss: 0.3555 - val_accuracy: 0.9231\n",
            "Epoch 316/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3570 - val_accuracy: 0.9231\n",
            "Epoch 317/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1388 - accuracy: 0.9588 - val_loss: 0.3583 - val_accuracy: 0.9231\n",
            "Epoch 318/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1387 - accuracy: 0.9615 - val_loss: 0.3555 - val_accuracy: 0.9231\n",
            "Epoch 319/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3548 - val_accuracy: 0.9231\n",
            "Epoch 320/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1388 - accuracy: 0.9588 - val_loss: 0.3555 - val_accuracy: 0.9231\n",
            "Epoch 321/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1388 - accuracy: 0.9588 - val_loss: 0.3571 - val_accuracy: 0.9231\n",
            "Epoch 322/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1388 - accuracy: 0.9560 - val_loss: 0.3564 - val_accuracy: 0.9231\n",
            "Epoch 323/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1387 - accuracy: 0.9615 - val_loss: 0.3554 - val_accuracy: 0.9231\n",
            "Epoch 324/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1388 - accuracy: 0.9588 - val_loss: 0.3605 - val_accuracy: 0.9231\n",
            "Epoch 325/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1385 - accuracy: 0.9560 - val_loss: 0.3602 - val_accuracy: 0.9231\n",
            "Epoch 326/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1382 - accuracy: 0.9560 - val_loss: 0.3597 - val_accuracy: 0.9231\n",
            "Epoch 327/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1381 - accuracy: 0.9588 - val_loss: 0.3602 - val_accuracy: 0.9231\n",
            "Epoch 328/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3595 - val_accuracy: 0.9231\n",
            "Epoch 329/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1382 - accuracy: 0.9615 - val_loss: 0.3599 - val_accuracy: 0.9231\n",
            "Epoch 330/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1385 - accuracy: 0.9560 - val_loss: 0.3576 - val_accuracy: 0.9231\n",
            "Epoch 331/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1395 - accuracy: 0.9588 - val_loss: 0.3627 - val_accuracy: 0.9231\n",
            "Epoch 332/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1389 - accuracy: 0.9560 - val_loss: 0.3602 - val_accuracy: 0.9231\n",
            "Epoch 333/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1384 - accuracy: 0.9560 - val_loss: 0.3588 - val_accuracy: 0.9231\n",
            "Epoch 334/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1384 - accuracy: 0.9588 - val_loss: 0.3596 - val_accuracy: 0.9231\n",
            "Epoch 335/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3579 - val_accuracy: 0.9231\n",
            "Epoch 336/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1383 - accuracy: 0.9588 - val_loss: 0.3589 - val_accuracy: 0.9231\n",
            "Epoch 337/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1382 - accuracy: 0.9615 - val_loss: 0.3620 - val_accuracy: 0.9231\n",
            "Epoch 338/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1383 - accuracy: 0.9588 - val_loss: 0.3629 - val_accuracy: 0.9231\n",
            "Epoch 339/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1389 - accuracy: 0.9615 - val_loss: 0.3607 - val_accuracy: 0.9231\n",
            "Epoch 340/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1391 - accuracy: 0.9615 - val_loss: 0.3602 - val_accuracy: 0.9231\n",
            "Epoch 341/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3606 - val_accuracy: 0.9231\n",
            "Epoch 342/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1392 - accuracy: 0.9615 - val_loss: 0.3648 - val_accuracy: 0.9231\n",
            "Epoch 343/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3636 - val_accuracy: 0.9231\n",
            "Epoch 344/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1379 - accuracy: 0.9615 - val_loss: 0.3619 - val_accuracy: 0.9231\n",
            "Epoch 345/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1383 - accuracy: 0.9588 - val_loss: 0.3606 - val_accuracy: 0.9231\n",
            "Epoch 346/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1382 - accuracy: 0.9588 - val_loss: 0.3619 - val_accuracy: 0.9231\n",
            "Epoch 347/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1391 - accuracy: 0.9588 - val_loss: 0.3621 - val_accuracy: 0.9231\n",
            "Epoch 348/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3574 - val_accuracy: 0.9231\n",
            "Epoch 349/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1402 - accuracy: 0.9588 - val_loss: 0.3573 - val_accuracy: 0.9231\n",
            "Epoch 350/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1386 - accuracy: 0.9588 - val_loss: 0.3600 - val_accuracy: 0.9231\n",
            "Epoch 351/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1393 - accuracy: 0.9588 - val_loss: 0.3582 - val_accuracy: 0.9231\n",
            "Epoch 352/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3597 - val_accuracy: 0.9231\n",
            "Epoch 353/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1389 - accuracy: 0.9560 - val_loss: 0.3600 - val_accuracy: 0.9231\n",
            "Epoch 354/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3612 - val_accuracy: 0.9231\n",
            "Epoch 355/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1390 - accuracy: 0.9615 - val_loss: 0.3574 - val_accuracy: 0.9231\n",
            "Epoch 356/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3565 - val_accuracy: 0.9231\n",
            "Epoch 357/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1388 - accuracy: 0.9560 - val_loss: 0.3605 - val_accuracy: 0.9231\n",
            "Epoch 358/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1382 - accuracy: 0.9588 - val_loss: 0.3604 - val_accuracy: 0.9231\n",
            "Epoch 359/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1379 - accuracy: 0.9588 - val_loss: 0.3591 - val_accuracy: 0.9231\n",
            "Epoch 360/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3588 - val_accuracy: 0.9231\n",
            "Epoch 361/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1388 - accuracy: 0.9615 - val_loss: 0.3608 - val_accuracy: 0.9231\n",
            "Epoch 362/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3614 - val_accuracy: 0.9231\n",
            "Epoch 363/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1382 - accuracy: 0.9588 - val_loss: 0.3609 - val_accuracy: 0.9231\n",
            "Epoch 364/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1388 - accuracy: 0.9588 - val_loss: 0.3633 - val_accuracy: 0.9231\n",
            "Epoch 365/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1393 - accuracy: 0.9560 - val_loss: 0.3625 - val_accuracy: 0.9231\n",
            "Epoch 366/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3600 - val_accuracy: 0.9231\n",
            "Epoch 367/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1387 - accuracy: 0.9615 - val_loss: 0.3574 - val_accuracy: 0.9231\n",
            "Epoch 368/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1382 - accuracy: 0.9615 - val_loss: 0.3576 - val_accuracy: 0.9231\n",
            "Epoch 369/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3608 - val_accuracy: 0.9231\n",
            "Epoch 370/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3617 - val_accuracy: 0.9231\n",
            "Epoch 371/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3622 - val_accuracy: 0.9231\n",
            "Epoch 372/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1386 - accuracy: 0.9588 - val_loss: 0.3632 - val_accuracy: 0.9231\n",
            "Epoch 373/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1382 - accuracy: 0.9615 - val_loss: 0.3646 - val_accuracy: 0.9231\n",
            "Epoch 374/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1379 - accuracy: 0.9588 - val_loss: 0.3625 - val_accuracy: 0.9231\n",
            "Epoch 375/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3630 - val_accuracy: 0.9231\n",
            "Epoch 376/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1383 - accuracy: 0.9588 - val_loss: 0.3616 - val_accuracy: 0.9231\n",
            "Epoch 377/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3618 - val_accuracy: 0.9231\n",
            "Epoch 378/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3637 - val_accuracy: 0.9231\n",
            "Epoch 379/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3636 - val_accuracy: 0.9231\n",
            "Epoch 380/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1393 - accuracy: 0.9560 - val_loss: 0.3614 - val_accuracy: 0.9231\n",
            "Epoch 381/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3623 - val_accuracy: 0.9231\n",
            "Epoch 382/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1384 - accuracy: 0.9560 - val_loss: 0.3648 - val_accuracy: 0.9231\n",
            "Epoch 383/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1389 - accuracy: 0.9615 - val_loss: 0.3654 - val_accuracy: 0.9231\n",
            "Epoch 384/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1398 - accuracy: 0.9588 - val_loss: 0.3666 - val_accuracy: 0.9231\n",
            "Epoch 385/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1395 - accuracy: 0.9560 - val_loss: 0.3624 - val_accuracy: 0.9231\n",
            "Epoch 386/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1383 - accuracy: 0.9560 - val_loss: 0.3651 - val_accuracy: 0.9231\n",
            "Epoch 387/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1381 - accuracy: 0.9588 - val_loss: 0.3639 - val_accuracy: 0.9231\n",
            "Epoch 388/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1379 - accuracy: 0.9588 - val_loss: 0.3669 - val_accuracy: 0.9231\n",
            "Epoch 389/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1380 - accuracy: 0.9588 - val_loss: 0.3684 - val_accuracy: 0.9231\n",
            "Epoch 390/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1385 - accuracy: 0.9588 - val_loss: 0.3685 - val_accuracy: 0.9231\n",
            "Epoch 391/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1384 - accuracy: 0.9588 - val_loss: 0.3694 - val_accuracy: 0.9231\n",
            "Epoch 392/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1385 - accuracy: 0.9588 - val_loss: 0.3677 - val_accuracy: 0.9231\n",
            "Epoch 393/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1387 - accuracy: 0.9588 - val_loss: 0.3663 - val_accuracy: 0.9231\n",
            "Epoch 394/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1382 - accuracy: 0.9560 - val_loss: 0.3656 - val_accuracy: 0.9231\n",
            "Epoch 395/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1381 - accuracy: 0.9588 - val_loss: 0.3631 - val_accuracy: 0.9231\n",
            "Epoch 396/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1385 - accuracy: 0.9560 - val_loss: 0.3658 - val_accuracy: 0.9231\n",
            "Epoch 397/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3668 - val_accuracy: 0.9231\n",
            "Epoch 398/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1379 - accuracy: 0.9615 - val_loss: 0.3674 - val_accuracy: 0.9231\n",
            "Epoch 399/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1384 - accuracy: 0.9588 - val_loss: 0.3667 - val_accuracy: 0.9231\n",
            "Epoch 400/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3669 - val_accuracy: 0.9231\n",
            "Epoch 401/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1376 - accuracy: 0.9588 - val_loss: 0.3650 - val_accuracy: 0.9231\n",
            "Epoch 402/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1389 - accuracy: 0.9615 - val_loss: 0.3684 - val_accuracy: 0.9231\n",
            "Epoch 403/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3672 - val_accuracy: 0.9231\n",
            "Epoch 404/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3671 - val_accuracy: 0.9231\n",
            "Epoch 405/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1387 - accuracy: 0.9533 - val_loss: 0.3679 - val_accuracy: 0.9231\n",
            "Epoch 406/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3666 - val_accuracy: 0.9231\n",
            "Epoch 407/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1379 - accuracy: 0.9560 - val_loss: 0.3681 - val_accuracy: 0.9231\n",
            "Epoch 408/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3706 - val_accuracy: 0.9231\n",
            "Epoch 409/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3697 - val_accuracy: 0.9231\n",
            "Epoch 410/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1388 - accuracy: 0.9588 - val_loss: 0.3667 - val_accuracy: 0.9231\n",
            "Epoch 411/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3678 - val_accuracy: 0.9231\n",
            "Epoch 412/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3679 - val_accuracy: 0.9231\n",
            "Epoch 413/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1388 - accuracy: 0.9588 - val_loss: 0.3686 - val_accuracy: 0.9231\n",
            "Epoch 414/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1400 - accuracy: 0.9588 - val_loss: 0.3643 - val_accuracy: 0.9231\n",
            "Epoch 415/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1389 - accuracy: 0.9588 - val_loss: 0.3659 - val_accuracy: 0.9231\n",
            "Epoch 416/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3672 - val_accuracy: 0.9231\n",
            "Epoch 417/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1375 - accuracy: 0.9588 - val_loss: 0.3671 - val_accuracy: 0.9231\n",
            "Epoch 418/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3680 - val_accuracy: 0.9231\n",
            "Epoch 419/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1390 - accuracy: 0.9588 - val_loss: 0.3631 - val_accuracy: 0.9231\n",
            "Epoch 420/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1379 - accuracy: 0.9615 - val_loss: 0.3657 - val_accuracy: 0.9231\n",
            "Epoch 421/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3690 - val_accuracy: 0.9231\n",
            "Epoch 422/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1378 - accuracy: 0.9615 - val_loss: 0.3683 - val_accuracy: 0.9231\n",
            "Epoch 423/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1389 - accuracy: 0.9588 - val_loss: 0.3680 - val_accuracy: 0.9231\n",
            "Epoch 424/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1377 - accuracy: 0.9615 - val_loss: 0.3671 - val_accuracy: 0.9231\n",
            "Epoch 425/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1382 - accuracy: 0.9615 - val_loss: 0.3686 - val_accuracy: 0.9231\n",
            "Epoch 426/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1378 - accuracy: 0.9615 - val_loss: 0.3662 - val_accuracy: 0.9231\n",
            "Epoch 427/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1378 - accuracy: 0.9615 - val_loss: 0.3668 - val_accuracy: 0.9231\n",
            "Epoch 428/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3659 - val_accuracy: 0.9231\n",
            "Epoch 429/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3676 - val_accuracy: 0.9231\n",
            "Epoch 430/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1387 - accuracy: 0.9588 - val_loss: 0.3718 - val_accuracy: 0.9231\n",
            "Epoch 431/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3702 - val_accuracy: 0.9231\n",
            "Epoch 432/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1379 - accuracy: 0.9615 - val_loss: 0.3680 - val_accuracy: 0.9231\n",
            "Epoch 433/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1387 - accuracy: 0.9615 - val_loss: 0.3686 - val_accuracy: 0.9231\n",
            "Epoch 434/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1377 - accuracy: 0.9588 - val_loss: 0.3697 - val_accuracy: 0.9231\n",
            "Epoch 435/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3721 - val_accuracy: 0.9231\n",
            "Epoch 436/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1378 - accuracy: 0.9588 - val_loss: 0.3726 - val_accuracy: 0.9231\n",
            "Epoch 437/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1376 - accuracy: 0.9588 - val_loss: 0.3701 - val_accuracy: 0.9231\n",
            "Epoch 438/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1382 - accuracy: 0.9588 - val_loss: 0.3703 - val_accuracy: 0.9231\n",
            "Epoch 439/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3669 - val_accuracy: 0.9231\n",
            "Epoch 440/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1378 - accuracy: 0.9588 - val_loss: 0.3685 - val_accuracy: 0.9231\n",
            "Epoch 441/500\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3681 - val_accuracy: 0.9231\n",
            "Epoch 442/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1379 - accuracy: 0.9615 - val_loss: 0.3708 - val_accuracy: 0.9231\n",
            "Epoch 443/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1385 - accuracy: 0.9588 - val_loss: 0.3720 - val_accuracy: 0.9231\n",
            "Epoch 444/500\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3729 - val_accuracy: 0.9231\n",
            "Epoch 445/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1385 - accuracy: 0.9588 - val_loss: 0.3754 - val_accuracy: 0.9231\n",
            "Epoch 446/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3732 - val_accuracy: 0.9231\n",
            "Epoch 447/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3710 - val_accuracy: 0.9231\n",
            "Epoch 448/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1377 - accuracy: 0.9588 - val_loss: 0.3730 - val_accuracy: 0.9231\n",
            "Epoch 449/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1377 - accuracy: 0.9615 - val_loss: 0.3747 - val_accuracy: 0.9231\n",
            "Epoch 450/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1382 - accuracy: 0.9615 - val_loss: 0.3721 - val_accuracy: 0.9231\n",
            "Epoch 451/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3727 - val_accuracy: 0.9231\n",
            "Epoch 452/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3710 - val_accuracy: 0.9231\n",
            "Epoch 453/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1383 - accuracy: 0.9588 - val_loss: 0.3746 - val_accuracy: 0.9231\n",
            "Epoch 454/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1376 - accuracy: 0.9615 - val_loss: 0.3742 - val_accuracy: 0.9231\n",
            "Epoch 455/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1379 - accuracy: 0.9588 - val_loss: 0.3732 - val_accuracy: 0.9231\n",
            "Epoch 456/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1378 - accuracy: 0.9615 - val_loss: 0.3762 - val_accuracy: 0.9231\n",
            "Epoch 457/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1380 - accuracy: 0.9560 - val_loss: 0.3738 - val_accuracy: 0.9231\n",
            "Epoch 458/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1375 - accuracy: 0.9615 - val_loss: 0.3736 - val_accuracy: 0.9231\n",
            "Epoch 459/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1380 - accuracy: 0.9560 - val_loss: 0.3748 - val_accuracy: 0.9231\n",
            "Epoch 460/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1377 - accuracy: 0.9615 - val_loss: 0.3752 - val_accuracy: 0.9231\n",
            "Epoch 461/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1390 - accuracy: 0.9588 - val_loss: 0.3765 - val_accuracy: 0.9231\n",
            "Epoch 462/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1377 - accuracy: 0.9615 - val_loss: 0.3711 - val_accuracy: 0.9231\n",
            "Epoch 463/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1385 - accuracy: 0.9560 - val_loss: 0.3687 - val_accuracy: 0.9231\n",
            "Epoch 464/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1373 - accuracy: 0.9615 - val_loss: 0.3728 - val_accuracy: 0.9231\n",
            "Epoch 465/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1386 - accuracy: 0.9615 - val_loss: 0.3725 - val_accuracy: 0.9231\n",
            "Epoch 466/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1384 - accuracy: 0.9615 - val_loss: 0.3736 - val_accuracy: 0.9231\n",
            "Epoch 467/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1380 - accuracy: 0.9588 - val_loss: 0.3741 - val_accuracy: 0.9231\n",
            "Epoch 468/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3731 - val_accuracy: 0.9231\n",
            "Epoch 469/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3718 - val_accuracy: 0.9231\n",
            "Epoch 470/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3720 - val_accuracy: 0.9231\n",
            "Epoch 471/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1380 - accuracy: 0.9615 - val_loss: 0.3712 - val_accuracy: 0.9231\n",
            "Epoch 472/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1389 - accuracy: 0.9615 - val_loss: 0.3759 - val_accuracy: 0.9231\n",
            "Epoch 473/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1382 - accuracy: 0.9615 - val_loss: 0.3738 - val_accuracy: 0.9231\n",
            "Epoch 474/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1380 - accuracy: 0.9560 - val_loss: 0.3744 - val_accuracy: 0.9231\n",
            "Epoch 475/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3735 - val_accuracy: 0.9231\n",
            "Epoch 476/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1376 - accuracy: 0.9615 - val_loss: 0.3746 - val_accuracy: 0.9231\n",
            "Epoch 477/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3751 - val_accuracy: 0.9231\n",
            "Epoch 478/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1376 - accuracy: 0.9588 - val_loss: 0.3773 - val_accuracy: 0.9231\n",
            "Epoch 479/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1375 - accuracy: 0.9615 - val_loss: 0.3811 - val_accuracy: 0.9231\n",
            "Epoch 480/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3826 - val_accuracy: 0.9231\n",
            "Epoch 481/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3785 - val_accuracy: 0.9231\n",
            "Epoch 482/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1387 - accuracy: 0.9588 - val_loss: 0.3768 - val_accuracy: 0.9231\n",
            "Epoch 483/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1397 - accuracy: 0.9588 - val_loss: 0.3780 - val_accuracy: 0.9231\n",
            "Epoch 484/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.3740 - val_accuracy: 0.9231\n",
            "Epoch 485/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1378 - accuracy: 0.9615 - val_loss: 0.3750 - val_accuracy: 0.9231\n",
            "Epoch 486/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1376 - accuracy: 0.9615 - val_loss: 0.3796 - val_accuracy: 0.9231\n",
            "Epoch 487/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1395 - accuracy: 0.9615 - val_loss: 0.3786 - val_accuracy: 0.9231\n",
            "Epoch 488/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1383 - accuracy: 0.9588 - val_loss: 0.3777 - val_accuracy: 0.9231\n",
            "Epoch 489/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1376 - accuracy: 0.9615 - val_loss: 0.3750 - val_accuracy: 0.9231\n",
            "Epoch 490/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1385 - accuracy: 0.9615 - val_loss: 0.3746 - val_accuracy: 0.9231\n",
            "Epoch 491/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1385 - accuracy: 0.9588 - val_loss: 0.3788 - val_accuracy: 0.9231\n",
            "Epoch 492/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1378 - accuracy: 0.9588 - val_loss: 0.3775 - val_accuracy: 0.9231\n",
            "Epoch 493/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1377 - accuracy: 0.9615 - val_loss: 0.3792 - val_accuracy: 0.9231\n",
            "Epoch 494/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1398 - accuracy: 0.9588 - val_loss: 0.3795 - val_accuracy: 0.9231\n",
            "Epoch 495/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1385 - accuracy: 0.9588 - val_loss: 0.3772 - val_accuracy: 0.9231\n",
            "Epoch 496/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1375 - accuracy: 0.9615 - val_loss: 0.3757 - val_accuracy: 0.9231\n",
            "Epoch 497/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1383 - accuracy: 0.9615 - val_loss: 0.3764 - val_accuracy: 0.9231\n",
            "Epoch 498/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1375 - accuracy: 0.9588 - val_loss: 0.3788 - val_accuracy: 0.9231\n",
            "Epoch 499/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1379 - accuracy: 0.9588 - val_loss: 0.3802 - val_accuracy: 0.9231\n",
            "Epoch 500/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1382 - accuracy: 0.9615 - val_loss: 0.3800 - val_accuracy: 0.9231\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd974f7ac50>"
            ]
          },
          "metadata": {},
          "execution_count": 389
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred_DNN = (model.predict(DNNX) > 0.5).astype(\"int32\")\n",
        "DTr = model.evaluate(DX_train, Dy_train,verbose=0)[1]\n",
        "DTe = model.evaluate(DX_test, Dy_test,verbose=0)[1]\n",
        "DTr,DTe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhG0YCbhASH9",
        "outputId": "2f25f202-cc6c-4581-c623-298d38e8b4c5"
      },
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9538461565971375, 0.9035087823867798)"
            ]
          },
          "metadata": {},
          "execution_count": 390
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_DNN = (model.predict(DX_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKbxrS57kOBE",
        "outputId": "e96e5fb0-d83d-4397-ad7d-c1912c6f6eba"
      },
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"DNN\":[DTr,DTe]})\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "v_YjlCdOO1JY",
        "outputId": "f7fbedda-3029-4c57-87c3-bfb885a2a682"
      },
      "execution_count": 392,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        train      test\n",
              "SVM  0.951648  0.956140\n",
              "KNN  0.980220  0.991228\n",
              "RF   0.975824  0.991228\n",
              "LR   0.980220  0.973684\n",
              "ANN  0.951648  0.956140\n",
              "XGB  0.989011  0.982456\n",
              "DNN  0.953846  0.903509"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b58d905c-50ff-435a-99d2-fb3ec4f8af5e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.991228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>0.975824</td>\n",
              "      <td>0.991228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.973684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>0.989011</td>\n",
              "      <td>0.982456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>0.953846</td>\n",
              "      <td>0.903509</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b58d905c-50ff-435a-99d2-fb3ec4f8af5e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b58d905c-50ff-435a-99d2-fb3ec4f8af5e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b58d905c-50ff-435a-99d2-fb3ec4f8af5e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 392
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AutoML Individual and AutoML DNN**"
      ],
      "metadata": {
        "id": "nJaAMLDKKfKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H2O AutoML"
      ],
      "metadata": {
        "id": "EhNt_UkjDKcV"
      },
      "execution_count": 393,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h2o\n",
        "import h2o\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "h2o.init()\n",
        "from h2o.model.segment_models import H2OFrame\n",
        "from h2o.automl import H2OAutoML\n",
        "print(\"All Library Loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "iwMF9ROeDOzF",
        "outputId": "8c0b84cc-7d87-49e2-a1f1-1fafc9758bf6"
      },
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h2o in /usr/local/lib/python3.10/dist-packages (3.40.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.8.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from h2o) (0.18.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.4)\n",
            "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         6 hours 5 mins\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.40.0.4\n",
              "H2O_cluster_version_age:    1 month and 25 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_f0vf96\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    2.948 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://localhost:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-94.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-94 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-94 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-94 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-94 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-94 .h2o-table th,\n",
              "#h2o-table-94 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-94 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-94\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>6 hours 5 mins</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.40.0.4</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>1 month and 25 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_f0vf96</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>2.948 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://localhost:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Library Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWdoOLbsF2qs"
      },
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-QiglXeF_XE"
      },
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8P1HTOHbugE-"
      },
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asKywHvIu83f"
      },
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5wVvB0pvWHe"
      },
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train, valid = hdf.split_frame(ratios=[.8], seed=123)\n",
        "#hdf = h2o.H2OFrame(df)\n",
        "#hdf[\"diagnosis\"] = hdf[\"diagnosis\"].asfactor()\n",
        "hy = \"diagnosis\"\n",
        "hx = list(df.columns)\n",
        "hx.remove(hy)\n",
        "hdf  = df.copy()\n",
        "hdf.iloc[:,1:] = StandardScaler().fit_transform(hdf.iloc[:,1:])\n",
        "hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n",
        "hdf.iloc[:,0] = hdf.iloc[:,0].astype('category')\n",
        "train1, valid1 = train_test_split(hdf, test_size=0.2,random_state=123)\n",
        "train = h2o.H2OFrame(train1)\n",
        "valid = h2o.H2OFrame(valid1)\n",
        "train[\"diagnosis\"] = train[\"diagnosis\"].asfactor()\n",
        "valid[\"diagnosis\"] = valid[\"diagnosis\"].asfactor()"
      ],
      "metadata": {
        "id": "NVoFNbYCGxGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687e03de-c6b2-46bb-e17e-dcc3b1c27315"
      },
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-395-460708a37676>:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\",\n",
        "                nfolds=10, sort_metric='accuracy')"
      ],
      "metadata": {
        "id": "7JNO6XnVHH5P"
      },
      "execution_count": 396,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aml.train(x = hx, y = hy, training_frame = train,\n",
        "          validation_frame = valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lbo606kFH4Zc",
        "outputId": "75a39549-26da-41ce-db3f-fbe3ba57385d"
      },
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoML progress: |\n",
            "20:25:11.769: Project: AutoML_9_20230622_202511\n",
            "20:25:11.770: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "20:25:11.770: Setting stopping tolerance adaptively based on the training frame: 0.04688072309384954\n",
            "20:25:11.770: Build control seed: 123\n",
            "20:25:11.770: training frame: Frame key: AutoML_9_20230622_202511_training_py_188_sid_b073    cols: 31    rows: 455  chunks: 1    size: 111919  checksum: -6999049652048799504\n",
            "20:25:11.771: validation frame: Frame key: py_189_sid_b073    cols: 31    rows: 114  chunks: 1    size: 30037  checksum: 1244986154056216624\n",
            "20:25:11.771: leaderboard frame: NULL\n",
            "20:25:11.771: blending frame: NULL\n",
            "20:25:11.771: response column: diagnosis\n",
            "20:25:11.771: fold column: null\n",
            "20:25:11.771: weights column: null\n",
            "20:25:11.777: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "20:25:11.779: AutoML job created: 2023.06.22 20:25:11.766\n",
            "20:25:11.781: AutoML build started: 2023.06.22 20:25:11.781\n",
            "20:25:11.783: AutoML: starting XGBoost_1_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆ\n",
            "20:25:14.393: New leader: XGBoost_1_AutoML_9_20230622_202511, accuracy: 0.9428571428571428\n",
            "20:25:14.394: AutoML: starting GLM_1_AutoML_9_20230622_202511 model training\n",
            "\n",
            "\n",
            "20:25:15.907: AutoML: starting GBM_1_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆ\n",
            "20:25:23.712: AutoML: starting XGBoost_2_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆ\n",
            "20:25:28.959: AutoML: starting DRF_1_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆ\n",
            "20:25:30.870: AutoML: starting GBM_2_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "20:25:41.211: AutoML: starting GBM_3_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "20:25:50.316: AutoML: starting GBM_4_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆ\n",
            "20:26:00.175: AutoML: starting XGBoost_3_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆ\n",
            "20:26:06.3: AutoML: starting XRT_1_AutoML_9_20230622_202511 model training\n",
            "\n",
            "\n",
            "20:26:09.271: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "20:26:09.272: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆ\n",
            "20:26:13.903: AutoML: starting StackedEnsemble_AllModels_1_AutoML_9_20230622_202511 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "\n",
            "20:26:16.665: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "20:26:16.665: AutoML build stopped: 2023.06.22 20:26:16.665\n",
            "20:26:16.665: AutoML build done: built 10 models\n",
            "20:26:16.665: AutoML duration:  1 min  4.884 sec\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_9_20230622_202511\n",
              "\n",
              "\n",
              "Model Summary: \n",
              "    number_of_trees\n",
              "--  -----------------\n",
              "    36\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.029074514855628383\n",
              "RMSE: 0.17051250644931704\n",
              "LogLoss: 0.12394929039565182\n",
              "Mean Per-Class Error: 0.029270653158718393\n",
              "AUC: 0.9940799769376494\n",
              "AUCPR: 0.992056368504806\n",
              "Gini: 0.9881599538752988\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4925362765789032\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  ------------\n",
              "0      279  5    0.0176   (5.0/284.0)\n",
              "1      7    164  0.0409   (7.0/171.0)\n",
              "Total  286  169  0.0264   (12.0/455.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.492536     0.964706  38\n",
              "max f2                       0.317989     0.971098  49\n",
              "max f0point5                 0.514766     0.971395  37\n",
              "max accuracy                 0.514766     0.973626  37\n",
              "max precision                0.968002     1         0\n",
              "max recall                   0.0588847    1         100\n",
              "max specificity              0.968002     1         0\n",
              "max absolute_mcc             0.492536     0.943696  38\n",
              "max min_per_class_accuracy   0.402806     0.964912  42\n",
              "max mean_per_class_accuracy  0.492536     0.970729  38\n",
              "max tns                      0.968002     284       0\n",
              "max fns                      0.968002     110       0\n",
              "max fps                      0.0148769    284       134\n",
              "max tps                      0.0588847    171       100\n",
              "max tnr                      0.968002     1         0\n",
              "max fnr                      0.968002     0.643275  0\n",
              "max fpr                      0.0148769    1         134\n",
              "max tpr                      0.0588847    1         100\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.29 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.134066                    0.968002           2.66082    2.66082            1                0.968002   1                           0.968002            0.356725        0.356725                   166.082   166.082            0.356725\n",
              "2        0.162637                    0.957612           2.66082    2.66082            1                0.957612   1                           0.966177            0.0760234       0.432749                   166.082   166.082            0.432749\n",
              "3        0.2                         0.950501           2.66082    2.66082            1                0.953488   1                           0.963806            0.0994152       0.532164                   166.082   166.082            0.532164\n",
              "4        0.301099                    0.807568           2.66082    2.66082            1                0.897179   1                           0.941435            0.269006        0.80117                    166.082   166.082            0.80117\n",
              "5        0.4                         0.314704           1.83301    2.45614            0.688889         0.582323   0.923077                    0.852644            0.181287        0.982456                   83.3008   145.614            0.93316\n",
              "6        0.501099                    0.076693           0.0578439  1.97227            0.0217391        0.161579   0.741228                    0.713218            0.00584795      0.988304                   -94.2156  97.2274            0.780558\n",
              "7        0.624176                    0.0503081          0.0950292  1.60211            0.0357143        0.058398   0.602113                    0.584099            0.0116959       1                          -90.4971  60.2113            0.602113\n",
              "8        0.698901                    0.0279195          0          1.43082            0                0.0365022  0.537736                    0.525551            0               1                          -100      43.0818            0.482394\n",
              "9        0.804396                    0.0177071          0          1.24317            0                0.0225316  0.467213                    0.459581            0               1                          -100      24.3169            0.31338\n",
              "10       0.931868                    0.0175245          0          1.07311            0                0.0175245  0.403302                    0.399111            0               1                          -100      7.31132            0.109155\n",
              "11       1                           0.0148769          0          1                  0                0.0148769  0.375824                    0.372932            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.03762067586537018\n",
              "RMSE: 0.19396050078655236\n",
              "LogLoss: 0.1512542647982658\n",
              "Mean Per-Class Error: 0.03274306715669896\n",
              "AUC: 0.976445038422987\n",
              "AUCPR: 0.9791961628675718\n",
              "Gini: 0.952890076845974\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25368359684944153\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  -----------\n",
              "0      70   3    0.0411   (3.0/73.0)\n",
              "1      1    40   0.0244   (1.0/41.0)\n",
              "Total  71   43   0.0351   (4.0/114.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.253684     0.952381  20\n",
              "max f2                       0.253684     0.966184  20\n",
              "max f0point5                 0.705651     0.972973  15\n",
              "max accuracy                 0.253684     0.964912  20\n",
              "max precision                0.968002     1         0\n",
              "max recall                   0.0175245    1         46\n",
              "max specificity              0.968002     1         0\n",
              "max absolute_mcc             0.253684     0.925285  20\n",
              "max min_per_class_accuracy   0.253684     0.958904  20\n",
              "max mean_per_class_accuracy  0.253684     0.967257  20\n",
              "max tns                      0.968002     73        0\n",
              "max fns                      0.968002     27        0\n",
              "max fps                      0.0148769    73        47\n",
              "max tps                      0.0175245    41        46\n",
              "max tnr                      0.968002     1         0\n",
              "max fnr                      0.968002     0.658537  0\n",
              "max fpr                      0.0148769    1         47\n",
              "max tpr                      0.0175245    1         46\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 35.96 %, avg score: 34.85 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.122807                    0.968002           2.78049    2.78049            1                0.968002   1                           0.968002            0.341463        0.341463                   178.049   178.049            0.341463\n",
              "2        0.157895                    0.954363           2.78049    2.78049            1                0.955175   1                           0.965152            0.097561        0.439024                   178.049   178.049            0.439024\n",
              "3        0.210526                    0.932284           2.78049    2.78049            1                0.935408   1                           0.957716            0.146341        0.585366                   178.049   178.049            0.585366\n",
              "4        0.298246                    0.717271           2.78049    2.78049            1                0.870593   1                           0.932091            0.243902        0.829268                   178.049   178.049            0.829268\n",
              "5        0.403509                    0.169029           1.39024    2.41782            0.5              0.45112    0.869565                    0.806621            0.146341        0.97561                    39.0244   141.782            0.893418\n",
              "6        0.508772                    0.0588847          0          1.91758            0                0.093715   0.689655                    0.659123            0               0.97561                    -100      91.7578            0.729034\n",
              "7        0.596491                    0.0359404          0          1.63558            0                0.0470055  0.588235                    0.569106            0               0.97561                    -100      63.5581            0.592048\n",
              "8        0.701754                    0.023509           0          1.39024            0                0.0319946  0.5                         0.488539            0               0.97561                    -100      39.0244            0.427665\n",
              "9        0.973684                    0.0175245          0.0896932  1.02703            0.0322581        0.0192596  0.369369                    0.357479            0.0243902       1                          -91.0307  2.7027             0.0410959\n",
              "10       1                           0.0148769          0          1                  0                0.0148769  0.359649                    0.348463            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.044205368519838424\n",
              "RMSE: 0.21025072775103162\n",
              "LogLoss: 0.16637884924436225\n",
              "Mean Per-Class Error: 0.05391854048266205\n",
              "AUC: 0.9841137468083354\n",
              "AUCPR: 0.9796363289807396\n",
              "Gini: 0.9682274936166708\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35231223702430725\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  ------------\n",
              "0      265  19   0.0669   (19.0/284.0)\n",
              "1      7    164  0.0409   (7.0/171.0)\n",
              "Total  272  183  0.0571   (26.0/455.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.352312     0.926554  103\n",
              "max f2                       0.352312     0.94579   103\n",
              "max f0point5                 0.648854     0.953307  71\n",
              "max accuracy                 0.624354     0.942857  74\n",
              "max precision                0.968105     1         0\n",
              "max recall                   0.0317377    1         212\n",
              "max specificity              0.968105     1         0\n",
              "max absolute_mcc             0.352312     0.881234  103\n",
              "max min_per_class_accuracy   0.393351     0.940141  98\n",
              "max mean_per_class_accuracy  0.352312     0.946081  103\n",
              "max tns                      0.968105     284       0\n",
              "max fns                      0.968105     168       0\n",
              "max fps                      0.0161675    284       239\n",
              "max tps                      0.0317377    171       212\n",
              "max tnr                      0.968105     1         0\n",
              "max fnr                      0.968105     0.982456  0\n",
              "max fpr                      0.0161675    1         239\n",
              "max tpr                      0.0317377    1         212\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.58 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.0153846                   0.964642           2.66082    2.66082            1                0.966126   1                           0.966126            0.0409357       0.0409357                  166.082   166.082            0.0409357\n",
              "2        0.032967                    0.964165           2.66082    2.66082            1                0.964165   1                           0.96508             0.0467836       0.0877193                  166.082   166.082            0.0877193\n",
              "3        0.0461538                   0.96322            2.66082    2.66082            1                0.96322    1                           0.964549            0.0350877       0.122807                   166.082   166.082            0.122807\n",
              "4        0.0659341                   0.961647           2.66082    2.66082            1                0.961647   1                           0.963678            0.0526316       0.175439                   166.082   166.082            0.175439\n",
              "5        0.101099                    0.959238           2.66082    2.66082            1                0.960176   1                           0.96246             0.0935673       0.269006                   166.082   166.082            0.269006\n",
              "6        0.151648                    0.95194            2.66082    2.66082            1                0.954832   1                           0.959917            0.134503        0.403509                   166.082   166.082            0.403509\n",
              "7        0.2                         0.934824           2.66082    2.66082            1                0.944351   1                           0.956154            0.128655        0.532164                   166.082   166.082            0.532164\n",
              "8        0.301099                    0.781079           2.54513    2.62197            0.956522         0.887324   0.985401                    0.933043            0.25731         0.789474                   154.513   162.197            0.782431\n",
              "9        0.4                         0.354154           1.65562    2.38304            0.622222         0.566931   0.895604                    0.842521            0.163743        0.953216                   65.5621   138.304            0.886315\n",
              "10       0.501099                    0.0901423          0.289219   1.9606             0.108696         0.19796    0.736842                    0.712478            0.0292398       0.982456                   -71.0781  96.0603            0.771189\n",
              "11       0.6                         0.0496738          0.0591293  1.64717            0.0222222        0.0669556  0.619048                    0.606073            0.00584795      0.988304                   -94.0871  64.7173            0.622107\n",
              "12       0.698901                    0.0362709          0.0591293  1.42245            0.0222222        0.0437602  0.534591                    0.526501            0.00584795      0.994152                   -94.0871  42.245             0.473025\n",
              "13       0.8                         0.0304273          0.0578439  1.25               0.0217391        0.0321457  0.46978                     0.464027            0.00584795      1                          -94.2156  25                 0.320423\n",
              "14       0.907692                    0.0233827          0          1.10169            0                0.0257063  0.414044                    0.412023            0               1                          -100      10.1695            0.147887\n",
              "15       1                           0.0161675          0          1                  0                0.0195924  0.375824                    0.375799            0               1                          -100      0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                         mean       sd         cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid    cv_6_valid    cv_7_valid    cv_8_valid    cv_9_valid    cv_10_valid\n",
              "-----------------------  ---------  ---------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  -------------\n",
              "accuracy                 0.96715    0.0235589  0.934783      0.978261      0.978261      0.956522      0.934783      0.955556      1             0.955556      1             0.977778\n",
              "auc                      0.983879   0.0178793  0.960938      0.995833      0.997972      0.984615      0.975446      0.980159      1             0.948276      1             0.995556\n",
              "err                      0.0328502  0.0235589  0.0652174     0.0217391     0.0217391     0.0434783     0.0652174     0.0444444     0             0.0444444     0             0.0222222\n",
              "err_count                1.5        1.08012    3             1             1             2             3             2             0             2             0             1\n",
              "f0point5                 0.958851   0.0287485  0.948276      0.986842      0.955056      0.95          0.909091      0.952381      1             0.9375        1             0.949367\n",
              "f1                       0.951568   0.0406347  0.88          0.967742      0.971429      0.95          0.888889      0.952381      1             0.9375        1             0.967742\n",
              "f2                       0.945492   0.0586199  0.820896      0.949367      0.988372      0.95          0.869565      0.952381      1             0.9375        1             0.986842\n",
              "lift_top_group           2.7497     0.495565   3.28571       2.875         2.70588       2.3           3.28571       2.14286       1.875         2.8125        3.21429       3\n",
              "logloss                  0.166222   0.0649039  0.212998      0.147144      0.106434      0.204719      0.231138      0.199252      0.0746679     0.255789      0.0740798     0.156001\n",
              "max_per_class_error      0.0647578  0.0660564  0.214286      0.0625        0.0344828     0.05          0.142857      0.0476191     0             0.0625        0             0.0333333\n",
              "mcc                      0.927621   0.0547608  0.847566      0.952501      0.954923      0.911539      0.843984      0.910714      1             0.903017      1             0.951972\n",
              "mean_per_class_accuracy  0.960328   0.035232   0.892857      0.96875       0.982759      0.955769      0.912946      0.955357      1             0.951509      1             0.983333\n",
              "mean_per_class_error     0.0396719  0.035232   0.107143      0.03125       0.0172414     0.0442308     0.0870536     0.0446429     0             0.0484914     0             0.0166667\n",
              "mse                      0.0441377  0.0226749  0.0620417     0.0377012     0.0215032     0.0596129     0.0706184     0.0543299     0.0114468     0.0693428     0.0124989     0.0422811\n",
              "pr_auc                   0.977882   0.023669   0.938531      0.992855      0.996638      0.980331      0.952928      0.982007      1             0.944435      1             0.991098\n",
              "precision                0.96449    0.0315774  1             1             0.944444      0.95          0.923077      0.952381      1             0.9375        1             0.9375\n",
              "r2                       0.805685   0.102998   0.706964      0.833801      0.907706      0.757421      0.666454      0.78171       0.954008      0.697372      0.941681      0.809735\n",
              "recall                   0.942024   0.0709415  0.785714      0.9375        1             0.95          0.857143      0.952381      1             0.9375        1             1\n",
              "rmse                     0.202062   0.0606329  0.249082      0.194168      0.14664       0.244158      0.265741      0.233088      0.10699       0.26333       0.111799      0.205624\n",
              "specificity              0.978632   0.0186053  1             1             0.965517      0.961538      0.96875       0.958333      1             0.965517      1             0.966667\n",
              "\n",
              "Scoring History: \n",
              "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
              "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
              "    2023-06-22 20:25:14  2.312 sec   0                  0.5              0.693147            0.5             0.375824           1                0.624176                         0.5                0.693147              0.5               0.359649             1                  0.640351\n",
              "    2023-06-22 20:25:14  2.330 sec   5                  0.225101         0.220985            0.991465        0.987958           2.66082          0.0307692                        0.225428           0.222521              0.976779          0.978796             2.78049            0.0438596\n",
              "    2023-06-22 20:25:14  2.347 sec   10                 0.182175         0.144104            0.993503        0.991125           2.66082          0.0307692                        0.194942           0.160143              0.97728           0.979975             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.364 sec   15                 0.172352         0.130611            0.994533        0.992725           2.66082          0.0241758                        0.191048           0.152637              0.977113          0.980463             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.383 sec   20                 0.170296         0.1239              0.99408         0.992056           2.66082          0.0263736                        0.193492           0.150659              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.413 sec   25                 0.170296         0.1239              0.99408         0.992056           2.66082          0.0263736                        0.193495           0.150663              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.451 sec   30                 0.170295         0.123902            0.99408         0.992056           2.66082          0.0263736                        0.193487           0.150653              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.536 sec   35                 0.17029          0.123951            0.99408         0.992056           2.66082          0.0263736                        0.193367           0.150506              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.563 sec   36                 0.170513         0.123949            0.99408         0.992056           2.66082          0.0263736                        0.193961           0.151254              0.976445          0.979196             2.78049            0.0350877\n",
              "\n",
              "Variable Importances: \n",
              "variable              relative_importance    scaled_importance    percentage\n",
              "--------------------  ---------------------  -------------------  ------------\n",
              "concave points_worst  204.282                1                    0.341069\n",
              "area_worst            111.787                0.547218             0.186639\n",
              "concave points_mean   80.5573                0.394343             0.134498\n",
              "radius_worst          72.7003                0.355881             0.12138\n",
              "perimeter_worst       63.8343                0.312481             0.106577\n",
              "texture_worst         22.0758                0.108065             0.0368576\n",
              "concavity_worst       20.4532                0.100122             0.0341486\n",
              "texture_mean          10.5059                0.0514284            0.0175406\n",
              "radius_mean           8.47355                0.0414796            0.0141474\n",
              "concavity_mean        3.36487                0.0164716            0.00561796\n",
              "compactness_se        0.913376               0.00447114           0.00152497\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ],
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_9_20230622_202511\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-95.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-95 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-95 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-95 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-95 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-95 .h2o-table th,\n",
              "#h2o-table-95 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-95 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-95\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>number_of_trees</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>36.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.029074514855628383\n",
              "RMSE: 0.17051250644931704\n",
              "LogLoss: 0.12394929039565182\n",
              "Mean Per-Class Error: 0.029270653158718393\n",
              "AUC: 0.9940799769376494\n",
              "AUCPR: 0.992056368504806\n",
              "Gini: 0.9881599538752988</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-96.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-96 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-96 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-96 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-96 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-96 .h2o-table th,\n",
              "#h2o-table-96 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-96 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-96\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4925362765789032</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>279.0</td>\n",
              "<td>5.0</td>\n",
              "<td>0.0176</td>\n",
              "<td> (5.0/284.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>7.0</td>\n",
              "<td>164.0</td>\n",
              "<td>0.0409</td>\n",
              "<td> (7.0/171.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>286.0</td>\n",
              "<td>169.0</td>\n",
              "<td>0.0264</td>\n",
              "<td> (12.0/455.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-97.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-97 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-97 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-97 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-97 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-97 .h2o-table th,\n",
              "#h2o-table-97 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-97 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-97\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9647059</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.3179893</td>\n",
              "<td>0.9710983</td>\n",
              "<td>49.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.5147656</td>\n",
              "<td>0.9713945</td>\n",
              "<td>37.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.5147656</td>\n",
              "<td>0.9736264</td>\n",
              "<td>37.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0588847</td>\n",
              "<td>1.0</td>\n",
              "<td>100.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9436960</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.4028056</td>\n",
              "<td>0.9649123</td>\n",
              "<td>42.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9707293</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>284.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>110.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0148769</td>\n",
              "<td>284.0</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0588847</td>\n",
              "<td>171.0</td>\n",
              "<td>100.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.6432749</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0148769</td>\n",
              "<td>1.0</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0588847</td>\n",
              "<td>1.0</td>\n",
              "<td>100.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-98.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-98 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-98 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-98 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-98 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-98 .h2o-table th,\n",
              "#h2o-table-98 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-98 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-98\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.29 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.1340659</td>\n",
              "<td>0.9680020</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.3567251</td>\n",
              "<td>0.3567251</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.3567251</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.1626374</td>\n",
              "<td>0.9576122</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9576122</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661767</td>\n",
              "<td>0.0760234</td>\n",
              "<td>0.4327485</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.4327485</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.2</td>\n",
              "<td>0.9505008</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9534877</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9638063</td>\n",
              "<td>0.0994152</td>\n",
              "<td>0.5321637</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.5321637</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.3010989</td>\n",
              "<td>0.8075677</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.8971791</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9414351</td>\n",
              "<td>0.2690058</td>\n",
              "<td>0.8011696</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.8011696</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.4</td>\n",
              "<td>0.3147044</td>\n",
              "<td>1.8330084</td>\n",
              "<td>2.4561404</td>\n",
              "<td>0.6888889</td>\n",
              "<td>0.5823229</td>\n",
              "<td>0.9230769</td>\n",
              "<td>0.8526436</td>\n",
              "<td>0.1812865</td>\n",
              "<td>0.9824561</td>\n",
              "<td>83.3008447</td>\n",
              "<td>145.6140351</td>\n",
              "<td>0.9331604</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.5010989</td>\n",
              "<td>0.0766930</td>\n",
              "<td>0.0578439</td>\n",
              "<td>1.9722735</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.1615786</td>\n",
              "<td>0.7412281</td>\n",
              "<td>0.7132182</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9883041</td>\n",
              "<td>-94.2156115</td>\n",
              "<td>97.2273520</td>\n",
              "<td>0.7805576</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.6241758</td>\n",
              "<td>0.0503081</td>\n",
              "<td>0.0950292</td>\n",
              "<td>1.6021127</td>\n",
              "<td>0.0357143</td>\n",
              "<td>0.0583980</td>\n",
              "<td>0.6021127</td>\n",
              "<td>0.5840988</td>\n",
              "<td>0.0116959</td>\n",
              "<td>1.0</td>\n",
              "<td>-90.4970760</td>\n",
              "<td>60.2112676</td>\n",
              "<td>0.6021127</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.6989011</td>\n",
              "<td>0.0279195</td>\n",
              "<td>0.0</td>\n",
              "<td>1.4308176</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0365022</td>\n",
              "<td>0.5377358</td>\n",
              "<td>0.5255507</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>43.0817610</td>\n",
              "<td>0.4823944</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.8043956</td>\n",
              "<td>0.0177071</td>\n",
              "<td>0.0</td>\n",
              "<td>1.2431694</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0225316</td>\n",
              "<td>0.4672131</td>\n",
              "<td>0.4595810</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>24.3169399</td>\n",
              "<td>0.3133803</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.9318681</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0731132</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.4033019</td>\n",
              "<td>0.3991110</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>7.3113208</td>\n",
              "<td>0.1091549</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.3758242</td>\n",
              "<td>0.3729324</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.03762067586537018\n",
              "RMSE: 0.19396050078655236\n",
              "LogLoss: 0.1512542647982658\n",
              "Mean Per-Class Error: 0.03274306715669896\n",
              "AUC: 0.976445038422987\n",
              "AUCPR: 0.9791961628675718\n",
              "Gini: 0.952890076845974</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-99.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-99 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-99 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-99 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-99 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-99 .h2o-table th,\n",
              "#h2o-table-99 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-99 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-99\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25368359684944153</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>70.0</td>\n",
              "<td>3.0</td>\n",
              "<td>0.0411</td>\n",
              "<td> (3.0/73.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>1.0</td>\n",
              "<td>40.0</td>\n",
              "<td>0.0244</td>\n",
              "<td> (1.0/41.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>71.0</td>\n",
              "<td>43.0</td>\n",
              "<td>0.0351</td>\n",
              "<td> (4.0/114.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-100.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-100 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-100 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-100 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-100 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-100 .h2o-table th,\n",
              "#h2o-table-100 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-100 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-100\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9523810</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9661836</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.7056506</td>\n",
              "<td>0.9729730</td>\n",
              "<td>15.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9649123</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0175245</td>\n",
              "<td>1.0</td>\n",
              "<td>46.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9252854</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9589041</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9672569</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>73.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>27.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0148769</td>\n",
              "<td>73.0</td>\n",
              "<td>47.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0175245</td>\n",
              "<td>41.0</td>\n",
              "<td>46.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.6585366</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0148769</td>\n",
              "<td>1.0</td>\n",
              "<td>47.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0175245</td>\n",
              "<td>1.0</td>\n",
              "<td>46.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-101.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-101 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-101 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-101 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-101 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-101 .h2o-table th,\n",
              "#h2o-table-101 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-101 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-101\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 35.96 %, avg score: 34.85 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.1228070</td>\n",
              "<td>0.9680020</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.3414634</td>\n",
              "<td>0.3414634</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.3414634</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.1578947</td>\n",
              "<td>0.9543629</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9551752</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9651516</td>\n",
              "<td>0.0975610</td>\n",
              "<td>0.4390244</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.4390244</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.2105263</td>\n",
              "<td>0.9322835</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9354083</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9577157</td>\n",
              "<td>0.1463415</td>\n",
              "<td>0.5853659</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.5853659</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.2982456</td>\n",
              "<td>0.7172713</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.8705932</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9320915</td>\n",
              "<td>0.2439024</td>\n",
              "<td>0.8292683</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.8292683</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.4035088</td>\n",
              "<td>0.1690287</td>\n",
              "<td>1.3902439</td>\n",
              "<td>2.4178155</td>\n",
              "<td>0.5</td>\n",
              "<td>0.4511201</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.8066207</td>\n",
              "<td>0.1463415</td>\n",
              "<td>0.9756098</td>\n",
              "<td>39.0243902</td>\n",
              "<td>141.7815483</td>\n",
              "<td>0.8934180</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.5087719</td>\n",
              "<td>0.0588847</td>\n",
              "<td>0.0</td>\n",
              "<td>1.9175778</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0937150</td>\n",
              "<td>0.6896552</td>\n",
              "<td>0.6591230</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>91.7577796</td>\n",
              "<td>0.7290344</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.5964912</td>\n",
              "<td>0.0359404</td>\n",
              "<td>0.0</td>\n",
              "<td>1.6355811</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0470055</td>\n",
              "<td>0.5882353</td>\n",
              "<td>0.5691057</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>63.5581062</td>\n",
              "<td>0.5920481</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.7017544</td>\n",
              "<td>0.0235090</td>\n",
              "<td>0.0</td>\n",
              "<td>1.3902439</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0319946</td>\n",
              "<td>0.5</td>\n",
              "<td>0.4885390</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>39.0243902</td>\n",
              "<td>0.4276646</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.9736842</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.0896932</td>\n",
              "<td>1.0270270</td>\n",
              "<td>0.0322581</td>\n",
              "<td>0.0192596</td>\n",
              "<td>0.3693694</td>\n",
              "<td>0.3574790</td>\n",
              "<td>0.0243902</td>\n",
              "<td>1.0</td>\n",
              "<td>-91.0306845</td>\n",
              "<td>2.7027027</td>\n",
              "<td>0.0410959</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.3596491</td>\n",
              "<td>0.3484632</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.044205368519838424\n",
              "RMSE: 0.21025072775103162\n",
              "LogLoss: 0.16637884924436225\n",
              "Mean Per-Class Error: 0.05391854048266205\n",
              "AUC: 0.9841137468083354\n",
              "AUCPR: 0.9796363289807396\n",
              "Gini: 0.9682274936166708</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-102.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-102 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-102 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-102 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-102 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-102 .h2o-table th,\n",
              "#h2o-table-102 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-102 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-102\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35231223702430725</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>265.0</td>\n",
              "<td>19.0</td>\n",
              "<td>0.0669</td>\n",
              "<td> (19.0/284.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>7.0</td>\n",
              "<td>164.0</td>\n",
              "<td>0.0409</td>\n",
              "<td> (7.0/171.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>272.0</td>\n",
              "<td>183.0</td>\n",
              "<td>0.0571</td>\n",
              "<td> (26.0/455.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-103.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-103 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-103 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-103 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-103 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-103 .h2o-table th,\n",
              "#h2o-table-103 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-103 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-103\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9265537</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9457901</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6488545</td>\n",
              "<td>0.9533074</td>\n",
              "<td>71.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.6243538</td>\n",
              "<td>0.9428571</td>\n",
              "<td>74.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0317377</td>\n",
              "<td>1.0</td>\n",
              "<td>212.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.8812343</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3933513</td>\n",
              "<td>0.9401408</td>\n",
              "<td>98.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9460815</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9681047</td>\n",
              "<td>284.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9681047</td>\n",
              "<td>168.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0161675</td>\n",
              "<td>284.0</td>\n",
              "<td>239.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0317377</td>\n",
              "<td>171.0</td>\n",
              "<td>212.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9681047</td>\n",
              "<td>0.9824561</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0161675</td>\n",
              "<td>1.0</td>\n",
              "<td>239.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0317377</td>\n",
              "<td>1.0</td>\n",
              "<td>212.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-104.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-104 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-104 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-104 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-104 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-104 .h2o-table th,\n",
              "#h2o-table-104 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-104 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-104\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.58 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.0153846</td>\n",
              "<td>0.9646416</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661258</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661258</td>\n",
              "<td>0.0409357</td>\n",
              "<td>0.0409357</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.0409357</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.0329670</td>\n",
              "<td>0.9641652</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9641652</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9650801</td>\n",
              "<td>0.0467836</td>\n",
              "<td>0.0877193</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.0877193</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.0461538</td>\n",
              "<td>0.9632205</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9632205</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9645488</td>\n",
              "<td>0.0350877</td>\n",
              "<td>0.1228070</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.1228070</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.0659341</td>\n",
              "<td>0.9616467</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9616467</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9636782</td>\n",
              "<td>0.0526316</td>\n",
              "<td>0.1754386</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.1754386</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.1010989</td>\n",
              "<td>0.9592377</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9601760</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9624600</td>\n",
              "<td>0.0935673</td>\n",
              "<td>0.2690058</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.2690058</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1516484</td>\n",
              "<td>0.9519403</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9548322</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9599174</td>\n",
              "<td>0.1345029</td>\n",
              "<td>0.4035088</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.4035088</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.2</td>\n",
              "<td>0.9348235</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9443512</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9561542</td>\n",
              "<td>0.1286550</td>\n",
              "<td>0.5321637</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.5321637</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.3010989</td>\n",
              "<td>0.7810790</td>\n",
              "<td>2.5451309</td>\n",
              "<td>2.6219746</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.8873240</td>\n",
              "<td>0.9854015</td>\n",
              "<td>0.9330433</td>\n",
              "<td>0.2573099</td>\n",
              "<td>0.7894737</td>\n",
              "<td>154.5130943</td>\n",
              "<td>162.1974645</td>\n",
              "<td>0.7824314</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.4</td>\n",
              "<td>0.3541536</td>\n",
              "<td>1.6556205</td>\n",
              "<td>2.3830409</td>\n",
              "<td>0.6222222</td>\n",
              "<td>0.5669310</td>\n",
              "<td>0.8956044</td>\n",
              "<td>0.8425210</td>\n",
              "<td>0.1637427</td>\n",
              "<td>0.9532164</td>\n",
              "<td>65.5620533</td>\n",
              "<td>138.3040936</td>\n",
              "<td>0.8863150</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.5010989</td>\n",
              "<td>0.0901423</td>\n",
              "<td>0.2892194</td>\n",
              "<td>1.9606033</td>\n",
              "<td>0.1086957</td>\n",
              "<td>0.1979604</td>\n",
              "<td>0.7368421</td>\n",
              "<td>0.7124781</td>\n",
              "<td>0.0292398</td>\n",
              "<td>0.9824561</td>\n",
              "<td>-71.0780575</td>\n",
              "<td>96.0603263</td>\n",
              "<td>0.7711885</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.6</td>\n",
              "<td>0.0496738</td>\n",
              "<td>0.0591293</td>\n",
              "<td>1.6471735</td>\n",
              "<td>0.0222222</td>\n",
              "<td>0.0669556</td>\n",
              "<td>0.6190476</td>\n",
              "<td>0.6060733</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9883041</td>\n",
              "<td>-94.0870695</td>\n",
              "<td>64.7173489</td>\n",
              "<td>0.6221069</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6989011</td>\n",
              "<td>0.0362709</td>\n",
              "<td>0.0591293</td>\n",
              "<td>1.4224503</td>\n",
              "<td>0.0222222</td>\n",
              "<td>0.0437602</td>\n",
              "<td>0.5345912</td>\n",
              "<td>0.5265007</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9941520</td>\n",
              "<td>-94.0870695</td>\n",
              "<td>42.2450256</td>\n",
              "<td>0.4730253</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0304273</td>\n",
              "<td>0.0578439</td>\n",
              "<td>1.25</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0321457</td>\n",
              "<td>0.4697802</td>\n",
              "<td>0.4640272</td>\n",
              "<td>0.0058480</td>\n",
              "<td>1.0</td>\n",
              "<td>-94.2156115</td>\n",
              "<td>25.0</td>\n",
              "<td>0.3204225</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.9076923</td>\n",
              "<td>0.0233827</td>\n",
              "<td>0.0</td>\n",
              "<td>1.1016949</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0257063</td>\n",
              "<td>0.4140436</td>\n",
              "<td>0.4120231</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>10.1694915</td>\n",
              "<td>0.1478873</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0161675</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0195924</td>\n",
              "<td>0.3758242</td>\n",
              "<td>0.3757987</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-105.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-105 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-105 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-105 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-105 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-105 .h2o-table th,\n",
              "#h2o-table-105 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-105 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-105\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th>\n",
              "<th>cv_6_valid</th>\n",
              "<th>cv_7_valid</th>\n",
              "<th>cv_8_valid</th>\n",
              "<th>cv_9_valid</th>\n",
              "<th>cv_10_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.9671497</td>\n",
              "<td>0.0235589</td>\n",
              "<td>0.9347826</td>\n",
              "<td>0.9782609</td>\n",
              "<td>0.9782609</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.9347826</td>\n",
              "<td>0.9555556</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9555556</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9777778</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.9838794</td>\n",
              "<td>0.0178793</td>\n",
              "<td>0.9609375</td>\n",
              "<td>0.9958333</td>\n",
              "<td>0.9979716</td>\n",
              "<td>0.9846154</td>\n",
              "<td>0.9754464</td>\n",
              "<td>0.9801587</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9482759</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9955556</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.0328502</td>\n",
              "<td>0.0235589</td>\n",
              "<td>0.0652174</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0434783</td>\n",
              "<td>0.0652174</td>\n",
              "<td>0.0444444</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0444444</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0222222</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>1.5</td>\n",
              "<td>1.0801234</td>\n",
              "<td>3.0</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>2.0</td>\n",
              "<td>3.0</td>\n",
              "<td>2.0</td>\n",
              "<td>0.0</td>\n",
              "<td>2.0</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.9588513</td>\n",
              "<td>0.0287485</td>\n",
              "<td>0.9482759</td>\n",
              "<td>0.9868421</td>\n",
              "<td>0.9550562</td>\n",
              "<td>0.95</td>\n",
              "<td>0.9090909</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9493671</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.9515683</td>\n",
              "<td>0.0406347</td>\n",
              "<td>0.88</td>\n",
              "<td>0.9677419</td>\n",
              "<td>0.9714286</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8888889</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9677419</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.9454923</td>\n",
              "<td>0.0586199</td>\n",
              "<td>0.8208955</td>\n",
              "<td>0.9493671</td>\n",
              "<td>0.9883721</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9868421</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.7496953</td>\n",
              "<td>0.4955653</td>\n",
              "<td>3.2857144</td>\n",
              "<td>2.875</td>\n",
              "<td>2.7058823</td>\n",
              "<td>2.3</td>\n",
              "<td>3.2857144</td>\n",
              "<td>2.142857</td>\n",
              "<td>1.875</td>\n",
              "<td>2.8125</td>\n",
              "<td>3.2142856</td>\n",
              "<td>3.0</td></tr>\n",
              "<tr><td>logloss</td>\n",
              "<td>0.1662221</td>\n",
              "<td>0.0649039</td>\n",
              "<td>0.2129980</td>\n",
              "<td>0.1471438</td>\n",
              "<td>0.1064336</td>\n",
              "<td>0.2047186</td>\n",
              "<td>0.2311378</td>\n",
              "<td>0.199252</td>\n",
              "<td>0.0746679</td>\n",
              "<td>0.2557887</td>\n",
              "<td>0.0740798</td>\n",
              "<td>0.1560008</td></tr>\n",
              "<tr><td>max_per_class_error</td>\n",
              "<td>0.0647578</td>\n",
              "<td>0.0660564</td>\n",
              "<td>0.2142857</td>\n",
              "<td>0.0625</td>\n",
              "<td>0.0344828</td>\n",
              "<td>0.05</td>\n",
              "<td>0.1428571</td>\n",
              "<td>0.0476191</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0625</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0333333</td></tr>\n",
              "<tr><td>mcc</td>\n",
              "<td>0.9276215</td>\n",
              "<td>0.0547608</td>\n",
              "<td>0.8475655</td>\n",
              "<td>0.9525009</td>\n",
              "<td>0.9549227</td>\n",
              "<td>0.9115385</td>\n",
              "<td>0.8439837</td>\n",
              "<td>0.9107143</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9030172</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9519716</td></tr>\n",
              "<tr><td>mean_per_class_accuracy</td>\n",
              "<td>0.9603280</td>\n",
              "<td>0.0352320</td>\n",
              "<td>0.8928571</td>\n",
              "<td>0.96875</td>\n",
              "<td>0.9827586</td>\n",
              "<td>0.9557692</td>\n",
              "<td>0.9129464</td>\n",
              "<td>0.9553571</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9515086</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9833333</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.0396719</td>\n",
              "<td>0.0352320</td>\n",
              "<td>0.1071429</td>\n",
              "<td>0.03125</td>\n",
              "<td>0.0172414</td>\n",
              "<td>0.0442308</td>\n",
              "<td>0.0870536</td>\n",
              "<td>0.0446429</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0484914</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0166667</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.0441377</td>\n",
              "<td>0.0226749</td>\n",
              "<td>0.0620417</td>\n",
              "<td>0.0377012</td>\n",
              "<td>0.0215032</td>\n",
              "<td>0.0596129</td>\n",
              "<td>0.0706184</td>\n",
              "<td>0.0543299</td>\n",
              "<td>0.0114468</td>\n",
              "<td>0.0693428</td>\n",
              "<td>0.0124989</td>\n",
              "<td>0.0422811</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.9778822</td>\n",
              "<td>0.0236690</td>\n",
              "<td>0.9385305</td>\n",
              "<td>0.9928552</td>\n",
              "<td>0.9966378</td>\n",
              "<td>0.9803309</td>\n",
              "<td>0.9529279</td>\n",
              "<td>0.9820066</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9444352</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9910979</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.9644902</td>\n",
              "<td>0.0315774</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9444444</td>\n",
              "<td>0.95</td>\n",
              "<td>0.9230769</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.8056853</td>\n",
              "<td>0.1029978</td>\n",
              "<td>0.7069637</td>\n",
              "<td>0.8338005</td>\n",
              "<td>0.9077061</td>\n",
              "<td>0.7574214</td>\n",
              "<td>0.6664543</td>\n",
              "<td>0.7817103</td>\n",
              "<td>0.9540083</td>\n",
              "<td>0.6973723</td>\n",
              "<td>0.9416813</td>\n",
              "<td>0.8097349</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.9420238</td>\n",
              "<td>0.0709415</td>\n",
              "<td>0.7857143</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8571429</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.2020618</td>\n",
              "<td>0.0606329</td>\n",
              "<td>0.2490817</td>\n",
              "<td>0.1941680</td>\n",
              "<td>0.1466398</td>\n",
              "<td>0.2441575</td>\n",
              "<td>0.2657412</td>\n",
              "<td>0.2330877</td>\n",
              "<td>0.1069898</td>\n",
              "<td>0.2633303</td>\n",
              "<td>0.1117986</td>\n",
              "<td>0.2056237</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.9786323</td>\n",
              "<td>0.0186053</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9655172</td>\n",
              "<td>0.9615384</td>\n",
              "<td>0.96875</td>\n",
              "<td>0.9583333</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9655172</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9666666</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-106.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-106 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-106 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-106 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-106 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-106 .h2o-table th,\n",
              "#h2o-table-106 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-106 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-106\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Scoring History: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>timestamp</th>\n",
              "<th>duration</th>\n",
              "<th>number_of_trees</th>\n",
              "<th>training_rmse</th>\n",
              "<th>training_logloss</th>\n",
              "<th>training_auc</th>\n",
              "<th>training_pr_auc</th>\n",
              "<th>training_lift</th>\n",
              "<th>training_classification_error</th>\n",
              "<th>validation_rmse</th>\n",
              "<th>validation_logloss</th>\n",
              "<th>validation_auc</th>\n",
              "<th>validation_pr_auc</th>\n",
              "<th>validation_lift</th>\n",
              "<th>validation_classification_error</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.312 sec</td>\n",
              "<td>0.0</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3758242</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6241758</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3596491</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6403509</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.330 sec</td>\n",
              "<td>5.0</td>\n",
              "<td>0.2251009</td>\n",
              "<td>0.2209849</td>\n",
              "<td>0.9914649</td>\n",
              "<td>0.9879580</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0307692</td>\n",
              "<td>0.2254283</td>\n",
              "<td>0.2225214</td>\n",
              "<td>0.9767792</td>\n",
              "<td>0.9787964</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0438596</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.347 sec</td>\n",
              "<td>10.0</td>\n",
              "<td>0.1821748</td>\n",
              "<td>0.1441039</td>\n",
              "<td>0.9935034</td>\n",
              "<td>0.9911247</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0307692</td>\n",
              "<td>0.1949417</td>\n",
              "<td>0.1601430</td>\n",
              "<td>0.9772803</td>\n",
              "<td>0.9799751</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.364 sec</td>\n",
              "<td>15.0</td>\n",
              "<td>0.1723523</td>\n",
              "<td>0.1306105</td>\n",
              "<td>0.9945330</td>\n",
              "<td>0.9927252</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0241758</td>\n",
              "<td>0.1910482</td>\n",
              "<td>0.1526369</td>\n",
              "<td>0.9771133</td>\n",
              "<td>0.9804633</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.383 sec</td>\n",
              "<td>20.0</td>\n",
              "<td>0.1702960</td>\n",
              "<td>0.1239005</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934925</td>\n",
              "<td>0.1506594</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.413 sec</td>\n",
              "<td>25.0</td>\n",
              "<td>0.1702965</td>\n",
              "<td>0.1238999</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934951</td>\n",
              "<td>0.1506626</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.451 sec</td>\n",
              "<td>30.0</td>\n",
              "<td>0.1702950</td>\n",
              "<td>0.1239018</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934870</td>\n",
              "<td>0.1506526</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.536 sec</td>\n",
              "<td>35.0</td>\n",
              "<td>0.1702896</td>\n",
              "<td>0.1239514</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1933670</td>\n",
              "<td>0.1505057</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.563 sec</td>\n",
              "<td>36.0</td>\n",
              "<td>0.1705125</td>\n",
              "<td>0.1239493</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1939605</td>\n",
              "<td>0.1512543</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-107.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-107 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-107 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-107 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-107 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-107 .h2o-table th,\n",
              "#h2o-table-107 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-107 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-107\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Variable Importances: </caption>\n",
              "    <thead><tr><th>variable</th>\n",
              "<th>relative_importance</th>\n",
              "<th>scaled_importance</th>\n",
              "<th>percentage</th></tr></thead>\n",
              "    <tbody><tr><td>concave points_worst</td>\n",
              "<td>204.2824097</td>\n",
              "<td>1.0</td>\n",
              "<td>0.3410687</td></tr>\n",
              "<tr><td>area_worst</td>\n",
              "<td>111.7869415</td>\n",
              "<td>0.5472177</td>\n",
              "<td>0.1866388</td></tr>\n",
              "<tr><td>concave points_mean</td>\n",
              "<td>80.5573425</td>\n",
              "<td>0.3943430</td>\n",
              "<td>0.1344981</td></tr>\n",
              "<tr><td>radius_worst</td>\n",
              "<td>72.7002869</td>\n",
              "<td>0.3558813</td>\n",
              "<td>0.1213800</td></tr>\n",
              "<tr><td>perimeter_worst</td>\n",
              "<td>63.8343277</td>\n",
              "<td>0.3124808</td>\n",
              "<td>0.1065774</td></tr>\n",
              "<tr><td>texture_worst</td>\n",
              "<td>22.0757656</td>\n",
              "<td>0.1080649</td>\n",
              "<td>0.0368576</td></tr>\n",
              "<tr><td>concavity_worst</td>\n",
              "<td>20.4532471</td>\n",
              "<td>0.1001224</td>\n",
              "<td>0.0341486</td></tr>\n",
              "<tr><td>texture_mean</td>\n",
              "<td>10.5059090</td>\n",
              "<td>0.0514284</td>\n",
              "<td>0.0175406</td></tr>\n",
              "<tr><td>radius_mean</td>\n",
              "<td>8.4735489</td>\n",
              "<td>0.0414796</td>\n",
              "<td>0.0141474</td></tr>\n",
              "<tr><td>concavity_mean</td>\n",
              "<td>3.3648682</td>\n",
              "<td>0.0164716</td>\n",
              "<td>0.0056180</td></tr>\n",
              "<tr><td>compactness_se</td>\n",
              "<td>0.9133759</td>\n",
              "<td>0.0044711</td>\n",
              "<td>0.0015250</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb = aml.leaderboard\n",
        "lb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "gmCXw_D_4y1D",
        "outputId": "77d78fba-1a50-416d-f815-4be0edb689ee"
      },
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_id                                                   accuracy       auc    logloss     aucpr    mean_per_class_error      rmse        mse\n",
              "-------------------------------------------------------  ----------  --------  ---------  --------  ----------------------  --------  ---------\n",
              "XGBoost_1_AutoML_9_20230622_202511                         0.942857  0.984114  0.166379   0.979636               0.0539185  0.210251  0.0442054\n",
              "XRT_1_AutoML_9_20230622_202511                             0.96044   0.991619  0.116734   0.98782                0.0421609  0.184096  0.0338912\n",
              "DRF_1_AutoML_9_20230622_202511                             0.96044   0.987058  0.249487   0.985869               0.0421609  0.18258   0.0333355\n",
              "XGBoost_2_AutoML_9_20230622_202511                         0.962637  0.990394  0.11331    0.987678               0.0392369  0.174879  0.0305826\n",
              "GBM_4_AutoML_9_20230622_202511                             0.967033  0.993246  0.0998609  0.990397               0.0333889  0.170342  0.0290164\n",
              "GBM_2_AutoML_9_20230622_202511                             0.967033  0.994255  0.0933299  0.991681               0.0380426  0.162663  0.0264594\n",
              "XGBoost_3_AutoML_9_20230622_202511                         0.969231  0.992999  0.0965668  0.990609               0.0316284  0.161568  0.0261043\n",
              "GBM_3_AutoML_9_20230622_202511                             0.971429  0.994461  0.0902413  0.991937               0.027541   0.157908  0.0249349\n",
              "GBM_1_AutoML_9_20230622_202511                             0.971429  0.994461  0.0911605  0.992401               0.0287044  0.158892  0.0252468\n",
              "StackedEnsemble_BestOfFamily_1_AutoML_9_20230622_202511    0.978022  0.996541  0.0696293  0.995004               0.0292398  0.140822  0.0198309\n",
              "[10 rows x 8 columns]\n"
            ],
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th>model_id                                               </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">      mse</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>XGBoost_1_AutoML_9_20230622_202511                     </td><td style=\"text-align: right;\">  0.942857</td><td style=\"text-align: right;\">0.984114</td><td style=\"text-align: right;\">0.166379 </td><td style=\"text-align: right;\">0.979636</td><td style=\"text-align: right;\">             0.0539185</td><td style=\"text-align: right;\">0.210251</td><td style=\"text-align: right;\">0.0442054</td></tr>\n",
              "<tr><td>XRT_1_AutoML_9_20230622_202511                         </td><td style=\"text-align: right;\">  0.96044 </td><td style=\"text-align: right;\">0.991619</td><td style=\"text-align: right;\">0.116734 </td><td style=\"text-align: right;\">0.98782 </td><td style=\"text-align: right;\">             0.0421609</td><td style=\"text-align: right;\">0.184096</td><td style=\"text-align: right;\">0.0338912</td></tr>\n",
              "<tr><td>DRF_1_AutoML_9_20230622_202511                         </td><td style=\"text-align: right;\">  0.96044 </td><td style=\"text-align: right;\">0.987058</td><td style=\"text-align: right;\">0.249487 </td><td style=\"text-align: right;\">0.985869</td><td style=\"text-align: right;\">             0.0421609</td><td style=\"text-align: right;\">0.18258 </td><td style=\"text-align: right;\">0.0333355</td></tr>\n",
              "<tr><td>XGBoost_2_AutoML_9_20230622_202511                     </td><td style=\"text-align: right;\">  0.962637</td><td style=\"text-align: right;\">0.990394</td><td style=\"text-align: right;\">0.11331  </td><td style=\"text-align: right;\">0.987678</td><td style=\"text-align: right;\">             0.0392369</td><td style=\"text-align: right;\">0.174879</td><td style=\"text-align: right;\">0.0305826</td></tr>\n",
              "<tr><td>GBM_4_AutoML_9_20230622_202511                         </td><td style=\"text-align: right;\">  0.967033</td><td style=\"text-align: right;\">0.993246</td><td style=\"text-align: right;\">0.0998609</td><td style=\"text-align: right;\">0.990397</td><td style=\"text-align: right;\">             0.0333889</td><td style=\"text-align: right;\">0.170342</td><td style=\"text-align: right;\">0.0290164</td></tr>\n",
              "<tr><td>GBM_2_AutoML_9_20230622_202511                         </td><td style=\"text-align: right;\">  0.967033</td><td style=\"text-align: right;\">0.994255</td><td style=\"text-align: right;\">0.0933299</td><td style=\"text-align: right;\">0.991681</td><td style=\"text-align: right;\">             0.0380426</td><td style=\"text-align: right;\">0.162663</td><td style=\"text-align: right;\">0.0264594</td></tr>\n",
              "<tr><td>XGBoost_3_AutoML_9_20230622_202511                     </td><td style=\"text-align: right;\">  0.969231</td><td style=\"text-align: right;\">0.992999</td><td style=\"text-align: right;\">0.0965668</td><td style=\"text-align: right;\">0.990609</td><td style=\"text-align: right;\">             0.0316284</td><td style=\"text-align: right;\">0.161568</td><td style=\"text-align: right;\">0.0261043</td></tr>\n",
              "<tr><td>GBM_3_AutoML_9_20230622_202511                         </td><td style=\"text-align: right;\">  0.971429</td><td style=\"text-align: right;\">0.994461</td><td style=\"text-align: right;\">0.0902413</td><td style=\"text-align: right;\">0.991937</td><td style=\"text-align: right;\">             0.027541 </td><td style=\"text-align: right;\">0.157908</td><td style=\"text-align: right;\">0.0249349</td></tr>\n",
              "<tr><td>GBM_1_AutoML_9_20230622_202511                         </td><td style=\"text-align: right;\">  0.971429</td><td style=\"text-align: right;\">0.994461</td><td style=\"text-align: right;\">0.0911605</td><td style=\"text-align: right;\">0.992401</td><td style=\"text-align: right;\">             0.0287044</td><td style=\"text-align: right;\">0.158892</td><td style=\"text-align: right;\">0.0252468</td></tr>\n",
              "<tr><td>StackedEnsemble_BestOfFamily_1_AutoML_9_20230622_202511</td><td style=\"text-align: right;\">  0.978022</td><td style=\"text-align: right;\">0.996541</td><td style=\"text-align: right;\">0.0696293</td><td style=\"text-align: right;\">0.995004</td><td style=\"text-align: right;\">             0.0292398</td><td style=\"text-align: right;\">0.140822</td><td style=\"text-align: right;\">0.0198309</td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10 rows x 8 columns]</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 398
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "best_model"
      ],
      "metadata": {
        "id": "vlH8zwtisQU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e1684e5-c71e-45de-9688-bacdf6b288b3"
      },
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_9_20230622_202511\n",
              "\n",
              "\n",
              "Model Summary: \n",
              "    number_of_trees\n",
              "--  -----------------\n",
              "    36\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.029074514855628383\n",
              "RMSE: 0.17051250644931704\n",
              "LogLoss: 0.12394929039565182\n",
              "Mean Per-Class Error: 0.029270653158718393\n",
              "AUC: 0.9940799769376494\n",
              "AUCPR: 0.992056368504806\n",
              "Gini: 0.9881599538752988\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4925362765789032\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  ------------\n",
              "0      279  5    0.0176   (5.0/284.0)\n",
              "1      7    164  0.0409   (7.0/171.0)\n",
              "Total  286  169  0.0264   (12.0/455.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.492536     0.964706  38\n",
              "max f2                       0.317989     0.971098  49\n",
              "max f0point5                 0.514766     0.971395  37\n",
              "max accuracy                 0.514766     0.973626  37\n",
              "max precision                0.968002     1         0\n",
              "max recall                   0.0588847    1         100\n",
              "max specificity              0.968002     1         0\n",
              "max absolute_mcc             0.492536     0.943696  38\n",
              "max min_per_class_accuracy   0.402806     0.964912  42\n",
              "max mean_per_class_accuracy  0.492536     0.970729  38\n",
              "max tns                      0.968002     284       0\n",
              "max fns                      0.968002     110       0\n",
              "max fps                      0.0148769    284       134\n",
              "max tps                      0.0588847    171       100\n",
              "max tnr                      0.968002     1         0\n",
              "max fnr                      0.968002     0.643275  0\n",
              "max fpr                      0.0148769    1         134\n",
              "max tpr                      0.0588847    1         100\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.29 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.134066                    0.968002           2.66082    2.66082            1                0.968002   1                           0.968002            0.356725        0.356725                   166.082   166.082            0.356725\n",
              "2        0.162637                    0.957612           2.66082    2.66082            1                0.957612   1                           0.966177            0.0760234       0.432749                   166.082   166.082            0.432749\n",
              "3        0.2                         0.950501           2.66082    2.66082            1                0.953488   1                           0.963806            0.0994152       0.532164                   166.082   166.082            0.532164\n",
              "4        0.301099                    0.807568           2.66082    2.66082            1                0.897179   1                           0.941435            0.269006        0.80117                    166.082   166.082            0.80117\n",
              "5        0.4                         0.314704           1.83301    2.45614            0.688889         0.582323   0.923077                    0.852644            0.181287        0.982456                   83.3008   145.614            0.93316\n",
              "6        0.501099                    0.076693           0.0578439  1.97227            0.0217391        0.161579   0.741228                    0.713218            0.00584795      0.988304                   -94.2156  97.2274            0.780558\n",
              "7        0.624176                    0.0503081          0.0950292  1.60211            0.0357143        0.058398   0.602113                    0.584099            0.0116959       1                          -90.4971  60.2113            0.602113\n",
              "8        0.698901                    0.0279195          0          1.43082            0                0.0365022  0.537736                    0.525551            0               1                          -100      43.0818            0.482394\n",
              "9        0.804396                    0.0177071          0          1.24317            0                0.0225316  0.467213                    0.459581            0               1                          -100      24.3169            0.31338\n",
              "10       0.931868                    0.0175245          0          1.07311            0                0.0175245  0.403302                    0.399111            0               1                          -100      7.31132            0.109155\n",
              "11       1                           0.0148769          0          1                  0                0.0148769  0.375824                    0.372932            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.03762067586537018\n",
              "RMSE: 0.19396050078655236\n",
              "LogLoss: 0.1512542647982658\n",
              "Mean Per-Class Error: 0.03274306715669896\n",
              "AUC: 0.976445038422987\n",
              "AUCPR: 0.9791961628675718\n",
              "Gini: 0.952890076845974\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25368359684944153\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  -----------\n",
              "0      70   3    0.0411   (3.0/73.0)\n",
              "1      1    40   0.0244   (1.0/41.0)\n",
              "Total  71   43   0.0351   (4.0/114.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.253684     0.952381  20\n",
              "max f2                       0.253684     0.966184  20\n",
              "max f0point5                 0.705651     0.972973  15\n",
              "max accuracy                 0.253684     0.964912  20\n",
              "max precision                0.968002     1         0\n",
              "max recall                   0.0175245    1         46\n",
              "max specificity              0.968002     1         0\n",
              "max absolute_mcc             0.253684     0.925285  20\n",
              "max min_per_class_accuracy   0.253684     0.958904  20\n",
              "max mean_per_class_accuracy  0.253684     0.967257  20\n",
              "max tns                      0.968002     73        0\n",
              "max fns                      0.968002     27        0\n",
              "max fps                      0.0148769    73        47\n",
              "max tps                      0.0175245    41        46\n",
              "max tnr                      0.968002     1         0\n",
              "max fnr                      0.968002     0.658537  0\n",
              "max fpr                      0.0148769    1         47\n",
              "max tpr                      0.0175245    1         46\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 35.96 %, avg score: 34.85 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.122807                    0.968002           2.78049    2.78049            1                0.968002   1                           0.968002            0.341463        0.341463                   178.049   178.049            0.341463\n",
              "2        0.157895                    0.954363           2.78049    2.78049            1                0.955175   1                           0.965152            0.097561        0.439024                   178.049   178.049            0.439024\n",
              "3        0.210526                    0.932284           2.78049    2.78049            1                0.935408   1                           0.957716            0.146341        0.585366                   178.049   178.049            0.585366\n",
              "4        0.298246                    0.717271           2.78049    2.78049            1                0.870593   1                           0.932091            0.243902        0.829268                   178.049   178.049            0.829268\n",
              "5        0.403509                    0.169029           1.39024    2.41782            0.5              0.45112    0.869565                    0.806621            0.146341        0.97561                    39.0244   141.782            0.893418\n",
              "6        0.508772                    0.0588847          0          1.91758            0                0.093715   0.689655                    0.659123            0               0.97561                    -100      91.7578            0.729034\n",
              "7        0.596491                    0.0359404          0          1.63558            0                0.0470055  0.588235                    0.569106            0               0.97561                    -100      63.5581            0.592048\n",
              "8        0.701754                    0.023509           0          1.39024            0                0.0319946  0.5                         0.488539            0               0.97561                    -100      39.0244            0.427665\n",
              "9        0.973684                    0.0175245          0.0896932  1.02703            0.0322581        0.0192596  0.369369                    0.357479            0.0243902       1                          -91.0307  2.7027             0.0410959\n",
              "10       1                           0.0148769          0          1                  0                0.0148769  0.359649                    0.348463            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.044205368519838424\n",
              "RMSE: 0.21025072775103162\n",
              "LogLoss: 0.16637884924436225\n",
              "Mean Per-Class Error: 0.05391854048266205\n",
              "AUC: 0.9841137468083354\n",
              "AUCPR: 0.9796363289807396\n",
              "Gini: 0.9682274936166708\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35231223702430725\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  ------------\n",
              "0      265  19   0.0669   (19.0/284.0)\n",
              "1      7    164  0.0409   (7.0/171.0)\n",
              "Total  272  183  0.0571   (26.0/455.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.352312     0.926554  103\n",
              "max f2                       0.352312     0.94579   103\n",
              "max f0point5                 0.648854     0.953307  71\n",
              "max accuracy                 0.624354     0.942857  74\n",
              "max precision                0.968105     1         0\n",
              "max recall                   0.0317377    1         212\n",
              "max specificity              0.968105     1         0\n",
              "max absolute_mcc             0.352312     0.881234  103\n",
              "max min_per_class_accuracy   0.393351     0.940141  98\n",
              "max mean_per_class_accuracy  0.352312     0.946081  103\n",
              "max tns                      0.968105     284       0\n",
              "max fns                      0.968105     168       0\n",
              "max fps                      0.0161675    284       239\n",
              "max tps                      0.0317377    171       212\n",
              "max tnr                      0.968105     1         0\n",
              "max fnr                      0.968105     0.982456  0\n",
              "max fpr                      0.0161675    1         239\n",
              "max tpr                      0.0317377    1         212\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.58 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.0153846                   0.964642           2.66082    2.66082            1                0.966126   1                           0.966126            0.0409357       0.0409357                  166.082   166.082            0.0409357\n",
              "2        0.032967                    0.964165           2.66082    2.66082            1                0.964165   1                           0.96508             0.0467836       0.0877193                  166.082   166.082            0.0877193\n",
              "3        0.0461538                   0.96322            2.66082    2.66082            1                0.96322    1                           0.964549            0.0350877       0.122807                   166.082   166.082            0.122807\n",
              "4        0.0659341                   0.961647           2.66082    2.66082            1                0.961647   1                           0.963678            0.0526316       0.175439                   166.082   166.082            0.175439\n",
              "5        0.101099                    0.959238           2.66082    2.66082            1                0.960176   1                           0.96246             0.0935673       0.269006                   166.082   166.082            0.269006\n",
              "6        0.151648                    0.95194            2.66082    2.66082            1                0.954832   1                           0.959917            0.134503        0.403509                   166.082   166.082            0.403509\n",
              "7        0.2                         0.934824           2.66082    2.66082            1                0.944351   1                           0.956154            0.128655        0.532164                   166.082   166.082            0.532164\n",
              "8        0.301099                    0.781079           2.54513    2.62197            0.956522         0.887324   0.985401                    0.933043            0.25731         0.789474                   154.513   162.197            0.782431\n",
              "9        0.4                         0.354154           1.65562    2.38304            0.622222         0.566931   0.895604                    0.842521            0.163743        0.953216                   65.5621   138.304            0.886315\n",
              "10       0.501099                    0.0901423          0.289219   1.9606             0.108696         0.19796    0.736842                    0.712478            0.0292398       0.982456                   -71.0781  96.0603            0.771189\n",
              "11       0.6                         0.0496738          0.0591293  1.64717            0.0222222        0.0669556  0.619048                    0.606073            0.00584795      0.988304                   -94.0871  64.7173            0.622107\n",
              "12       0.698901                    0.0362709          0.0591293  1.42245            0.0222222        0.0437602  0.534591                    0.526501            0.00584795      0.994152                   -94.0871  42.245             0.473025\n",
              "13       0.8                         0.0304273          0.0578439  1.25               0.0217391        0.0321457  0.46978                     0.464027            0.00584795      1                          -94.2156  25                 0.320423\n",
              "14       0.907692                    0.0233827          0          1.10169            0                0.0257063  0.414044                    0.412023            0               1                          -100      10.1695            0.147887\n",
              "15       1                           0.0161675          0          1                  0                0.0195924  0.375824                    0.375799            0               1                          -100      0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                         mean       sd         cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid    cv_6_valid    cv_7_valid    cv_8_valid    cv_9_valid    cv_10_valid\n",
              "-----------------------  ---------  ---------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  -------------\n",
              "accuracy                 0.96715    0.0235589  0.934783      0.978261      0.978261      0.956522      0.934783      0.955556      1             0.955556      1             0.977778\n",
              "auc                      0.983879   0.0178793  0.960938      0.995833      0.997972      0.984615      0.975446      0.980159      1             0.948276      1             0.995556\n",
              "err                      0.0328502  0.0235589  0.0652174     0.0217391     0.0217391     0.0434783     0.0652174     0.0444444     0             0.0444444     0             0.0222222\n",
              "err_count                1.5        1.08012    3             1             1             2             3             2             0             2             0             1\n",
              "f0point5                 0.958851   0.0287485  0.948276      0.986842      0.955056      0.95          0.909091      0.952381      1             0.9375        1             0.949367\n",
              "f1                       0.951568   0.0406347  0.88          0.967742      0.971429      0.95          0.888889      0.952381      1             0.9375        1             0.967742\n",
              "f2                       0.945492   0.0586199  0.820896      0.949367      0.988372      0.95          0.869565      0.952381      1             0.9375        1             0.986842\n",
              "lift_top_group           2.7497     0.495565   3.28571       2.875         2.70588       2.3           3.28571       2.14286       1.875         2.8125        3.21429       3\n",
              "logloss                  0.166222   0.0649039  0.212998      0.147144      0.106434      0.204719      0.231138      0.199252      0.0746679     0.255789      0.0740798     0.156001\n",
              "max_per_class_error      0.0647578  0.0660564  0.214286      0.0625        0.0344828     0.05          0.142857      0.0476191     0             0.0625        0             0.0333333\n",
              "mcc                      0.927621   0.0547608  0.847566      0.952501      0.954923      0.911539      0.843984      0.910714      1             0.903017      1             0.951972\n",
              "mean_per_class_accuracy  0.960328   0.035232   0.892857      0.96875       0.982759      0.955769      0.912946      0.955357      1             0.951509      1             0.983333\n",
              "mean_per_class_error     0.0396719  0.035232   0.107143      0.03125       0.0172414     0.0442308     0.0870536     0.0446429     0             0.0484914     0             0.0166667\n",
              "mse                      0.0441377  0.0226749  0.0620417     0.0377012     0.0215032     0.0596129     0.0706184     0.0543299     0.0114468     0.0693428     0.0124989     0.0422811\n",
              "pr_auc                   0.977882   0.023669   0.938531      0.992855      0.996638      0.980331      0.952928      0.982007      1             0.944435      1             0.991098\n",
              "precision                0.96449    0.0315774  1             1             0.944444      0.95          0.923077      0.952381      1             0.9375        1             0.9375\n",
              "r2                       0.805685   0.102998   0.706964      0.833801      0.907706      0.757421      0.666454      0.78171       0.954008      0.697372      0.941681      0.809735\n",
              "recall                   0.942024   0.0709415  0.785714      0.9375        1             0.95          0.857143      0.952381      1             0.9375        1             1\n",
              "rmse                     0.202062   0.0606329  0.249082      0.194168      0.14664       0.244158      0.265741      0.233088      0.10699       0.26333       0.111799      0.205624\n",
              "specificity              0.978632   0.0186053  1             1             0.965517      0.961538      0.96875       0.958333      1             0.965517      1             0.966667\n",
              "\n",
              "Scoring History: \n",
              "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
              "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
              "    2023-06-22 20:25:14  2.312 sec   0                  0.5              0.693147            0.5             0.375824           1                0.624176                         0.5                0.693147              0.5               0.359649             1                  0.640351\n",
              "    2023-06-22 20:25:14  2.330 sec   5                  0.225101         0.220985            0.991465        0.987958           2.66082          0.0307692                        0.225428           0.222521              0.976779          0.978796             2.78049            0.0438596\n",
              "    2023-06-22 20:25:14  2.347 sec   10                 0.182175         0.144104            0.993503        0.991125           2.66082          0.0307692                        0.194942           0.160143              0.97728           0.979975             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.364 sec   15                 0.172352         0.130611            0.994533        0.992725           2.66082          0.0241758                        0.191048           0.152637              0.977113          0.980463             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.383 sec   20                 0.170296         0.1239              0.99408         0.992056           2.66082          0.0263736                        0.193492           0.150659              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.413 sec   25                 0.170296         0.1239              0.99408         0.992056           2.66082          0.0263736                        0.193495           0.150663              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.451 sec   30                 0.170295         0.123902            0.99408         0.992056           2.66082          0.0263736                        0.193487           0.150653              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.536 sec   35                 0.17029          0.123951            0.99408         0.992056           2.66082          0.0263736                        0.193367           0.150506              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-06-22 20:25:14  2.563 sec   36                 0.170513         0.123949            0.99408         0.992056           2.66082          0.0263736                        0.193961           0.151254              0.976445          0.979196             2.78049            0.0350877\n",
              "\n",
              "Variable Importances: \n",
              "variable              relative_importance    scaled_importance    percentage\n",
              "--------------------  ---------------------  -------------------  ------------\n",
              "concave points_worst  204.282                1                    0.341069\n",
              "area_worst            111.787                0.547218             0.186639\n",
              "concave points_mean   80.5573                0.394343             0.134498\n",
              "radius_worst          72.7003                0.355881             0.12138\n",
              "perimeter_worst       63.8343                0.312481             0.106577\n",
              "texture_worst         22.0758                0.108065             0.0368576\n",
              "concavity_worst       20.4532                0.100122             0.0341486\n",
              "texture_mean          10.5059                0.0514284            0.0175406\n",
              "radius_mean           8.47355                0.0414796            0.0141474\n",
              "concavity_mean        3.36487                0.0164716            0.00561796\n",
              "compactness_se        0.913376               0.00447114           0.00152497\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ],
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_9_20230622_202511\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-108.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-108 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-108 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-108 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-108 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-108 .h2o-table th,\n",
              "#h2o-table-108 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-108 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-108\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>number_of_trees</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>36.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.029074514855628383\n",
              "RMSE: 0.17051250644931704\n",
              "LogLoss: 0.12394929039565182\n",
              "Mean Per-Class Error: 0.029270653158718393\n",
              "AUC: 0.9940799769376494\n",
              "AUCPR: 0.992056368504806\n",
              "Gini: 0.9881599538752988</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-109.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-109 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-109 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-109 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-109 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-109 .h2o-table th,\n",
              "#h2o-table-109 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-109 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-109\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4925362765789032</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>279.0</td>\n",
              "<td>5.0</td>\n",
              "<td>0.0176</td>\n",
              "<td> (5.0/284.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>7.0</td>\n",
              "<td>164.0</td>\n",
              "<td>0.0409</td>\n",
              "<td> (7.0/171.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>286.0</td>\n",
              "<td>169.0</td>\n",
              "<td>0.0264</td>\n",
              "<td> (12.0/455.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-110.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-110 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-110 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-110 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-110 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-110 .h2o-table th,\n",
              "#h2o-table-110 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-110 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-110\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9647059</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.3179893</td>\n",
              "<td>0.9710983</td>\n",
              "<td>49.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.5147656</td>\n",
              "<td>0.9713945</td>\n",
              "<td>37.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.5147656</td>\n",
              "<td>0.9736264</td>\n",
              "<td>37.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0588847</td>\n",
              "<td>1.0</td>\n",
              "<td>100.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9436960</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.4028056</td>\n",
              "<td>0.9649123</td>\n",
              "<td>42.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9707293</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>284.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>110.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0148769</td>\n",
              "<td>284.0</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0588847</td>\n",
              "<td>171.0</td>\n",
              "<td>100.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.6432749</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0148769</td>\n",
              "<td>1.0</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0588847</td>\n",
              "<td>1.0</td>\n",
              "<td>100.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-111.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-111 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-111 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-111 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-111 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-111 .h2o-table th,\n",
              "#h2o-table-111 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-111 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-111\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.29 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.1340659</td>\n",
              "<td>0.9680020</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.3567251</td>\n",
              "<td>0.3567251</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.3567251</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.1626374</td>\n",
              "<td>0.9576122</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9576122</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661767</td>\n",
              "<td>0.0760234</td>\n",
              "<td>0.4327485</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.4327485</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.2</td>\n",
              "<td>0.9505008</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9534877</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9638063</td>\n",
              "<td>0.0994152</td>\n",
              "<td>0.5321637</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.5321637</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.3010989</td>\n",
              "<td>0.8075677</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.8971791</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9414351</td>\n",
              "<td>0.2690058</td>\n",
              "<td>0.8011696</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.8011696</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.4</td>\n",
              "<td>0.3147044</td>\n",
              "<td>1.8330084</td>\n",
              "<td>2.4561404</td>\n",
              "<td>0.6888889</td>\n",
              "<td>0.5823229</td>\n",
              "<td>0.9230769</td>\n",
              "<td>0.8526436</td>\n",
              "<td>0.1812865</td>\n",
              "<td>0.9824561</td>\n",
              "<td>83.3008447</td>\n",
              "<td>145.6140351</td>\n",
              "<td>0.9331604</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.5010989</td>\n",
              "<td>0.0766930</td>\n",
              "<td>0.0578439</td>\n",
              "<td>1.9722735</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.1615786</td>\n",
              "<td>0.7412281</td>\n",
              "<td>0.7132182</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9883041</td>\n",
              "<td>-94.2156115</td>\n",
              "<td>97.2273520</td>\n",
              "<td>0.7805576</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.6241758</td>\n",
              "<td>0.0503081</td>\n",
              "<td>0.0950292</td>\n",
              "<td>1.6021127</td>\n",
              "<td>0.0357143</td>\n",
              "<td>0.0583980</td>\n",
              "<td>0.6021127</td>\n",
              "<td>0.5840988</td>\n",
              "<td>0.0116959</td>\n",
              "<td>1.0</td>\n",
              "<td>-90.4970760</td>\n",
              "<td>60.2112676</td>\n",
              "<td>0.6021127</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.6989011</td>\n",
              "<td>0.0279195</td>\n",
              "<td>0.0</td>\n",
              "<td>1.4308176</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0365022</td>\n",
              "<td>0.5377358</td>\n",
              "<td>0.5255507</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>43.0817610</td>\n",
              "<td>0.4823944</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.8043956</td>\n",
              "<td>0.0177071</td>\n",
              "<td>0.0</td>\n",
              "<td>1.2431694</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0225316</td>\n",
              "<td>0.4672131</td>\n",
              "<td>0.4595810</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>24.3169399</td>\n",
              "<td>0.3133803</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.9318681</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0731132</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.4033019</td>\n",
              "<td>0.3991110</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>7.3113208</td>\n",
              "<td>0.1091549</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.3758242</td>\n",
              "<td>0.3729324</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.03762067586537018\n",
              "RMSE: 0.19396050078655236\n",
              "LogLoss: 0.1512542647982658\n",
              "Mean Per-Class Error: 0.03274306715669896\n",
              "AUC: 0.976445038422987\n",
              "AUCPR: 0.9791961628675718\n",
              "Gini: 0.952890076845974</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-112.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-112 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-112 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-112 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-112 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-112 .h2o-table th,\n",
              "#h2o-table-112 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-112 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-112\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25368359684944153</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>70.0</td>\n",
              "<td>3.0</td>\n",
              "<td>0.0411</td>\n",
              "<td> (3.0/73.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>1.0</td>\n",
              "<td>40.0</td>\n",
              "<td>0.0244</td>\n",
              "<td> (1.0/41.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>71.0</td>\n",
              "<td>43.0</td>\n",
              "<td>0.0351</td>\n",
              "<td> (4.0/114.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-113.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-113 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-113 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-113 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-113 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-113 .h2o-table th,\n",
              "#h2o-table-113 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-113 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-113\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9523810</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9661836</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.7056506</td>\n",
              "<td>0.9729730</td>\n",
              "<td>15.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9649123</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0175245</td>\n",
              "<td>1.0</td>\n",
              "<td>46.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9252854</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9589041</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9672569</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>73.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>27.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0148769</td>\n",
              "<td>73.0</td>\n",
              "<td>47.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0175245</td>\n",
              "<td>41.0</td>\n",
              "<td>46.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.6585366</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0148769</td>\n",
              "<td>1.0</td>\n",
              "<td>47.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0175245</td>\n",
              "<td>1.0</td>\n",
              "<td>46.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-114.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-114 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-114 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-114 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-114 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-114 .h2o-table th,\n",
              "#h2o-table-114 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-114 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-114\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 35.96 %, avg score: 34.85 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.1228070</td>\n",
              "<td>0.9680020</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.3414634</td>\n",
              "<td>0.3414634</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.3414634</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.1578947</td>\n",
              "<td>0.9543629</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9551752</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9651516</td>\n",
              "<td>0.0975610</td>\n",
              "<td>0.4390244</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.4390244</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.2105263</td>\n",
              "<td>0.9322835</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9354083</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9577157</td>\n",
              "<td>0.1463415</td>\n",
              "<td>0.5853659</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.5853659</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.2982456</td>\n",
              "<td>0.7172713</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.8705932</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9320915</td>\n",
              "<td>0.2439024</td>\n",
              "<td>0.8292683</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.8292683</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.4035088</td>\n",
              "<td>0.1690287</td>\n",
              "<td>1.3902439</td>\n",
              "<td>2.4178155</td>\n",
              "<td>0.5</td>\n",
              "<td>0.4511201</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.8066207</td>\n",
              "<td>0.1463415</td>\n",
              "<td>0.9756098</td>\n",
              "<td>39.0243902</td>\n",
              "<td>141.7815483</td>\n",
              "<td>0.8934180</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.5087719</td>\n",
              "<td>0.0588847</td>\n",
              "<td>0.0</td>\n",
              "<td>1.9175778</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0937150</td>\n",
              "<td>0.6896552</td>\n",
              "<td>0.6591230</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>91.7577796</td>\n",
              "<td>0.7290344</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.5964912</td>\n",
              "<td>0.0359404</td>\n",
              "<td>0.0</td>\n",
              "<td>1.6355811</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0470055</td>\n",
              "<td>0.5882353</td>\n",
              "<td>0.5691057</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>63.5581062</td>\n",
              "<td>0.5920481</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.7017544</td>\n",
              "<td>0.0235090</td>\n",
              "<td>0.0</td>\n",
              "<td>1.3902439</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0319946</td>\n",
              "<td>0.5</td>\n",
              "<td>0.4885390</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>39.0243902</td>\n",
              "<td>0.4276646</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.9736842</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.0896932</td>\n",
              "<td>1.0270270</td>\n",
              "<td>0.0322581</td>\n",
              "<td>0.0192596</td>\n",
              "<td>0.3693694</td>\n",
              "<td>0.3574790</td>\n",
              "<td>0.0243902</td>\n",
              "<td>1.0</td>\n",
              "<td>-91.0306845</td>\n",
              "<td>2.7027027</td>\n",
              "<td>0.0410959</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.3596491</td>\n",
              "<td>0.3484632</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.044205368519838424\n",
              "RMSE: 0.21025072775103162\n",
              "LogLoss: 0.16637884924436225\n",
              "Mean Per-Class Error: 0.05391854048266205\n",
              "AUC: 0.9841137468083354\n",
              "AUCPR: 0.9796363289807396\n",
              "Gini: 0.9682274936166708</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-115.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-115 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-115 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-115 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-115 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-115 .h2o-table th,\n",
              "#h2o-table-115 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-115 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-115\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35231223702430725</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>265.0</td>\n",
              "<td>19.0</td>\n",
              "<td>0.0669</td>\n",
              "<td> (19.0/284.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>7.0</td>\n",
              "<td>164.0</td>\n",
              "<td>0.0409</td>\n",
              "<td> (7.0/171.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>272.0</td>\n",
              "<td>183.0</td>\n",
              "<td>0.0571</td>\n",
              "<td> (26.0/455.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-116.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-116 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-116 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-116 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-116 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-116 .h2o-table th,\n",
              "#h2o-table-116 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-116 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-116\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9265537</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9457901</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6488545</td>\n",
              "<td>0.9533074</td>\n",
              "<td>71.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.6243538</td>\n",
              "<td>0.9428571</td>\n",
              "<td>74.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0317377</td>\n",
              "<td>1.0</td>\n",
              "<td>212.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.8812343</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3933513</td>\n",
              "<td>0.9401408</td>\n",
              "<td>98.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9460815</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9681047</td>\n",
              "<td>284.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9681047</td>\n",
              "<td>168.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0161675</td>\n",
              "<td>284.0</td>\n",
              "<td>239.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0317377</td>\n",
              "<td>171.0</td>\n",
              "<td>212.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9681047</td>\n",
              "<td>0.9824561</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0161675</td>\n",
              "<td>1.0</td>\n",
              "<td>239.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0317377</td>\n",
              "<td>1.0</td>\n",
              "<td>212.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-117.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-117 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-117 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-117 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-117 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-117 .h2o-table th,\n",
              "#h2o-table-117 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-117 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-117\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.58 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.0153846</td>\n",
              "<td>0.9646416</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661258</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661258</td>\n",
              "<td>0.0409357</td>\n",
              "<td>0.0409357</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.0409357</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.0329670</td>\n",
              "<td>0.9641652</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9641652</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9650801</td>\n",
              "<td>0.0467836</td>\n",
              "<td>0.0877193</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.0877193</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.0461538</td>\n",
              "<td>0.9632205</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9632205</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9645488</td>\n",
              "<td>0.0350877</td>\n",
              "<td>0.1228070</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.1228070</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.0659341</td>\n",
              "<td>0.9616467</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9616467</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9636782</td>\n",
              "<td>0.0526316</td>\n",
              "<td>0.1754386</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.1754386</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.1010989</td>\n",
              "<td>0.9592377</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9601760</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9624600</td>\n",
              "<td>0.0935673</td>\n",
              "<td>0.2690058</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.2690058</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1516484</td>\n",
              "<td>0.9519403</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9548322</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9599174</td>\n",
              "<td>0.1345029</td>\n",
              "<td>0.4035088</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.4035088</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.2</td>\n",
              "<td>0.9348235</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9443512</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9561542</td>\n",
              "<td>0.1286550</td>\n",
              "<td>0.5321637</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.5321637</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.3010989</td>\n",
              "<td>0.7810790</td>\n",
              "<td>2.5451309</td>\n",
              "<td>2.6219746</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.8873240</td>\n",
              "<td>0.9854015</td>\n",
              "<td>0.9330433</td>\n",
              "<td>0.2573099</td>\n",
              "<td>0.7894737</td>\n",
              "<td>154.5130943</td>\n",
              "<td>162.1974645</td>\n",
              "<td>0.7824314</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.4</td>\n",
              "<td>0.3541536</td>\n",
              "<td>1.6556205</td>\n",
              "<td>2.3830409</td>\n",
              "<td>0.6222222</td>\n",
              "<td>0.5669310</td>\n",
              "<td>0.8956044</td>\n",
              "<td>0.8425210</td>\n",
              "<td>0.1637427</td>\n",
              "<td>0.9532164</td>\n",
              "<td>65.5620533</td>\n",
              "<td>138.3040936</td>\n",
              "<td>0.8863150</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.5010989</td>\n",
              "<td>0.0901423</td>\n",
              "<td>0.2892194</td>\n",
              "<td>1.9606033</td>\n",
              "<td>0.1086957</td>\n",
              "<td>0.1979604</td>\n",
              "<td>0.7368421</td>\n",
              "<td>0.7124781</td>\n",
              "<td>0.0292398</td>\n",
              "<td>0.9824561</td>\n",
              "<td>-71.0780575</td>\n",
              "<td>96.0603263</td>\n",
              "<td>0.7711885</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.6</td>\n",
              "<td>0.0496738</td>\n",
              "<td>0.0591293</td>\n",
              "<td>1.6471735</td>\n",
              "<td>0.0222222</td>\n",
              "<td>0.0669556</td>\n",
              "<td>0.6190476</td>\n",
              "<td>0.6060733</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9883041</td>\n",
              "<td>-94.0870695</td>\n",
              "<td>64.7173489</td>\n",
              "<td>0.6221069</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6989011</td>\n",
              "<td>0.0362709</td>\n",
              "<td>0.0591293</td>\n",
              "<td>1.4224503</td>\n",
              "<td>0.0222222</td>\n",
              "<td>0.0437602</td>\n",
              "<td>0.5345912</td>\n",
              "<td>0.5265007</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9941520</td>\n",
              "<td>-94.0870695</td>\n",
              "<td>42.2450256</td>\n",
              "<td>0.4730253</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0304273</td>\n",
              "<td>0.0578439</td>\n",
              "<td>1.25</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0321457</td>\n",
              "<td>0.4697802</td>\n",
              "<td>0.4640272</td>\n",
              "<td>0.0058480</td>\n",
              "<td>1.0</td>\n",
              "<td>-94.2156115</td>\n",
              "<td>25.0</td>\n",
              "<td>0.3204225</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.9076923</td>\n",
              "<td>0.0233827</td>\n",
              "<td>0.0</td>\n",
              "<td>1.1016949</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0257063</td>\n",
              "<td>0.4140436</td>\n",
              "<td>0.4120231</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>10.1694915</td>\n",
              "<td>0.1478873</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0161675</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0195924</td>\n",
              "<td>0.3758242</td>\n",
              "<td>0.3757987</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-118.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-118 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-118 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-118 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-118 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-118 .h2o-table th,\n",
              "#h2o-table-118 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-118 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-118\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th>\n",
              "<th>cv_6_valid</th>\n",
              "<th>cv_7_valid</th>\n",
              "<th>cv_8_valid</th>\n",
              "<th>cv_9_valid</th>\n",
              "<th>cv_10_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.9671497</td>\n",
              "<td>0.0235589</td>\n",
              "<td>0.9347826</td>\n",
              "<td>0.9782609</td>\n",
              "<td>0.9782609</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.9347826</td>\n",
              "<td>0.9555556</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9555556</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9777778</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.9838794</td>\n",
              "<td>0.0178793</td>\n",
              "<td>0.9609375</td>\n",
              "<td>0.9958333</td>\n",
              "<td>0.9979716</td>\n",
              "<td>0.9846154</td>\n",
              "<td>0.9754464</td>\n",
              "<td>0.9801587</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9482759</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9955556</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.0328502</td>\n",
              "<td>0.0235589</td>\n",
              "<td>0.0652174</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0434783</td>\n",
              "<td>0.0652174</td>\n",
              "<td>0.0444444</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0444444</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0222222</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>1.5</td>\n",
              "<td>1.0801234</td>\n",
              "<td>3.0</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>2.0</td>\n",
              "<td>3.0</td>\n",
              "<td>2.0</td>\n",
              "<td>0.0</td>\n",
              "<td>2.0</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.9588513</td>\n",
              "<td>0.0287485</td>\n",
              "<td>0.9482759</td>\n",
              "<td>0.9868421</td>\n",
              "<td>0.9550562</td>\n",
              "<td>0.95</td>\n",
              "<td>0.9090909</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9493671</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.9515683</td>\n",
              "<td>0.0406347</td>\n",
              "<td>0.88</td>\n",
              "<td>0.9677419</td>\n",
              "<td>0.9714286</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8888889</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9677419</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.9454923</td>\n",
              "<td>0.0586199</td>\n",
              "<td>0.8208955</td>\n",
              "<td>0.9493671</td>\n",
              "<td>0.9883721</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9868421</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.7496953</td>\n",
              "<td>0.4955653</td>\n",
              "<td>3.2857144</td>\n",
              "<td>2.875</td>\n",
              "<td>2.7058823</td>\n",
              "<td>2.3</td>\n",
              "<td>3.2857144</td>\n",
              "<td>2.142857</td>\n",
              "<td>1.875</td>\n",
              "<td>2.8125</td>\n",
              "<td>3.2142856</td>\n",
              "<td>3.0</td></tr>\n",
              "<tr><td>logloss</td>\n",
              "<td>0.1662221</td>\n",
              "<td>0.0649039</td>\n",
              "<td>0.2129980</td>\n",
              "<td>0.1471438</td>\n",
              "<td>0.1064336</td>\n",
              "<td>0.2047186</td>\n",
              "<td>0.2311378</td>\n",
              "<td>0.199252</td>\n",
              "<td>0.0746679</td>\n",
              "<td>0.2557887</td>\n",
              "<td>0.0740798</td>\n",
              "<td>0.1560008</td></tr>\n",
              "<tr><td>max_per_class_error</td>\n",
              "<td>0.0647578</td>\n",
              "<td>0.0660564</td>\n",
              "<td>0.2142857</td>\n",
              "<td>0.0625</td>\n",
              "<td>0.0344828</td>\n",
              "<td>0.05</td>\n",
              "<td>0.1428571</td>\n",
              "<td>0.0476191</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0625</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0333333</td></tr>\n",
              "<tr><td>mcc</td>\n",
              "<td>0.9276215</td>\n",
              "<td>0.0547608</td>\n",
              "<td>0.8475655</td>\n",
              "<td>0.9525009</td>\n",
              "<td>0.9549227</td>\n",
              "<td>0.9115385</td>\n",
              "<td>0.8439837</td>\n",
              "<td>0.9107143</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9030172</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9519716</td></tr>\n",
              "<tr><td>mean_per_class_accuracy</td>\n",
              "<td>0.9603280</td>\n",
              "<td>0.0352320</td>\n",
              "<td>0.8928571</td>\n",
              "<td>0.96875</td>\n",
              "<td>0.9827586</td>\n",
              "<td>0.9557692</td>\n",
              "<td>0.9129464</td>\n",
              "<td>0.9553571</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9515086</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9833333</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.0396719</td>\n",
              "<td>0.0352320</td>\n",
              "<td>0.1071429</td>\n",
              "<td>0.03125</td>\n",
              "<td>0.0172414</td>\n",
              "<td>0.0442308</td>\n",
              "<td>0.0870536</td>\n",
              "<td>0.0446429</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0484914</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0166667</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.0441377</td>\n",
              "<td>0.0226749</td>\n",
              "<td>0.0620417</td>\n",
              "<td>0.0377012</td>\n",
              "<td>0.0215032</td>\n",
              "<td>0.0596129</td>\n",
              "<td>0.0706184</td>\n",
              "<td>0.0543299</td>\n",
              "<td>0.0114468</td>\n",
              "<td>0.0693428</td>\n",
              "<td>0.0124989</td>\n",
              "<td>0.0422811</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.9778822</td>\n",
              "<td>0.0236690</td>\n",
              "<td>0.9385305</td>\n",
              "<td>0.9928552</td>\n",
              "<td>0.9966378</td>\n",
              "<td>0.9803309</td>\n",
              "<td>0.9529279</td>\n",
              "<td>0.9820066</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9444352</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9910979</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.9644902</td>\n",
              "<td>0.0315774</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9444444</td>\n",
              "<td>0.95</td>\n",
              "<td>0.9230769</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.8056853</td>\n",
              "<td>0.1029978</td>\n",
              "<td>0.7069637</td>\n",
              "<td>0.8338005</td>\n",
              "<td>0.9077061</td>\n",
              "<td>0.7574214</td>\n",
              "<td>0.6664543</td>\n",
              "<td>0.7817103</td>\n",
              "<td>0.9540083</td>\n",
              "<td>0.6973723</td>\n",
              "<td>0.9416813</td>\n",
              "<td>0.8097349</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.9420238</td>\n",
              "<td>0.0709415</td>\n",
              "<td>0.7857143</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8571429</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.2020618</td>\n",
              "<td>0.0606329</td>\n",
              "<td>0.2490817</td>\n",
              "<td>0.1941680</td>\n",
              "<td>0.1466398</td>\n",
              "<td>0.2441575</td>\n",
              "<td>0.2657412</td>\n",
              "<td>0.2330877</td>\n",
              "<td>0.1069898</td>\n",
              "<td>0.2633303</td>\n",
              "<td>0.1117986</td>\n",
              "<td>0.2056237</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.9786323</td>\n",
              "<td>0.0186053</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9655172</td>\n",
              "<td>0.9615384</td>\n",
              "<td>0.96875</td>\n",
              "<td>0.9583333</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9655172</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9666666</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-119.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-119 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-119 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-119 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-119 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-119 .h2o-table th,\n",
              "#h2o-table-119 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-119 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-119\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Scoring History: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>timestamp</th>\n",
              "<th>duration</th>\n",
              "<th>number_of_trees</th>\n",
              "<th>training_rmse</th>\n",
              "<th>training_logloss</th>\n",
              "<th>training_auc</th>\n",
              "<th>training_pr_auc</th>\n",
              "<th>training_lift</th>\n",
              "<th>training_classification_error</th>\n",
              "<th>validation_rmse</th>\n",
              "<th>validation_logloss</th>\n",
              "<th>validation_auc</th>\n",
              "<th>validation_pr_auc</th>\n",
              "<th>validation_lift</th>\n",
              "<th>validation_classification_error</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.312 sec</td>\n",
              "<td>0.0</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3758242</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6241758</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3596491</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6403509</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.330 sec</td>\n",
              "<td>5.0</td>\n",
              "<td>0.2251009</td>\n",
              "<td>0.2209849</td>\n",
              "<td>0.9914649</td>\n",
              "<td>0.9879580</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0307692</td>\n",
              "<td>0.2254283</td>\n",
              "<td>0.2225214</td>\n",
              "<td>0.9767792</td>\n",
              "<td>0.9787964</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0438596</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.347 sec</td>\n",
              "<td>10.0</td>\n",
              "<td>0.1821748</td>\n",
              "<td>0.1441039</td>\n",
              "<td>0.9935034</td>\n",
              "<td>0.9911247</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0307692</td>\n",
              "<td>0.1949417</td>\n",
              "<td>0.1601430</td>\n",
              "<td>0.9772803</td>\n",
              "<td>0.9799751</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.364 sec</td>\n",
              "<td>15.0</td>\n",
              "<td>0.1723523</td>\n",
              "<td>0.1306105</td>\n",
              "<td>0.9945330</td>\n",
              "<td>0.9927252</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0241758</td>\n",
              "<td>0.1910482</td>\n",
              "<td>0.1526369</td>\n",
              "<td>0.9771133</td>\n",
              "<td>0.9804633</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.383 sec</td>\n",
              "<td>20.0</td>\n",
              "<td>0.1702960</td>\n",
              "<td>0.1239005</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934925</td>\n",
              "<td>0.1506594</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.413 sec</td>\n",
              "<td>25.0</td>\n",
              "<td>0.1702965</td>\n",
              "<td>0.1238999</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934951</td>\n",
              "<td>0.1506626</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.451 sec</td>\n",
              "<td>30.0</td>\n",
              "<td>0.1702950</td>\n",
              "<td>0.1239018</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934870</td>\n",
              "<td>0.1506526</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.536 sec</td>\n",
              "<td>35.0</td>\n",
              "<td>0.1702896</td>\n",
              "<td>0.1239514</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1933670</td>\n",
              "<td>0.1505057</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-06-22 20:25:14</td>\n",
              "<td> 2.563 sec</td>\n",
              "<td>36.0</td>\n",
              "<td>0.1705125</td>\n",
              "<td>0.1239493</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1939605</td>\n",
              "<td>0.1512543</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-120.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-120 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-120 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-120 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-120 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-120 .h2o-table th,\n",
              "#h2o-table-120 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-120 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-120\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Variable Importances: </caption>\n",
              "    <thead><tr><th>variable</th>\n",
              "<th>relative_importance</th>\n",
              "<th>scaled_importance</th>\n",
              "<th>percentage</th></tr></thead>\n",
              "    <tbody><tr><td>concave points_worst</td>\n",
              "<td>204.2824097</td>\n",
              "<td>1.0</td>\n",
              "<td>0.3410687</td></tr>\n",
              "<tr><td>area_worst</td>\n",
              "<td>111.7869415</td>\n",
              "<td>0.5472177</td>\n",
              "<td>0.1866388</td></tr>\n",
              "<tr><td>concave points_mean</td>\n",
              "<td>80.5573425</td>\n",
              "<td>0.3943430</td>\n",
              "<td>0.1344981</td></tr>\n",
              "<tr><td>radius_worst</td>\n",
              "<td>72.7002869</td>\n",
              "<td>0.3558813</td>\n",
              "<td>0.1213800</td></tr>\n",
              "<tr><td>perimeter_worst</td>\n",
              "<td>63.8343277</td>\n",
              "<td>0.3124808</td>\n",
              "<td>0.1065774</td></tr>\n",
              "<tr><td>texture_worst</td>\n",
              "<td>22.0757656</td>\n",
              "<td>0.1080649</td>\n",
              "<td>0.0368576</td></tr>\n",
              "<tr><td>concavity_worst</td>\n",
              "<td>20.4532471</td>\n",
              "<td>0.1001224</td>\n",
              "<td>0.0341486</td></tr>\n",
              "<tr><td>texture_mean</td>\n",
              "<td>10.5059090</td>\n",
              "<td>0.0514284</td>\n",
              "<td>0.0175406</td></tr>\n",
              "<tr><td>radius_mean</td>\n",
              "<td>8.4735489</td>\n",
              "<td>0.0414796</td>\n",
              "<td>0.0141474</td></tr>\n",
              "<tr><td>concavity_mean</td>\n",
              "<td>3.3648682</td>\n",
              "<td>0.0164716</td>\n",
              "<td>0.0056180</td></tr>\n",
              "<tr><td>compactness_se</td>\n",
              "<td>0.9133759</td>\n",
              "<td>0.0044711</td>\n",
              "<td>0.0015250</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 399
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.model_performance(train).accuracy()"
      ],
      "metadata": {
        "id": "G_PyYFNMs97_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528a9aa1-1176-4a94-fc47-d998466c2b00"
      },
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.5147656202316284, 0.9736263736263736]]"
            ]
          },
          "metadata": {},
          "execution_count": 400
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "HATr  = best_model.model_performance(train)\n",
        "HATe  = best_model.model_performance(valid)"
      ],
      "metadata": {
        "id": "jKquKjVVJBL9"
      },
      "execution_count": 401,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_h2o = pd.DataFrame(h2o.as_list(best_model.predict(valid)))['predict']\n",
        "y_test_h2o = np.array(valid1['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-caBWGPlp8P",
        "outputId": "ac507c56-25a0-4785-bc3c-864978102d6a"
      },
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SFOLD DATA AUTOML\n",
        "#strain, svalid = shdf.split_frame(ratios=[.8], seed=123)\n",
        "shdf  = newdata.copy()\n",
        "#shdf['y_test'] = shdf['y_test'].replace(0,\"B\")\n",
        "#shdf['y_test'] = shdf['y_test'].replace(1,\"M\")\n",
        "shy = \"y_test\"\n",
        "shx = list(shdf.columns)\n",
        "shx.remove(shy)\n",
        "\n",
        "shdf.iloc[:,0:6] = StandardScaler().fit_transform(shdf.iloc[:,0:6])\n",
        "#shdf.iloc[:,-1] = LabelEncoder().fit_transform(shdf.iloc[:,-1])\n",
        "strain1, svalid1 = train_test_split(shdf, test_size=0.2,random_state=123)\n",
        "strain = h2o.H2OFrame(strain1)\n",
        "svalid = h2o.H2OFrame(svalid1)\n",
        "strain[\"y_test\"] = strain[\"y_test\"].asfactor()\n",
        "svalid[\"y_test\"] = svalid[\"y_test\"].asfactor()\n",
        "\n",
        "\n",
        "saml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\", nfolds=10, sort_metric='accuracy')\n",
        "saml.train(x = shx, y = shy, training_frame = strain, validation_frame = svalid)\n",
        "sbest_model = saml.get_best_model()\n",
        "sHATr  = sbest_model.model_performance(strain)\n",
        "sHATe  = sbest_model.model_performance(svalid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMDqylVK7svS",
        "outputId": "d17e5ab7-bf7c-4300-9138-597e2bbfadc4"
      },
      "execution_count": 404,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "AutoML progress: |\n",
            "20:26:20.33: Project: AutoML_10_20230622_202620\n",
            "20:26:20.33: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "20:26:20.33: Setting stopping tolerance adaptively based on the training frame: 0.04688072309384954\n",
            "20:26:20.33: Build control seed: 123\n",
            "20:26:20.33: training frame: Frame key: AutoML_10_20230622_202620_training_py_198_sid_b073    cols: 7    rows: 455  chunks: 1    size: 6687  checksum: 580350703051570352\n",
            "20:26:20.34: validation frame: Frame key: py_199_sid_b073    cols: 7    rows: 114  chunks: 1    size: 2557  checksum: -5131178475415970944\n",
            "20:26:20.34: leaderboard frame: NULL\n",
            "20:26:20.34: blending frame: NULL\n",
            "20:26:20.34: response column: y_test\n",
            "20:26:20.34: fold column: null\n",
            "20:26:20.34: weights column: null\n",
            "20:26:20.34: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "20:26:20.35: AutoML job created: 2023.06.22 20:26:20.33\n",
            "20:26:20.37: AutoML build started: 2023.06.22 20:26:20.37\n",
            "20:26:20.38: AutoML: starting XGBoost_1_AutoML_10_20230622_202620 model training\n",
            "\n",
            "\n",
            "20:26:21.802: New leader: XGBoost_1_AutoML_10_20230622_202620, accuracy: 0.9318681318681319\n",
            "20:26:21.803: AutoML: starting GLM_1_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆ\n",
            "20:26:22.583: AutoML: starting GBM_1_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "20:26:24.806: New leader: GBM_1_AutoML_10_20230622_202620, accuracy: 0.9318681318681319\n",
            "20:26:24.806: AutoML: starting XGBoost_2_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "20:26:27.532: AutoML: starting DRF_1_AutoML_10_20230622_202620 model training\n",
            "20:26:28.738: AutoML: starting GBM_2_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆ\n",
            "20:26:30.331: AutoML: starting GBM_3_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆ\n",
            "20:26:31.870: AutoML: starting GBM_4_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆâ–ˆ\n",
            "20:26:33.419: AutoML: starting XGBoost_3_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆ\n",
            "20:26:36.987: AutoML: starting XRT_1_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆ\n",
            "20:26:38.601: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "20:26:38.603: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆ\n",
            "20:26:41.446: AutoML: starting StackedEnsemble_AllModels_1_AutoML_10_20230622_202620 model training\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "\n",
            "20:26:44.444: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "20:26:44.444: AutoML build stopped: 2023.06.22 20:26:44.444\n",
            "20:26:44.444: AutoML build done: built 10 models\n",
            "20:26:44.444: AutoML duration: 24.407 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_sh2o = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))['predict']\n",
        "y_test_sh2o = np.array(svalid1['y_test']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivMaaDNNmYY2",
        "outputId": "760ed177-5bce-4350-ec70-0fda052cb2f1"
      },
      "execution_count": 405,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gbm prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "nJrcI3Gn8we2",
        "outputId": "6973b9ca-d432-434b-b759-e3cbfff0fa66"
      },
      "execution_count": 407,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 7 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 407
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]\n",
        "\n",
        "label = [\"ArtificialNeuralNetwork\", 'DeepNeuralNetwork',\n",
        "         'KNearestNeighborsClassifier', 'LogisticRegression',\n",
        "         'RandomForestClassifier', 'SupportVectorClassifier',\n",
        "         'XGBoost', type(best_model).__name__, type(sbest_model).__name__ ]\n",
        "\n",
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"DNN\":[DTr,DTe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"H_OD\":[HATr.accuracy()[0][1],HATe.accuracy()[0][1]],\n",
        "    \"H_SOD\":[sHATr.accuracy()[0][1],sHATe.accuracy()[0][1]]\n",
        "    })\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc['Model'] = label\n",
        "\n",
        "acc = acc[['Model', 'train', 'test']]\n",
        "acc['avg'] = round((acc['train'] + acc['test'])/2, 6)\n",
        "acc[acc[\"avg\"] == acc[\"avg\"].max()]\n",
        "acc['BestModel'] = 0\n",
        "for i in range(len(acc)):\n",
        "  if acc['avg'][i] >= 90 and acc['avg'][i] < acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"good\"\n",
        "  elif acc['avg'][i] == acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"best\"\n",
        "  else:\n",
        "    acc.iloc[i,-1] = \"not good\"\n",
        "\n",
        "acc[\"Precision\"] = np.zeros(len(acc))\n",
        "acc[\"Recall\"]    = np.zeros(len(acc))\n",
        "acc[\"F1_Score\"]  = np.zeros(len(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ba6kRO-eI60S"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ANNn = []\n",
        "y_pred_DNNn = []\n",
        "for i in range(len(y_pred_ANN)):\n",
        "  y_pred_ANNn.append(y_pred_ANN[i][0])\n",
        "  y_pred_DNNn.append(y_pred_DNN[i][0])"
      ],
      "metadata": {
        "id": "S7pf8G6ao4oF"
      },
      "execution_count": 410,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [np.array(y_pred_ANNn), np.array(y_pred_DNNn), y_pred_knn,\n",
        "        y_pred_lr, y_pred_rf,\n",
        "        y_pred_svm, y_pred_xgb, y_pred_h2o, y_pred_sh2o]\n",
        "\n",
        "tes  = [y_test_indi_ML, np.array(Dy_test), y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_indi_ML, y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_h2o.copy(), y_test_sh2o.copy()]"
      ],
      "metadata": {
        "id": "D2n2VnW1jKp4"
      },
      "execution_count": 411,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pred)):\n",
        "  p,r,f,_ = precision_recall_fscore_support(tes[i], pred[i],\n",
        "                                            average='macro')\n",
        "  acc.iloc[i,5]= p\n",
        "  acc.iloc[i,6]= r\n",
        "  acc.iloc[i,7]= f\n",
        "  p = 0\n",
        "  r = 0\n",
        "  f = 0\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "2DlSQ29moN9T",
        "outputId": "49e10e35-b38e-464d-fc17-001e79dc4ab4"
      },
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              Model     train      test       avg BestModel  \\\n",
              "ANN         ArtificialNeuralNetwork  0.951648  0.956140  0.953894  not good   \n",
              "DNN               DeepNeuralNetwork  0.953846  0.903509  0.928677  not good   \n",
              "KNN     KNearestNeighborsClassifier  0.980220  0.991228  0.985724  not good   \n",
              "LR               LogisticRegression  0.980220  0.973684  0.976952  not good   \n",
              "RF           RandomForestClassifier  0.975824  0.991228  0.983526  not good   \n",
              "SVM         SupportVectorClassifier  0.951648  0.956140  0.953894  not good   \n",
              "XGB                         XGBoost  0.989011  0.982456  0.985734      best   \n",
              "H_OD            H2OXGBoostEstimator  0.973626  0.964912  0.969269  not good   \n",
              "H_SOD  H2OGradientBoostingEstimator  0.938462  0.912281  0.925371  not good   \n",
              "\n",
              "       Precision    Recall  F1_Score  \n",
              "ANN     0.967949  0.939024  0.950976  \n",
              "DNN     0.907051  0.881891  0.892148  \n",
              "KNN     0.993243  0.987805  0.990426  \n",
              "LR      0.980263  0.963415  0.970946  \n",
              "RF      0.993243  0.987805  0.990426  \n",
              "SVM     0.967949  0.939024  0.950976  \n",
              "XGB     0.986667  0.975610  0.980743  \n",
              "H_OD    0.958074  0.967257  0.962302  \n",
              "H_SOD   0.914005  0.894086  0.902564  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be00fc7a-cf3b-4643-a95d-9845145af4e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>avg</th>\n",
              "      <th>BestModel</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>ArtificialNeuralNetwork</td>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.953894</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.967949</td>\n",
              "      <td>0.939024</td>\n",
              "      <td>0.950976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>DeepNeuralNetwork</td>\n",
              "      <td>0.953846</td>\n",
              "      <td>0.903509</td>\n",
              "      <td>0.928677</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.907051</td>\n",
              "      <td>0.881891</td>\n",
              "      <td>0.892148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>KNearestNeighborsClassifier</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.985724</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.976952</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.963415</td>\n",
              "      <td>0.970946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.975824</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.983526</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>SupportVectorClassifier</td>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.953894</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.967949</td>\n",
              "      <td>0.939024</td>\n",
              "      <td>0.950976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.989011</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.985734</td>\n",
              "      <td>best</td>\n",
              "      <td>0.986667</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.980743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_OD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.973626</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.969269</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.958074</td>\n",
              "      <td>0.967257</td>\n",
              "      <td>0.962302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_SOD</th>\n",
              "      <td>H2OGradientBoostingEstimator</td>\n",
              "      <td>0.938462</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.925371</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.914005</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.902564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be00fc7a-cf3b-4643-a95d-9845145af4e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be00fc7a-cf3b-4643-a95d-9845145af4e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be00fc7a-cf3b-4643-a95d-9845145af4e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 413
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "ru4oH6-YzGPw",
        "outputId": "11ba6e74-b132-4112-8153-b5c6ed38957b"
      },
      "execution_count": 418,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 9 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 418
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve"
      ],
      "metadata": {
        "id": "Qma6hWrmDBvf"
      },
      "execution_count": 414,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]"
      ],
      "metadata": {
        "id": "03rwpiYpoAGs"
      },
      "execution_count": 415,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder = LabelEncoder()\n",
        "#y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "#X = df.drop(columns=['diagnosis']).copy()\n",
        "#X = StandardScaler().fit_transform(X).copy()\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "#y_test_indi_ML = y_test.copy()"
      ],
      "metadata": {
        "id": "x5wrzn-qPaXw"
      },
      "execution_count": 416,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
        "\n",
        "y_pred = ANNmodel.predict(X_test).ravel()\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[0]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = model.predict(DX_test).ravel()\n",
        "y_test = Dy_test.copy()\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[1]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = knn.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[2]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = lr.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[3]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = rf.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[4]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = svm.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[5]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = xgb.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[6]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(best_model.predict(valid)))\n",
        "y_test = h2o.as_list(valid['diagnosis'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(best_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))\n",
        "y_test = h2o.as_list(svalid['y_test'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(sbest_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('AUC ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "4Bi4CMUJm6yH",
        "outputId": "30f797ce-cdf0-4596-9f56-5241a0e11063"
      },
      "execution_count": 417,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "xgboost prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n",
            "gbm prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (done) 100%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}