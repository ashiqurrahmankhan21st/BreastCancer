{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashiqurrahmankhan21st/BreastCancer/blob/main/Breast_Cancer_V4_Original_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EUMYU_d6_J-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "#import keras_metrics\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#original data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/data.csv\")\n",
        "del df['id']\n",
        "df"
      ],
      "metadata": {
        "id": "Ntdqq4T_aGXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "5f074d22-8714-4bb4-9112-a355f66f93bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0           M        17.99         10.38          122.80     1001.0   \n",
              "1           M        20.57         17.77          132.90     1326.0   \n",
              "2           M        19.69         21.25          130.00     1203.0   \n",
              "3           M        11.42         20.38           77.58      386.1   \n",
              "4           M        20.29         14.34          135.10     1297.0   \n",
              "..        ...          ...           ...             ...        ...   \n",
              "564         M        21.56         22.39          142.00     1479.0   \n",
              "565         M        20.13         28.25          131.20     1261.0   \n",
              "566         M        16.60         28.08          108.30      858.1   \n",
              "567         M        20.60         29.33          140.10     1265.0   \n",
              "568         B         7.76         24.54           47.92      181.0   \n",
              "\n",
              "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0            0.11840           0.27760         0.30010              0.14710   \n",
              "1            0.08474           0.07864         0.08690              0.07017   \n",
              "2            0.10960           0.15990         0.19740              0.12790   \n",
              "3            0.14250           0.28390         0.24140              0.10520   \n",
              "4            0.10030           0.13280         0.19800              0.10430   \n",
              "..               ...               ...             ...                  ...   \n",
              "564          0.11100           0.11590         0.24390              0.13890   \n",
              "565          0.09780           0.10340         0.14400              0.09791   \n",
              "566          0.08455           0.10230         0.09251              0.05302   \n",
              "567          0.11780           0.27700         0.35140              0.15200   \n",
              "568          0.05263           0.04362         0.00000              0.00000   \n",
              "\n",
              "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
              "0           0.2419  ...        25.380          17.33           184.60   \n",
              "1           0.1812  ...        24.990          23.41           158.80   \n",
              "2           0.2069  ...        23.570          25.53           152.50   \n",
              "3           0.2597  ...        14.910          26.50            98.87   \n",
              "4           0.1809  ...        22.540          16.67           152.20   \n",
              "..             ...  ...           ...            ...              ...   \n",
              "564         0.1726  ...        25.450          26.40           166.10   \n",
              "565         0.1752  ...        23.690          38.25           155.00   \n",
              "566         0.1590  ...        18.980          34.12           126.70   \n",
              "567         0.2397  ...        25.740          39.42           184.60   \n",
              "568         0.1587  ...         9.456          30.37            59.16   \n",
              "\n",
              "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "0        2019.0           0.16220            0.66560           0.7119   \n",
              "1        1956.0           0.12380            0.18660           0.2416   \n",
              "2        1709.0           0.14440            0.42450           0.4504   \n",
              "3         567.7           0.20980            0.86630           0.6869   \n",
              "4        1575.0           0.13740            0.20500           0.4000   \n",
              "..          ...               ...                ...              ...   \n",
              "564      2027.0           0.14100            0.21130           0.4107   \n",
              "565      1731.0           0.11660            0.19220           0.3215   \n",
              "566      1124.0           0.11390            0.30940           0.3403   \n",
              "567      1821.0           0.16500            0.86810           0.9387   \n",
              "568       268.6           0.08996            0.06444           0.0000   \n",
              "\n",
              "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
              "0                  0.2654          0.4601                  0.11890  \n",
              "1                  0.1860          0.2750                  0.08902  \n",
              "2                  0.2430          0.3613                  0.08758  \n",
              "3                  0.2575          0.6638                  0.17300  \n",
              "4                  0.1625          0.2364                  0.07678  \n",
              "..                    ...             ...                      ...  \n",
              "564                0.2216          0.2060                  0.07115  \n",
              "565                0.1628          0.2572                  0.06637  \n",
              "566                0.1418          0.2218                  0.07820  \n",
              "567                0.2650          0.4087                  0.12400  \n",
              "568                0.0000          0.2871                  0.07039  \n",
              "\n",
              "[569 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7182ccfb-0840-4bd3-865b-7bb619ecd0d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>...</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>...</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>...</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>...</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>...</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>...</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>...</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>...</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>...</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>...</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7182ccfb-0840-4bd3-865b-7bb619ecd0d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7182ccfb-0840-4bd3-865b-7bb619ecd0d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7182ccfb-0840-4bd3-865b-7bb619ecd0d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()"
      ],
      "metadata": {
        "id": "4cimS259ti6F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diagnosis'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv9sG-P8M1Fe",
        "outputId": "8e37a2f8-19bc-4adb-8ee7-2a76c04f9f61"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    357\n",
              "M    212\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()\n",
        "print(\"Original data : \",df.shape)\n",
        "print(\"tarin         : \",X_train.shape)\n",
        "print(\"test          : \",X_test.shape[0])\n",
        "#print(\"validation    : \",X_val.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpyVwv13CTut",
        "outputId": "b2a70c73-1b94-486f-b828-d86eeb188740"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data :  (569, 31)\n",
            "tarin         :  (455, 30)\n",
            "test          :  114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "st = time.time()\n",
        "svm = SVC(C=0.1, gamma='auto', kernel = 'rbf',probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "send = time.time() - st\n",
        "STr = svm.score(X_train, y_train)\n",
        "STe = svm.score(X_test, y_test)\n",
        "y_pred_svm = svm.predict(X_test)"
      ],
      "metadata": {
        "id": "wnRYFOkyEEZ9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANN\n",
        "st = time.time()\n",
        "tf.random.set_seed(123)\n",
        "ANNmodel = Sequential()\n",
        "ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "ANNmodel.add(Dense(10, activation='relu'))\n",
        "ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "aend = time.time() - st\n",
        "ATr = ANNmodel.evaluate(X_train,y_train,verbose=0)[1]\n",
        "ATe = ANNmodel.evaluate(X_test,y_test,verbose=0)[1]\n",
        "y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZX7DbN-OIyu",
        "outputId": "c29a2ead-c338-4874-fa9a-06c76ef77bbc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 49ms/step - loss: 0.8772 - accuracy: 0.3741 - val_loss: 0.8399 - val_accuracy: 0.4130\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7677 - accuracy: 0.3814 - val_loss: 0.7713 - val_accuracy: 0.4130\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6790 - accuracy: 0.4108 - val_loss: 0.7140 - val_accuracy: 0.4348\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6109 - accuracy: 0.5012 - val_loss: 0.6662 - val_accuracy: 0.4783\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5540 - accuracy: 0.5892 - val_loss: 0.6261 - val_accuracy: 0.5217\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5084 - accuracy: 0.6944 - val_loss: 0.5908 - val_accuracy: 0.5870\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4687 - accuracy: 0.7800 - val_loss: 0.5595 - val_accuracy: 0.6739\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4346 - accuracy: 0.8386 - val_loss: 0.5313 - val_accuracy: 0.7391\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4040 - accuracy: 0.8900 - val_loss: 0.5052 - val_accuracy: 0.7826\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3750 - accuracy: 0.9022 - val_loss: 0.4812 - val_accuracy: 0.8043\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3485 - accuracy: 0.9193 - val_loss: 0.4579 - val_accuracy: 0.8043\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3241 - accuracy: 0.9291 - val_loss: 0.4369 - val_accuracy: 0.8478\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3016 - accuracy: 0.9438 - val_loss: 0.4178 - val_accuracy: 0.8478\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2818 - accuracy: 0.9462 - val_loss: 0.4002 - val_accuracy: 0.8478\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2633 - accuracy: 0.9487 - val_loss: 0.3845 - val_accuracy: 0.8261\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2461 - accuracy: 0.9487 - val_loss: 0.3701 - val_accuracy: 0.8478\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2308 - accuracy: 0.9487 - val_loss: 0.3565 - val_accuracy: 0.8696\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2168 - accuracy: 0.9560 - val_loss: 0.3447 - val_accuracy: 0.8696\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2042 - accuracy: 0.9560 - val_loss: 0.3340 - val_accuracy: 0.8696\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1928 - accuracy: 0.9560 - val_loss: 0.3239 - val_accuracy: 0.8696\n",
            "4/4 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "params = {\n",
        "            'objective':'binary:logistic',\n",
        "            'max_depth': 7,\n",
        "            'alpha': 10,\n",
        "            'learning_rate': 1,\n",
        "            'n_estimators':100\n",
        "        }\n",
        "st = time.time()\n",
        "xgb = XGBClassifier(objective='binary:logistic',max_depth= 7,alpha= 10,learning_rate= 1,n_estimators=100)\n",
        "xgb.fit(X_train, y_train)\n",
        "xend = time.time() - st\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "XTr = accuracy_score(y_train, xgb.predict(X_train))\n",
        "XTe = accuracy_score(y_test, xgb.predict(X_test))\n",
        "XTr,XTe"
      ],
      "metadata": {
        "id": "PCP82YZ9RjgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f26e9d80-cefe-4211-e40a-bbfa4f944360"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.989010989010989, 0.9824561403508771)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "# Define the range of n_neighbors values to test\n",
        "n_neighbors_values = [1,3, 5, 7, 9, 11]\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_n_neighbors = None\n",
        "\n",
        "for n_neighbors in n_neighbors_values:\n",
        "    print(\"Number of Neighbors:\", n_neighbors)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, knn.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "    print('KNN model train accuracy score: {0:0.4f}'.format(train_accuracy))\n",
        "    print('KNN model test accuracy score: {0:0.4f}'.format(test_accuracy))\n",
        "    print()\n",
        "\n",
        "    # Check if the current test accuracy is better than the previous best\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_n_neighbors = n_neighbors\n",
        "print(\"best neighbours: \", best_n_neighbors)\n",
        "\n",
        "st = time.time()\n",
        "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "knn.fit(X_train, y_train)\n",
        "kend = time.time() - st\n",
        "KTr = accuracy_score(y_train, knn.predict(X_train))\n",
        "KTe = accuracy_score(y_test, knn.predict(X_test))\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "KTr,KTe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y_6b33A1imy",
        "outputId": "f6f89e81-6f29-4280-c236-2757dc069c6f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Neighbors: 1\n",
            "KNN model train accuracy score: 1.0000\n",
            "KNN model test accuracy score: 0.9737\n",
            "\n",
            "Number of Neighbors: 3\n",
            "KNN model train accuracy score: 0.9802\n",
            "KNN model test accuracy score: 0.9912\n",
            "\n",
            "Number of Neighbors: 5\n",
            "KNN model train accuracy score: 0.9692\n",
            "KNN model test accuracy score: 0.9825\n",
            "\n",
            "Number of Neighbors: 7\n",
            "KNN model train accuracy score: 0.9692\n",
            "KNN model test accuracy score: 0.9737\n",
            "\n",
            "Number of Neighbors: 9\n",
            "KNN model train accuracy score: 0.9692\n",
            "KNN model test accuracy score: 0.9825\n",
            "\n",
            "Number of Neighbors: 11\n",
            "KNN model train accuracy score: 0.9714\n",
            "KNN model test accuracy score: 0.9825\n",
            "\n",
            "best neighbours:  3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9802197802197802, 0.9912280701754386)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RF\n",
        "st = time.time()\n",
        "rf = RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123)\n",
        "rf.fit(X_train, y_train)\n",
        "rend = time.time() - st\n",
        "RTr = accuracy_score(y_train, rf.predict(X_train))\n",
        "RTe = accuracy_score(y_test, rf.predict(X_test))\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "RTr,RTe"
      ],
      "metadata": {
        "id": "k_kyk_E92bSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24efa756-1e27-4f59-cd79-dba4761eb403"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9758241758241758, 0.9912280701754386)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LR\n",
        "st = time.time()\n",
        "lr = LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123)\n",
        "lr.fit(X_train, y_train)\n",
        "lend = time.time() - st\n",
        "LTr = accuracy_score(y_train, lr.predict(X_train))\n",
        "LTe = accuracy_score(y_test, lr.predict(X_test))\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "LTr,LTe"
      ],
      "metadata": {
        "id": "OWkZmwL23Fm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a02530-42ac-47a6-efe5-99455af4c6df"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9802197802197802, 0.9736842105263158)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E9_l6r5f0w5n"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CVal(ML):\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/data.csv\")\n",
        "  del df['id']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "\n",
        "    ML.fit(X_train, y_train)\n",
        "    y_pred_ML = ML.predict(X_test)\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ML)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]+1\n",
        "\n",
        "  y_pred_final = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_final.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "XqaJZ1Mb0xAe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CValANN():\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/data.csv\")\n",
        "  del df['id']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "    tf.random.set_seed(123)\n",
        "    ANNmodel = Sequential()\n",
        "    ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    ANNmodel.add(Dense(10, activation='relu'))\n",
        "    ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "    ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "    ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "    y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ANN)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]\n",
        "\n",
        "  y_pred_fina = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_fina.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  y_pred_final = []\n",
        "\n",
        "  for i in range(len(y_pred_fina)):\n",
        "    y_pred_final.append(y_pred_fina[i][0])\n",
        "\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "jRa_a7EY0y6B"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = pd.DataFrame({\n",
        "    \"SVM\": CVal(SVC(C=0.1, gamma='auto', kernel = 'rbf'))\n",
        "})\n",
        "newdata[\"KNN\"] = CVal(KNeighborsClassifier(n_neighbors=3))\n",
        "newdata[\"RF\"]  = CVal(RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123))\n",
        "newdata['LR']  = CVal(LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123))\n",
        "newdata[\"ANN\"] = CValANN()\n",
        "newdata[\"XGB\"] = CVal(XGBClassifier(objective='binary:logistic',max_depth= 7,alpha= 10,learning_rate= 1,n_estimators=100))\n",
        "newdata['y_test'] = encoder.fit_transform(df['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CGgdO1V0zA9",
        "outputId": "968263d1-0c8a-40be-904a-8fdde9e111c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 48ms/step - loss: 0.6036 - accuracy: 0.7139 - val_loss: 0.5083 - val_accuracy: 0.8261\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5165 - accuracy: 0.7800 - val_loss: 0.4362 - val_accuracy: 0.8913\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4464 - accuracy: 0.8362 - val_loss: 0.3753 - val_accuracy: 0.9130\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3899 - accuracy: 0.8778 - val_loss: 0.3241 - val_accuracy: 0.9565\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3442 - accuracy: 0.9071 - val_loss: 0.2822 - val_accuracy: 0.9565\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3064 - accuracy: 0.9193 - val_loss: 0.2489 - val_accuracy: 0.9783\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2748 - accuracy: 0.9242 - val_loss: 0.2208 - val_accuracy: 0.9783\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2484 - accuracy: 0.9315 - val_loss: 0.1979 - val_accuracy: 0.9783\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2265 - accuracy: 0.9364 - val_loss: 0.1788 - val_accuracy: 0.9783\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2079 - accuracy: 0.9389 - val_loss: 0.1633 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1920 - accuracy: 0.9462 - val_loss: 0.1509 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1787 - accuracy: 0.9462 - val_loss: 0.1402 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1669 - accuracy: 0.9511 - val_loss: 0.1311 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1570 - accuracy: 0.9560 - val_loss: 0.1228 - val_accuracy: 0.9783\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1479 - accuracy: 0.9584 - val_loss: 0.1153 - val_accuracy: 0.9783\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1398 - accuracy: 0.9609 - val_loss: 0.1091 - val_accuracy: 0.9783\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1327 - accuracy: 0.9609 - val_loss: 0.1040 - val_accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1264 - accuracy: 0.9609 - val_loss: 0.0989 - val_accuracy: 0.9783\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1208 - accuracy: 0.9633 - val_loss: 0.0948 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1159 - accuracy: 0.9682 - val_loss: 0.0919 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 69ms/step - loss: 0.7357 - accuracy: 0.4719 - val_loss: 0.6677 - val_accuracy: 0.5652\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.6232 - accuracy: 0.6675 - val_loss: 0.5754 - val_accuracy: 0.6957\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.5334 - accuracy: 0.8117 - val_loss: 0.4951 - val_accuracy: 0.8913\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4594 - accuracy: 0.8704 - val_loss: 0.4294 - val_accuracy: 0.8913\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4021 - accuracy: 0.8900 - val_loss: 0.3757 - val_accuracy: 0.9348\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3567 - accuracy: 0.9046 - val_loss: 0.3318 - val_accuracy: 0.9130\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3217 - accuracy: 0.9144 - val_loss: 0.2955 - val_accuracy: 0.9348\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2919 - accuracy: 0.9144 - val_loss: 0.2662 - val_accuracy: 0.9565\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2686 - accuracy: 0.9169 - val_loss: 0.2408 - val_accuracy: 0.9565\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2479 - accuracy: 0.9242 - val_loss: 0.2194 - val_accuracy: 0.9565\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2300 - accuracy: 0.9267 - val_loss: 0.2016 - val_accuracy: 0.9565\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2151 - accuracy: 0.9267 - val_loss: 0.1870 - val_accuracy: 0.9783\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2019 - accuracy: 0.9315 - val_loss: 0.1751 - val_accuracy: 0.9783\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1900 - accuracy: 0.9364 - val_loss: 0.1649 - val_accuracy: 0.9783\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1793 - accuracy: 0.9364 - val_loss: 0.1569 - val_accuracy: 0.9783\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1700 - accuracy: 0.9413 - val_loss: 0.1500 - val_accuracy: 0.9783\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1613 - accuracy: 0.9438 - val_loss: 0.1438 - val_accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1535 - accuracy: 0.9462 - val_loss: 0.1379 - val_accuracy: 0.9783\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1467 - accuracy: 0.9487 - val_loss: 0.1329 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1402 - accuracy: 0.9535 - val_loss: 0.1289 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 53ms/step - loss: 0.8967 - accuracy: 0.2567 - val_loss: 0.8905 - val_accuracy: 0.1739\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7846 - accuracy: 0.3961 - val_loss: 0.7846 - val_accuracy: 0.2826\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6913 - accuracy: 0.5575 - val_loss: 0.6920 - val_accuracy: 0.5217\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6082 - accuracy: 0.7262 - val_loss: 0.6125 - val_accuracy: 0.7826\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5387 - accuracy: 0.8289 - val_loss: 0.5434 - val_accuracy: 0.8913\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.4802 - accuracy: 0.8778 - val_loss: 0.4831 - val_accuracy: 0.9348\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4316 - accuracy: 0.8949 - val_loss: 0.4297 - val_accuracy: 0.9348\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3894 - accuracy: 0.9095 - val_loss: 0.3851 - val_accuracy: 0.9348\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.3549 - accuracy: 0.9169 - val_loss: 0.3474 - val_accuracy: 0.9348\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3246 - accuracy: 0.9218 - val_loss: 0.3161 - val_accuracy: 0.9348\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2991 - accuracy: 0.9242 - val_loss: 0.2893 - val_accuracy: 0.9348\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2771 - accuracy: 0.9291 - val_loss: 0.2667 - val_accuracy: 0.9130\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2580 - accuracy: 0.9315 - val_loss: 0.2464 - val_accuracy: 0.9130\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2413 - accuracy: 0.9340 - val_loss: 0.2284 - val_accuracy: 0.9130\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2263 - accuracy: 0.9389 - val_loss: 0.2135 - val_accuracy: 0.9565\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2137 - accuracy: 0.9413 - val_loss: 0.2006 - val_accuracy: 0.9565\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2025 - accuracy: 0.9413 - val_loss: 0.1896 - val_accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1922 - accuracy: 0.9438 - val_loss: 0.1799 - val_accuracy: 0.9783\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1829 - accuracy: 0.9462 - val_loss: 0.1716 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1745 - accuracy: 0.9487 - val_loss: 0.1646 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 47ms/step - loss: 0.8397 - accuracy: 0.3619 - val_loss: 0.9485 - val_accuracy: 0.2174\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7851 - accuracy: 0.4132 - val_loss: 0.8855 - val_accuracy: 0.2826\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7351 - accuracy: 0.4841 - val_loss: 0.8270 - val_accuracy: 0.3043\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.6888 - accuracy: 0.6064 - val_loss: 0.7715 - val_accuracy: 0.3696\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.6439 - accuracy: 0.7042 - val_loss: 0.7179 - val_accuracy: 0.4783\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5997 - accuracy: 0.7555 - val_loss: 0.6663 - val_accuracy: 0.5652\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.5563 - accuracy: 0.7824 - val_loss: 0.6148 - val_accuracy: 0.6739\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5138 - accuracy: 0.8142 - val_loss: 0.5631 - val_accuracy: 0.7609\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4735 - accuracy: 0.8386 - val_loss: 0.5165 - val_accuracy: 0.8261\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4355 - accuracy: 0.8582 - val_loss: 0.4718 - val_accuracy: 0.8261\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.4013 - accuracy: 0.8729 - val_loss: 0.4293 - val_accuracy: 0.8696\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3700 - accuracy: 0.8851 - val_loss: 0.3919 - val_accuracy: 0.8696\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3418 - accuracy: 0.8924 - val_loss: 0.3572 - val_accuracy: 0.8913\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3160 - accuracy: 0.8998 - val_loss: 0.3263 - val_accuracy: 0.9130\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2928 - accuracy: 0.9095 - val_loss: 0.2996 - val_accuracy: 0.9348\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2719 - accuracy: 0.9291 - val_loss: 0.2767 - val_accuracy: 0.9348\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2537 - accuracy: 0.9315 - val_loss: 0.2560 - val_accuracy: 0.9565\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2368 - accuracy: 0.9364 - val_loss: 0.2376 - val_accuracy: 0.9565\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2218 - accuracy: 0.9389 - val_loss: 0.2214 - val_accuracy: 0.9565\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2076 - accuracy: 0.9438 - val_loss: 0.2078 - val_accuracy: 0.9565\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 49ms/step - loss: 0.7551 - accuracy: 0.4610 - val_loss: 0.7791 - val_accuracy: 0.3913\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6879 - accuracy: 0.5341 - val_loss: 0.7182 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.6292 - accuracy: 0.6024 - val_loss: 0.6667 - val_accuracy: 0.5652\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5811 - accuracy: 0.6780 - val_loss: 0.6242 - val_accuracy: 0.6087\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5410 - accuracy: 0.7439 - val_loss: 0.5883 - val_accuracy: 0.7391\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.5074 - accuracy: 0.7756 - val_loss: 0.5563 - val_accuracy: 0.7826\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.4785 - accuracy: 0.8195 - val_loss: 0.5276 - val_accuracy: 0.8696\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4533 - accuracy: 0.8488 - val_loss: 0.5008 - val_accuracy: 0.8913\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4304 - accuracy: 0.8756 - val_loss: 0.4757 - val_accuracy: 0.9130\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4081 - accuracy: 0.9000 - val_loss: 0.4524 - val_accuracy: 0.9130\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3873 - accuracy: 0.9146 - val_loss: 0.4305 - val_accuracy: 0.9348\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3673 - accuracy: 0.9244 - val_loss: 0.4094 - val_accuracy: 0.9348\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3479 - accuracy: 0.9293 - val_loss: 0.3887 - val_accuracy: 0.9348\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3286 - accuracy: 0.9317 - val_loss: 0.3687 - val_accuracy: 0.9348\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3100 - accuracy: 0.9415 - val_loss: 0.3490 - val_accuracy: 0.9348\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2920 - accuracy: 0.9415 - val_loss: 0.3294 - val_accuracy: 0.9348\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2744 - accuracy: 0.9463 - val_loss: 0.3106 - val_accuracy: 0.9348\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2572 - accuracy: 0.9488 - val_loss: 0.2926 - val_accuracy: 0.9348\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2407 - accuracy: 0.9537 - val_loss: 0.2756 - val_accuracy: 0.9348\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2249 - accuracy: 0.9561 - val_loss: 0.2594 - val_accuracy: 0.9348\n",
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newdata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "05ryfvyLzFVK",
        "outputId": "58287197-f3a3-4f4c-d22b-499ce23c3009"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SVM  KNN  RF  LR  ANN  XGB  y_test\n",
              "0    1    1   1   1    1    0       1\n",
              "1    1    1   1   1    1    1       1\n",
              "2    1    1   1   1    1    1       1\n",
              "3    1    1   1   1    1    1       1\n",
              "4    1    1   1   1    1    0       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fce07864-92d3-4004-bc42-c46a1cc6effe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SVM</th>\n",
              "      <th>KNN</th>\n",
              "      <th>RF</th>\n",
              "      <th>LR</th>\n",
              "      <th>ANN</th>\n",
              "      <th>XGB</th>\n",
              "      <th>y_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fce07864-92d3-4004-bc42-c46a1cc6effe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fce07864-92d3-4004-bc42-c46a1cc6effe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fce07864-92d3-4004-bc42-c46a1cc6effe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DNN model\n",
        "DNNX = newdata.drop(columns=['y_test']).copy()\n",
        "DNNY = newdata.y_test.copy()\n",
        "DX_train, DX_test, Dy_train, Dy_test = train_test_split(\n",
        "    DNNX, DNNY, test_size=0.2, random_state=123)\n",
        "\n",
        "st = time.time()\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(30, activation='relu', input_shape=DNNX.shape[1:]),\n",
        "    tf.keras.layers.Dense(25, activation='relu'),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dense(15, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(DX_train, Dy_train, epochs=500, batch_size=64, validation_split=0.2)\n",
        "dend = time.time() - st"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4kf97a3yX15",
        "outputId": "764357bb-e9cc-4955-a8c2-34657fc9d927"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 35ms/step - loss: 0.6856 - accuracy: 0.7885 - val_loss: 0.6807 - val_accuracy: 0.8681\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6779 - accuracy: 0.8819 - val_loss: 0.6652 - val_accuracy: 0.8681\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6603 - accuracy: 0.8736 - val_loss: 0.6400 - val_accuracy: 0.8681\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6392 - accuracy: 0.8736 - val_loss: 0.6161 - val_accuracy: 0.8681\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6185 - accuracy: 0.8736 - val_loss: 0.5906 - val_accuracy: 0.8681\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5955 - accuracy: 0.8736 - val_loss: 0.5636 - val_accuracy: 0.8681\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.5721 - accuracy: 0.8736 - val_loss: 0.5379 - val_accuracy: 0.8681\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5522 - accuracy: 0.8736 - val_loss: 0.5162 - val_accuracy: 0.8681\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5354 - accuracy: 0.8736 - val_loss: 0.5017 - val_accuracy: 0.8681\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5247 - accuracy: 0.8736 - val_loss: 0.4908 - val_accuracy: 0.8681\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5155 - accuracy: 0.8791 - val_loss: 0.4826 - val_accuracy: 0.8681\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5074 - accuracy: 0.8846 - val_loss: 0.4734 - val_accuracy: 0.8791\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4990 - accuracy: 0.9093 - val_loss: 0.4660 - val_accuracy: 0.8901\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4910 - accuracy: 0.9176 - val_loss: 0.4589 - val_accuracy: 0.8901\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4845 - accuracy: 0.9176 - val_loss: 0.4521 - val_accuracy: 0.8901\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.4790 - accuracy: 0.9176 - val_loss: 0.4459 - val_accuracy: 0.9011\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4716 - accuracy: 0.9341 - val_loss: 0.4374 - val_accuracy: 0.9011\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4629 - accuracy: 0.9313 - val_loss: 0.4278 - val_accuracy: 0.9011\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4542 - accuracy: 0.9341 - val_loss: 0.4187 - val_accuracy: 0.9011\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4450 - accuracy: 0.9396 - val_loss: 0.4086 - val_accuracy: 0.9011\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.4349 - accuracy: 0.9368 - val_loss: 0.3972 - val_accuracy: 0.9011\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4231 - accuracy: 0.9368 - val_loss: 0.3827 - val_accuracy: 0.9121\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4091 - accuracy: 0.9313 - val_loss: 0.3649 - val_accuracy: 0.9341\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3909 - accuracy: 0.9368 - val_loss: 0.3460 - val_accuracy: 0.9341\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3676 - accuracy: 0.9396 - val_loss: 0.3223 - val_accuracy: 0.9341\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3388 - accuracy: 0.9368 - val_loss: 0.2950 - val_accuracy: 0.9341\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.3064 - accuracy: 0.9341 - val_loss: 0.2646 - val_accuracy: 0.9341\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2696 - accuracy: 0.9341 - val_loss: 0.2381 - val_accuracy: 0.9341\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2370 - accuracy: 0.9341 - val_loss: 0.2198 - val_accuracy: 0.9341\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2102 - accuracy: 0.9341 - val_loss: 0.2150 - val_accuracy: 0.9341\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1979 - accuracy: 0.9396 - val_loss: 0.2200 - val_accuracy: 0.9341\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1945 - accuracy: 0.9396 - val_loss: 0.2253 - val_accuracy: 0.9341\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1914 - accuracy: 0.9396 - val_loss: 0.2305 - val_accuracy: 0.9341\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1889 - accuracy: 0.9396 - val_loss: 0.2338 - val_accuracy: 0.9341\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1867 - accuracy: 0.9396 - val_loss: 0.2338 - val_accuracy: 0.9231\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1848 - accuracy: 0.9451 - val_loss: 0.2359 - val_accuracy: 0.9231\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1817 - accuracy: 0.9451 - val_loss: 0.2372 - val_accuracy: 0.9231\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1813 - accuracy: 0.9451 - val_loss: 0.2384 - val_accuracy: 0.9231\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1797 - accuracy: 0.9451 - val_loss: 0.2411 - val_accuracy: 0.9231\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1803 - accuracy: 0.9451 - val_loss: 0.2434 - val_accuracy: 0.9231\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1793 - accuracy: 0.9451 - val_loss: 0.2419 - val_accuracy: 0.9231\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1779 - accuracy: 0.9478 - val_loss: 0.2456 - val_accuracy: 0.9231\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1777 - accuracy: 0.9451 - val_loss: 0.2488 - val_accuracy: 0.9231\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1772 - accuracy: 0.9451 - val_loss: 0.2450 - val_accuracy: 0.9231\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1756 - accuracy: 0.9478 - val_loss: 0.2460 - val_accuracy: 0.9231\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1757 - accuracy: 0.9505 - val_loss: 0.2480 - val_accuracy: 0.9231\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1745 - accuracy: 0.9505 - val_loss: 0.2511 - val_accuracy: 0.9231\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1744 - accuracy: 0.9478 - val_loss: 0.2523 - val_accuracy: 0.9231\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1740 - accuracy: 0.9478 - val_loss: 0.2519 - val_accuracy: 0.9231\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1735 - accuracy: 0.9505 - val_loss: 0.2516 - val_accuracy: 0.9231\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1728 - accuracy: 0.9505 - val_loss: 0.2516 - val_accuracy: 0.9231\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1724 - accuracy: 0.9505 - val_loss: 0.2527 - val_accuracy: 0.9231\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1720 - accuracy: 0.9505 - val_loss: 0.2521 - val_accuracy: 0.9231\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1729 - accuracy: 0.9505 - val_loss: 0.2506 - val_accuracy: 0.9231\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1723 - accuracy: 0.9478 - val_loss: 0.2537 - val_accuracy: 0.9231\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1713 - accuracy: 0.9505 - val_loss: 0.2560 - val_accuracy: 0.9231\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1707 - accuracy: 0.9505 - val_loss: 0.2546 - val_accuracy: 0.9231\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1714 - accuracy: 0.9505 - val_loss: 0.2546 - val_accuracy: 0.9231\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.9505 - val_loss: 0.2549 - val_accuracy: 0.9231\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1709 - accuracy: 0.9505 - val_loss: 0.2556 - val_accuracy: 0.9231\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1692 - accuracy: 0.9505 - val_loss: 0.2580 - val_accuracy: 0.9231\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1706 - accuracy: 0.9478 - val_loss: 0.2594 - val_accuracy: 0.9121\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1696 - accuracy: 0.9451 - val_loss: 0.2580 - val_accuracy: 0.9231\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1700 - accuracy: 0.9505 - val_loss: 0.2600 - val_accuracy: 0.9231\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1693 - accuracy: 0.9505 - val_loss: 0.2573 - val_accuracy: 0.9231\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1700 - accuracy: 0.9451 - val_loss: 0.2598 - val_accuracy: 0.9231\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1683 - accuracy: 0.9505 - val_loss: 0.2605 - val_accuracy: 0.9231\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1701 - accuracy: 0.9451 - val_loss: 0.2612 - val_accuracy: 0.9231\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1686 - accuracy: 0.9505 - val_loss: 0.2585 - val_accuracy: 0.9231\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1678 - accuracy: 0.9505 - val_loss: 0.2593 - val_accuracy: 0.9231\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1688 - accuracy: 0.9505 - val_loss: 0.2593 - val_accuracy: 0.9231\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1685 - accuracy: 0.9478 - val_loss: 0.2602 - val_accuracy: 0.9231\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1674 - accuracy: 0.9478 - val_loss: 0.2600 - val_accuracy: 0.9231\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1681 - accuracy: 0.9505 - val_loss: 0.2621 - val_accuracy: 0.9231\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1673 - accuracy: 0.9478 - val_loss: 0.2609 - val_accuracy: 0.9231\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1671 - accuracy: 0.9505 - val_loss: 0.2606 - val_accuracy: 0.9231\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1673 - accuracy: 0.9451 - val_loss: 0.2629 - val_accuracy: 0.9231\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1675 - accuracy: 0.9478 - val_loss: 0.2627 - val_accuracy: 0.9231\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1674 - accuracy: 0.9505 - val_loss: 0.2633 - val_accuracy: 0.9231\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1668 - accuracy: 0.9505 - val_loss: 0.2615 - val_accuracy: 0.9231\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1675 - accuracy: 0.9451 - val_loss: 0.2646 - val_accuracy: 0.9231\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1662 - accuracy: 0.9505 - val_loss: 0.2627 - val_accuracy: 0.9231\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1661 - accuracy: 0.9505 - val_loss: 0.2618 - val_accuracy: 0.9231\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1668 - accuracy: 0.9423 - val_loss: 0.2639 - val_accuracy: 0.9231\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1660 - accuracy: 0.9505 - val_loss: 0.2644 - val_accuracy: 0.9231\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1661 - accuracy: 0.9505 - val_loss: 0.2646 - val_accuracy: 0.9231\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1661 - accuracy: 0.9505 - val_loss: 0.2666 - val_accuracy: 0.9231\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1657 - accuracy: 0.9505 - val_loss: 0.2660 - val_accuracy: 0.9231\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1672 - accuracy: 0.9505 - val_loss: 0.2624 - val_accuracy: 0.9231\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1653 - accuracy: 0.9505 - val_loss: 0.2660 - val_accuracy: 0.9231\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1655 - accuracy: 0.9505 - val_loss: 0.2659 - val_accuracy: 0.9231\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1650 - accuracy: 0.9505 - val_loss: 0.2661 - val_accuracy: 0.9231\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1649 - accuracy: 0.9478 - val_loss: 0.2664 - val_accuracy: 0.9231\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1651 - accuracy: 0.9505 - val_loss: 0.2675 - val_accuracy: 0.9231\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1650 - accuracy: 0.9478 - val_loss: 0.2700 - val_accuracy: 0.9231\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1654 - accuracy: 0.9505 - val_loss: 0.2686 - val_accuracy: 0.9231\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1645 - accuracy: 0.9478 - val_loss: 0.2683 - val_accuracy: 0.9231\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1641 - accuracy: 0.9505 - val_loss: 0.2679 - val_accuracy: 0.9231\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1649 - accuracy: 0.9505 - val_loss: 0.2665 - val_accuracy: 0.9231\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1648 - accuracy: 0.9505 - val_loss: 0.2682 - val_accuracy: 0.9231\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1647 - accuracy: 0.9423 - val_loss: 0.2715 - val_accuracy: 0.9231\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1648 - accuracy: 0.9478 - val_loss: 0.2686 - val_accuracy: 0.9231\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1642 - accuracy: 0.9478 - val_loss: 0.2704 - val_accuracy: 0.9231\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1654 - accuracy: 0.9505 - val_loss: 0.2698 - val_accuracy: 0.9231\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1649 - accuracy: 0.9505 - val_loss: 0.2694 - val_accuracy: 0.9231\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1629 - accuracy: 0.9505 - val_loss: 0.2707 - val_accuracy: 0.9231\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1627 - accuracy: 0.9478 - val_loss: 0.2717 - val_accuracy: 0.9231\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1621 - accuracy: 0.9533 - val_loss: 0.2725 - val_accuracy: 0.9231\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1624 - accuracy: 0.9560 - val_loss: 0.2739 - val_accuracy: 0.9231\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1612 - accuracy: 0.9560 - val_loss: 0.2734 - val_accuracy: 0.9231\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1609 - accuracy: 0.9560 - val_loss: 0.2730 - val_accuracy: 0.9231\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1611 - accuracy: 0.9533 - val_loss: 0.2739 - val_accuracy: 0.9231\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1604 - accuracy: 0.9560 - val_loss: 0.2725 - val_accuracy: 0.9231\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1605 - accuracy: 0.9560 - val_loss: 0.2729 - val_accuracy: 0.9231\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1598 - accuracy: 0.9533 - val_loss: 0.2731 - val_accuracy: 0.9231\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1598 - accuracy: 0.9533 - val_loss: 0.2747 - val_accuracy: 0.9231\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1591 - accuracy: 0.9560 - val_loss: 0.2752 - val_accuracy: 0.9231\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1599 - accuracy: 0.9560 - val_loss: 0.2741 - val_accuracy: 0.9231\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1589 - accuracy: 0.9533 - val_loss: 0.2741 - val_accuracy: 0.9231\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1591 - accuracy: 0.9560 - val_loss: 0.2773 - val_accuracy: 0.9231\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1594 - accuracy: 0.9533 - val_loss: 0.2764 - val_accuracy: 0.9231\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1584 - accuracy: 0.9560 - val_loss: 0.2765 - val_accuracy: 0.9231\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1591 - accuracy: 0.9560 - val_loss: 0.2743 - val_accuracy: 0.9231\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1584 - accuracy: 0.9533 - val_loss: 0.2746 - val_accuracy: 0.9231\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1589 - accuracy: 0.9560 - val_loss: 0.2747 - val_accuracy: 0.9231\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1583 - accuracy: 0.9533 - val_loss: 0.2761 - val_accuracy: 0.9231\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1577 - accuracy: 0.9560 - val_loss: 0.2768 - val_accuracy: 0.9231\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1576 - accuracy: 0.9533 - val_loss: 0.2778 - val_accuracy: 0.9231\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1587 - accuracy: 0.9533 - val_loss: 0.2787 - val_accuracy: 0.9231\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1581 - accuracy: 0.9560 - val_loss: 0.2786 - val_accuracy: 0.9231\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1585 - accuracy: 0.9560 - val_loss: 0.2777 - val_accuracy: 0.9231\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1572 - accuracy: 0.9560 - val_loss: 0.2783 - val_accuracy: 0.9231\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1571 - accuracy: 0.9560 - val_loss: 0.2813 - val_accuracy: 0.9231\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1563 - accuracy: 0.9588 - val_loss: 0.2807 - val_accuracy: 0.9231\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1559 - accuracy: 0.9588 - val_loss: 0.2802 - val_accuracy: 0.9231\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1560 - accuracy: 0.9588 - val_loss: 0.2810 - val_accuracy: 0.9231\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1557 - accuracy: 0.9588 - val_loss: 0.2822 - val_accuracy: 0.9231\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1555 - accuracy: 0.9560 - val_loss: 0.2812 - val_accuracy: 0.9231\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1551 - accuracy: 0.9588 - val_loss: 0.2807 - val_accuracy: 0.9231\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1545 - accuracy: 0.9588 - val_loss: 0.2838 - val_accuracy: 0.9231\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1546 - accuracy: 0.9560 - val_loss: 0.2858 - val_accuracy: 0.9231\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1553 - accuracy: 0.9615 - val_loss: 0.2875 - val_accuracy: 0.9231\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1547 - accuracy: 0.9588 - val_loss: 0.2865 - val_accuracy: 0.9231\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1554 - accuracy: 0.9588 - val_loss: 0.2830 - val_accuracy: 0.9231\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1539 - accuracy: 0.9588 - val_loss: 0.2851 - val_accuracy: 0.9231\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1564 - accuracy: 0.9588 - val_loss: 0.2858 - val_accuracy: 0.9231\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1544 - accuracy: 0.9560 - val_loss: 0.2825 - val_accuracy: 0.9231\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1538 - accuracy: 0.9588 - val_loss: 0.2850 - val_accuracy: 0.9231\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1538 - accuracy: 0.9588 - val_loss: 0.2851 - val_accuracy: 0.9231\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1535 - accuracy: 0.9560 - val_loss: 0.2854 - val_accuracy: 0.9231\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1533 - accuracy: 0.9588 - val_loss: 0.2864 - val_accuracy: 0.9231\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1538 - accuracy: 0.9588 - val_loss: 0.2838 - val_accuracy: 0.9231\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1530 - accuracy: 0.9588 - val_loss: 0.2849 - val_accuracy: 0.9231\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1533 - accuracy: 0.9533 - val_loss: 0.2849 - val_accuracy: 0.9231\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1529 - accuracy: 0.9560 - val_loss: 0.2860 - val_accuracy: 0.9231\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1529 - accuracy: 0.9588 - val_loss: 0.2891 - val_accuracy: 0.9231\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1531 - accuracy: 0.9560 - val_loss: 0.2893 - val_accuracy: 0.9231\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1538 - accuracy: 0.9533 - val_loss: 0.2881 - val_accuracy: 0.9231\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1537 - accuracy: 0.9533 - val_loss: 0.2884 - val_accuracy: 0.9231\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1528 - accuracy: 0.9560 - val_loss: 0.2891 - val_accuracy: 0.9231\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1534 - accuracy: 0.9588 - val_loss: 0.2886 - val_accuracy: 0.9231\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1554 - accuracy: 0.9588 - val_loss: 0.2933 - val_accuracy: 0.9231\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1543 - accuracy: 0.9533 - val_loss: 0.2862 - val_accuracy: 0.9231\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1526 - accuracy: 0.9588 - val_loss: 0.2870 - val_accuracy: 0.9231\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1527 - accuracy: 0.9560 - val_loss: 0.2871 - val_accuracy: 0.9231\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1520 - accuracy: 0.9588 - val_loss: 0.2888 - val_accuracy: 0.9231\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1520 - accuracy: 0.9560 - val_loss: 0.2898 - val_accuracy: 0.9231\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1521 - accuracy: 0.9588 - val_loss: 0.2884 - val_accuracy: 0.9231\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1526 - accuracy: 0.9588 - val_loss: 0.2873 - val_accuracy: 0.9231\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1520 - accuracy: 0.9560 - val_loss: 0.2908 - val_accuracy: 0.9231\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1523 - accuracy: 0.9560 - val_loss: 0.2941 - val_accuracy: 0.9231\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1522 - accuracy: 0.9533 - val_loss: 0.2914 - val_accuracy: 0.9231\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1521 - accuracy: 0.9560 - val_loss: 0.2919 - val_accuracy: 0.9231\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1537 - accuracy: 0.9560 - val_loss: 0.2903 - val_accuracy: 0.9231\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1526 - accuracy: 0.9533 - val_loss: 0.2935 - val_accuracy: 0.9231\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1517 - accuracy: 0.9560 - val_loss: 0.2932 - val_accuracy: 0.9231\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1515 - accuracy: 0.9588 - val_loss: 0.2909 - val_accuracy: 0.9231\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1509 - accuracy: 0.9588 - val_loss: 0.2897 - val_accuracy: 0.9231\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1515 - accuracy: 0.9588 - val_loss: 0.2899 - val_accuracy: 0.9231\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1519 - accuracy: 0.9588 - val_loss: 0.2944 - val_accuracy: 0.9231\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1517 - accuracy: 0.9588 - val_loss: 0.2924 - val_accuracy: 0.9231\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1512 - accuracy: 0.9560 - val_loss: 0.2920 - val_accuracy: 0.9231\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1513 - accuracy: 0.9588 - val_loss: 0.2914 - val_accuracy: 0.9231\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1511 - accuracy: 0.9560 - val_loss: 0.2911 - val_accuracy: 0.9231\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1515 - accuracy: 0.9560 - val_loss: 0.2924 - val_accuracy: 0.9231\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1502 - accuracy: 0.9615 - val_loss: 0.2930 - val_accuracy: 0.9231\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1527 - accuracy: 0.9588 - val_loss: 0.2959 - val_accuracy: 0.9231\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1529 - accuracy: 0.9588 - val_loss: 0.2956 - val_accuracy: 0.9231\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1514 - accuracy: 0.9588 - val_loss: 0.2961 - val_accuracy: 0.9231\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1514 - accuracy: 0.9560 - val_loss: 0.2921 - val_accuracy: 0.9231\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1508 - accuracy: 0.9588 - val_loss: 0.2948 - val_accuracy: 0.9231\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1514 - accuracy: 0.9588 - val_loss: 0.2971 - val_accuracy: 0.9231\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1508 - accuracy: 0.9588 - val_loss: 0.2970 - val_accuracy: 0.9231\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1514 - accuracy: 0.9588 - val_loss: 0.2948 - val_accuracy: 0.9231\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1509 - accuracy: 0.9588 - val_loss: 0.2961 - val_accuracy: 0.9231\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1522 - accuracy: 0.9588 - val_loss: 0.2965 - val_accuracy: 0.9231\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1521 - accuracy: 0.9533 - val_loss: 0.2986 - val_accuracy: 0.9231\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1505 - accuracy: 0.9588 - val_loss: 0.2970 - val_accuracy: 0.9231\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1509 - accuracy: 0.9588 - val_loss: 0.2953 - val_accuracy: 0.9231\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1506 - accuracy: 0.9588 - val_loss: 0.2969 - val_accuracy: 0.9231\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1505 - accuracy: 0.9588 - val_loss: 0.2969 - val_accuracy: 0.9231\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1508 - accuracy: 0.9560 - val_loss: 0.2960 - val_accuracy: 0.9231\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1505 - accuracy: 0.9560 - val_loss: 0.2970 - val_accuracy: 0.9231\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1514 - accuracy: 0.9560 - val_loss: 0.2974 - val_accuracy: 0.9231\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1510 - accuracy: 0.9533 - val_loss: 0.2976 - val_accuracy: 0.9231\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1512 - accuracy: 0.9533 - val_loss: 0.2987 - val_accuracy: 0.9231\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1500 - accuracy: 0.9588 - val_loss: 0.2983 - val_accuracy: 0.9231\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1503 - accuracy: 0.9588 - val_loss: 0.2984 - val_accuracy: 0.9231\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1504 - accuracy: 0.9560 - val_loss: 0.2979 - val_accuracy: 0.9231\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1500 - accuracy: 0.9588 - val_loss: 0.2972 - val_accuracy: 0.9231\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1517 - accuracy: 0.9560 - val_loss: 0.2966 - val_accuracy: 0.9231\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1497 - accuracy: 0.9615 - val_loss: 0.2972 - val_accuracy: 0.9231\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1513 - accuracy: 0.9588 - val_loss: 0.2981 - val_accuracy: 0.9231\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1496 - accuracy: 0.9588 - val_loss: 0.2998 - val_accuracy: 0.9231\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1515 - accuracy: 0.9588 - val_loss: 0.3046 - val_accuracy: 0.9231\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1504 - accuracy: 0.9588 - val_loss: 0.3028 - val_accuracy: 0.9231\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1502 - accuracy: 0.9588 - val_loss: 0.3022 - val_accuracy: 0.9231\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1510 - accuracy: 0.9560 - val_loss: 0.3028 - val_accuracy: 0.9231\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1505 - accuracy: 0.9588 - val_loss: 0.3013 - val_accuracy: 0.9231\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1502 - accuracy: 0.9588 - val_loss: 0.3000 - val_accuracy: 0.9231\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1510 - accuracy: 0.9588 - val_loss: 0.3012 - val_accuracy: 0.9231\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1516 - accuracy: 0.9533 - val_loss: 0.2992 - val_accuracy: 0.9231\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1498 - accuracy: 0.9588 - val_loss: 0.3004 - val_accuracy: 0.9231\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1500 - accuracy: 0.9588 - val_loss: 0.3006 - val_accuracy: 0.9231\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1500 - accuracy: 0.9588 - val_loss: 0.2999 - val_accuracy: 0.9231\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1500 - accuracy: 0.9588 - val_loss: 0.3015 - val_accuracy: 0.9231\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1498 - accuracy: 0.9533 - val_loss: 0.3015 - val_accuracy: 0.9231\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1499 - accuracy: 0.9588 - val_loss: 0.3004 - val_accuracy: 0.9231\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1500 - accuracy: 0.9615 - val_loss: 0.3025 - val_accuracy: 0.9231\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1501 - accuracy: 0.9588 - val_loss: 0.3025 - val_accuracy: 0.9231\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1500 - accuracy: 0.9533 - val_loss: 0.3033 - val_accuracy: 0.9231\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1495 - accuracy: 0.9588 - val_loss: 0.3023 - val_accuracy: 0.9231\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1501 - accuracy: 0.9588 - val_loss: 0.3019 - val_accuracy: 0.9231\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1502 - accuracy: 0.9560 - val_loss: 0.3051 - val_accuracy: 0.9231\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1494 - accuracy: 0.9560 - val_loss: 0.3033 - val_accuracy: 0.9231\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1496 - accuracy: 0.9533 - val_loss: 0.3029 - val_accuracy: 0.9231\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1503 - accuracy: 0.9560 - val_loss: 0.3018 - val_accuracy: 0.9231\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1495 - accuracy: 0.9560 - val_loss: 0.3019 - val_accuracy: 0.9231\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1494 - accuracy: 0.9588 - val_loss: 0.3022 - val_accuracy: 0.9231\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1496 - accuracy: 0.9560 - val_loss: 0.3029 - val_accuracy: 0.9231\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1497 - accuracy: 0.9560 - val_loss: 0.3027 - val_accuracy: 0.9231\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1503 - accuracy: 0.9560 - val_loss: 0.3026 - val_accuracy: 0.9231\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1501 - accuracy: 0.9533 - val_loss: 0.3038 - val_accuracy: 0.9231\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1497 - accuracy: 0.9533 - val_loss: 0.3058 - val_accuracy: 0.9231\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1496 - accuracy: 0.9588 - val_loss: 0.3058 - val_accuracy: 0.9231\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1501 - accuracy: 0.9533 - val_loss: 0.3062 - val_accuracy: 0.9231\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1503 - accuracy: 0.9560 - val_loss: 0.3063 - val_accuracy: 0.9231\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1490 - accuracy: 0.9588 - val_loss: 0.3041 - val_accuracy: 0.9231\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1499 - accuracy: 0.9588 - val_loss: 0.3046 - val_accuracy: 0.9231\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1508 - accuracy: 0.9560 - val_loss: 0.3030 - val_accuracy: 0.9231\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1493 - accuracy: 0.9588 - val_loss: 0.3047 - val_accuracy: 0.9231\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9560 - val_loss: 0.3060 - val_accuracy: 0.9231\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1496 - accuracy: 0.9588 - val_loss: 0.3056 - val_accuracy: 0.9231\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1501 - accuracy: 0.9533 - val_loss: 0.3067 - val_accuracy: 0.9231\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1496 - accuracy: 0.9588 - val_loss: 0.3085 - val_accuracy: 0.9231\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1502 - accuracy: 0.9588 - val_loss: 0.3092 - val_accuracy: 0.9231\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1497 - accuracy: 0.9505 - val_loss: 0.3069 - val_accuracy: 0.9231\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1500 - accuracy: 0.9588 - val_loss: 0.3064 - val_accuracy: 0.9231\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1495 - accuracy: 0.9533 - val_loss: 0.3099 - val_accuracy: 0.9231\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1500 - accuracy: 0.9560 - val_loss: 0.3095 - val_accuracy: 0.9231\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1489 - accuracy: 0.9588 - val_loss: 0.3075 - val_accuracy: 0.9231\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1492 - accuracy: 0.9588 - val_loss: 0.3072 - val_accuracy: 0.9231\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1493 - accuracy: 0.9560 - val_loss: 0.3075 - val_accuracy: 0.9231\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1490 - accuracy: 0.9560 - val_loss: 0.3058 - val_accuracy: 0.9231\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1496 - accuracy: 0.9560 - val_loss: 0.3070 - val_accuracy: 0.9231\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1490 - accuracy: 0.9560 - val_loss: 0.3064 - val_accuracy: 0.9231\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1488 - accuracy: 0.9560 - val_loss: 0.3064 - val_accuracy: 0.9231\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1498 - accuracy: 0.9560 - val_loss: 0.3080 - val_accuracy: 0.9231\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1495 - accuracy: 0.9560 - val_loss: 0.3096 - val_accuracy: 0.9231\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1490 - accuracy: 0.9588 - val_loss: 0.3077 - val_accuracy: 0.9231\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1488 - accuracy: 0.9588 - val_loss: 0.3084 - val_accuracy: 0.9231\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1490 - accuracy: 0.9560 - val_loss: 0.3087 - val_accuracy: 0.9231\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1490 - accuracy: 0.9588 - val_loss: 0.3102 - val_accuracy: 0.9231\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1490 - accuracy: 0.9560 - val_loss: 0.3094 - val_accuracy: 0.9231\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1494 - accuracy: 0.9560 - val_loss: 0.3100 - val_accuracy: 0.9231\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3098 - val_accuracy: 0.9231\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1492 - accuracy: 0.9588 - val_loss: 0.3089 - val_accuracy: 0.9231\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1488 - accuracy: 0.9588 - val_loss: 0.3093 - val_accuracy: 0.9231\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1494 - accuracy: 0.9588 - val_loss: 0.3106 - val_accuracy: 0.9231\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1504 - accuracy: 0.9560 - val_loss: 0.3120 - val_accuracy: 0.9231\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1494 - accuracy: 0.9588 - val_loss: 0.3106 - val_accuracy: 0.9231\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1490 - accuracy: 0.9533 - val_loss: 0.3123 - val_accuracy: 0.9231\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1490 - accuracy: 0.9560 - val_loss: 0.3114 - val_accuracy: 0.9231\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1492 - accuracy: 0.9560 - val_loss: 0.3125 - val_accuracy: 0.9231\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1489 - accuracy: 0.9560 - val_loss: 0.3134 - val_accuracy: 0.9231\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.9588 - val_loss: 0.3155 - val_accuracy: 0.9231\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1497 - accuracy: 0.9588 - val_loss: 0.3153 - val_accuracy: 0.9231\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1490 - accuracy: 0.9588 - val_loss: 0.3143 - val_accuracy: 0.9231\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1507 - accuracy: 0.9588 - val_loss: 0.3172 - val_accuracy: 0.9231\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3145 - val_accuracy: 0.9231\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1502 - accuracy: 0.9588 - val_loss: 0.3133 - val_accuracy: 0.9231\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1511 - accuracy: 0.9560 - val_loss: 0.3167 - val_accuracy: 0.9231\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1505 - accuracy: 0.9588 - val_loss: 0.3149 - val_accuracy: 0.9231\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1495 - accuracy: 0.9560 - val_loss: 0.3113 - val_accuracy: 0.9231\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1494 - accuracy: 0.9560 - val_loss: 0.3130 - val_accuracy: 0.9231\n",
            "Epoch 296/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1493 - accuracy: 0.9560 - val_loss: 0.3144 - val_accuracy: 0.9231\n",
            "Epoch 297/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1494 - accuracy: 0.9560 - val_loss: 0.3134 - val_accuracy: 0.9231\n",
            "Epoch 298/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1495 - accuracy: 0.9588 - val_loss: 0.3143 - val_accuracy: 0.9231\n",
            "Epoch 299/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1495 - accuracy: 0.9588 - val_loss: 0.3126 - val_accuracy: 0.9231\n",
            "Epoch 300/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1493 - accuracy: 0.9560 - val_loss: 0.3124 - val_accuracy: 0.9231\n",
            "Epoch 301/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1488 - accuracy: 0.9588 - val_loss: 0.3149 - val_accuracy: 0.9231\n",
            "Epoch 302/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.9588 - val_loss: 0.3158 - val_accuracy: 0.9231\n",
            "Epoch 303/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1487 - accuracy: 0.9588 - val_loss: 0.3155 - val_accuracy: 0.9231\n",
            "Epoch 304/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1486 - accuracy: 0.9560 - val_loss: 0.3150 - val_accuracy: 0.9231\n",
            "Epoch 305/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1492 - accuracy: 0.9588 - val_loss: 0.3174 - val_accuracy: 0.9231\n",
            "Epoch 306/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1489 - accuracy: 0.9560 - val_loss: 0.3152 - val_accuracy: 0.9231\n",
            "Epoch 307/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1502 - accuracy: 0.9560 - val_loss: 0.3137 - val_accuracy: 0.9231\n",
            "Epoch 308/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1493 - accuracy: 0.9560 - val_loss: 0.3208 - val_accuracy: 0.9231\n",
            "Epoch 309/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1505 - accuracy: 0.9588 - val_loss: 0.3168 - val_accuracy: 0.9231\n",
            "Epoch 310/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1494 - accuracy: 0.9588 - val_loss: 0.3145 - val_accuracy: 0.9231\n",
            "Epoch 311/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1488 - accuracy: 0.9588 - val_loss: 0.3118 - val_accuracy: 0.9231\n",
            "Epoch 312/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1497 - accuracy: 0.9560 - val_loss: 0.3149 - val_accuracy: 0.9231\n",
            "Epoch 313/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1494 - accuracy: 0.9588 - val_loss: 0.3163 - val_accuracy: 0.9231\n",
            "Epoch 314/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1501 - accuracy: 0.9505 - val_loss: 0.3167 - val_accuracy: 0.9231\n",
            "Epoch 315/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1493 - accuracy: 0.9588 - val_loss: 0.3159 - val_accuracy: 0.9231\n",
            "Epoch 316/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1483 - accuracy: 0.9560 - val_loss: 0.3155 - val_accuracy: 0.9231\n",
            "Epoch 317/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3160 - val_accuracy: 0.9231\n",
            "Epoch 318/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9533 - val_loss: 0.3158 - val_accuracy: 0.9231\n",
            "Epoch 319/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.9560 - val_loss: 0.3169 - val_accuracy: 0.9231\n",
            "Epoch 320/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1505 - accuracy: 0.9533 - val_loss: 0.3190 - val_accuracy: 0.9231\n",
            "Epoch 321/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9560 - val_loss: 0.3189 - val_accuracy: 0.9231\n",
            "Epoch 322/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1495 - accuracy: 0.9588 - val_loss: 0.3179 - val_accuracy: 0.9231\n",
            "Epoch 323/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9560 - val_loss: 0.3177 - val_accuracy: 0.9231\n",
            "Epoch 324/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1493 - accuracy: 0.9588 - val_loss: 0.3184 - val_accuracy: 0.9231\n",
            "Epoch 325/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1492 - accuracy: 0.9560 - val_loss: 0.3168 - val_accuracy: 0.9231\n",
            "Epoch 326/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1485 - accuracy: 0.9560 - val_loss: 0.3179 - val_accuracy: 0.9231\n",
            "Epoch 327/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1486 - accuracy: 0.9533 - val_loss: 0.3181 - val_accuracy: 0.9231\n",
            "Epoch 328/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1483 - accuracy: 0.9588 - val_loss: 0.3183 - val_accuracy: 0.9231\n",
            "Epoch 329/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1483 - accuracy: 0.9560 - val_loss: 0.3184 - val_accuracy: 0.9231\n",
            "Epoch 330/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3181 - val_accuracy: 0.9231\n",
            "Epoch 331/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.9560 - val_loss: 0.3209 - val_accuracy: 0.9231\n",
            "Epoch 332/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1502 - accuracy: 0.9533 - val_loss: 0.3204 - val_accuracy: 0.9231\n",
            "Epoch 333/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9560 - val_loss: 0.3198 - val_accuracy: 0.9231\n",
            "Epoch 334/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3195 - val_accuracy: 0.9231\n",
            "Epoch 335/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1494 - accuracy: 0.9533 - val_loss: 0.3238 - val_accuracy: 0.9231\n",
            "Epoch 336/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1489 - accuracy: 0.9560 - val_loss: 0.3204 - val_accuracy: 0.9231\n",
            "Epoch 337/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1486 - accuracy: 0.9560 - val_loss: 0.3225 - val_accuracy: 0.9231\n",
            "Epoch 338/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3225 - val_accuracy: 0.9231\n",
            "Epoch 339/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3215 - val_accuracy: 0.9231\n",
            "Epoch 340/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3206 - val_accuracy: 0.9231\n",
            "Epoch 341/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1490 - accuracy: 0.9588 - val_loss: 0.3205 - val_accuracy: 0.9231\n",
            "Epoch 342/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1489 - accuracy: 0.9560 - val_loss: 0.3226 - val_accuracy: 0.9231\n",
            "Epoch 343/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3232 - val_accuracy: 0.9231\n",
            "Epoch 344/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9533 - val_loss: 0.3221 - val_accuracy: 0.9231\n",
            "Epoch 345/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1484 - accuracy: 0.9560 - val_loss: 0.3213 - val_accuracy: 0.9231\n",
            "Epoch 346/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3219 - val_accuracy: 0.9231\n",
            "Epoch 347/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9588 - val_loss: 0.3224 - val_accuracy: 0.9231\n",
            "Epoch 348/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1487 - accuracy: 0.9588 - val_loss: 0.3213 - val_accuracy: 0.9231\n",
            "Epoch 349/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1488 - accuracy: 0.9588 - val_loss: 0.3232 - val_accuracy: 0.9231\n",
            "Epoch 350/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1488 - accuracy: 0.9588 - val_loss: 0.3217 - val_accuracy: 0.9231\n",
            "Epoch 351/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1486 - accuracy: 0.9560 - val_loss: 0.3240 - val_accuracy: 0.9231\n",
            "Epoch 352/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1491 - accuracy: 0.9588 - val_loss: 0.3271 - val_accuracy: 0.9231\n",
            "Epoch 353/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3258 - val_accuracy: 0.9231\n",
            "Epoch 354/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.9588 - val_loss: 0.3287 - val_accuracy: 0.9231\n",
            "Epoch 355/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1498 - accuracy: 0.9588 - val_loss: 0.3285 - val_accuracy: 0.9231\n",
            "Epoch 356/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3244 - val_accuracy: 0.9231\n",
            "Epoch 357/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1502 - accuracy: 0.9560 - val_loss: 0.3265 - val_accuracy: 0.9231\n",
            "Epoch 358/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1482 - accuracy: 0.9560 - val_loss: 0.3255 - val_accuracy: 0.9231\n",
            "Epoch 359/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3257 - val_accuracy: 0.9231\n",
            "Epoch 360/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1492 - accuracy: 0.9588 - val_loss: 0.3243 - val_accuracy: 0.9231\n",
            "Epoch 361/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3255 - val_accuracy: 0.9231\n",
            "Epoch 362/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3252 - val_accuracy: 0.9231\n",
            "Epoch 363/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3261 - val_accuracy: 0.9231\n",
            "Epoch 364/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9588 - val_loss: 0.3281 - val_accuracy: 0.9231\n",
            "Epoch 365/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1504 - accuracy: 0.9533 - val_loss: 0.3285 - val_accuracy: 0.9231\n",
            "Epoch 366/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1494 - accuracy: 0.9560 - val_loss: 0.3254 - val_accuracy: 0.9231\n",
            "Epoch 367/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3230 - val_accuracy: 0.9231\n",
            "Epoch 368/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1486 - accuracy: 0.9560 - val_loss: 0.3229 - val_accuracy: 0.9231\n",
            "Epoch 369/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9560 - val_loss: 0.3258 - val_accuracy: 0.9231\n",
            "Epoch 370/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9588 - val_loss: 0.3273 - val_accuracy: 0.9231\n",
            "Epoch 371/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1482 - accuracy: 0.9588 - val_loss: 0.3271 - val_accuracy: 0.9231\n",
            "Epoch 372/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1496 - accuracy: 0.9588 - val_loss: 0.3276 - val_accuracy: 0.9231\n",
            "Epoch 373/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1472 - accuracy: 0.9588 - val_loss: 0.3320 - val_accuracy: 0.9231\n",
            "Epoch 374/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9588 - val_loss: 0.3331 - val_accuracy: 0.9231\n",
            "Epoch 375/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3278 - val_accuracy: 0.9231\n",
            "Epoch 376/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1480 - accuracy: 0.9588 - val_loss: 0.3268 - val_accuracy: 0.9231\n",
            "Epoch 377/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1492 - accuracy: 0.9588 - val_loss: 0.3262 - val_accuracy: 0.9231\n",
            "Epoch 378/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1482 - accuracy: 0.9560 - val_loss: 0.3267 - val_accuracy: 0.9231\n",
            "Epoch 379/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1479 - accuracy: 0.9588 - val_loss: 0.3285 - val_accuracy: 0.9231\n",
            "Epoch 380/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1490 - accuracy: 0.9533 - val_loss: 0.3292 - val_accuracy: 0.9231\n",
            "Epoch 381/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1483 - accuracy: 0.9560 - val_loss: 0.3283 - val_accuracy: 0.9231\n",
            "Epoch 382/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1482 - accuracy: 0.9560 - val_loss: 0.3302 - val_accuracy: 0.9231\n",
            "Epoch 383/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3290 - val_accuracy: 0.9231\n",
            "Epoch 384/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1510 - accuracy: 0.9505 - val_loss: 0.3301 - val_accuracy: 0.9231\n",
            "Epoch 385/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1502 - accuracy: 0.9560 - val_loss: 0.3295 - val_accuracy: 0.9231\n",
            "Epoch 386/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1479 - accuracy: 0.9560 - val_loss: 0.3300 - val_accuracy: 0.9231\n",
            "Epoch 387/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3296 - val_accuracy: 0.9231\n",
            "Epoch 388/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1485 - accuracy: 0.9560 - val_loss: 0.3321 - val_accuracy: 0.9231\n",
            "Epoch 389/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3327 - val_accuracy: 0.9231\n",
            "Epoch 390/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1485 - accuracy: 0.9560 - val_loss: 0.3326 - val_accuracy: 0.9231\n",
            "Epoch 391/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3337 - val_accuracy: 0.9231\n",
            "Epoch 392/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3313 - val_accuracy: 0.9231\n",
            "Epoch 393/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1480 - accuracy: 0.9588 - val_loss: 0.3292 - val_accuracy: 0.9231\n",
            "Epoch 394/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1486 - accuracy: 0.9560 - val_loss: 0.3298 - val_accuracy: 0.9231\n",
            "Epoch 395/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1507 - accuracy: 0.9533 - val_loss: 0.3335 - val_accuracy: 0.9231\n",
            "Epoch 396/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1511 - accuracy: 0.9588 - val_loss: 0.3351 - val_accuracy: 0.9231\n",
            "Epoch 397/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1481 - accuracy: 0.9560 - val_loss: 0.3299 - val_accuracy: 0.9231\n",
            "Epoch 398/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3306 - val_accuracy: 0.9231\n",
            "Epoch 399/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1487 - accuracy: 0.9588 - val_loss: 0.3315 - val_accuracy: 0.9231\n",
            "Epoch 400/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3318 - val_accuracy: 0.9231\n",
            "Epoch 401/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.9588 - val_loss: 0.3317 - val_accuracy: 0.9231\n",
            "Epoch 402/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1490 - accuracy: 0.9588 - val_loss: 0.3317 - val_accuracy: 0.9231\n",
            "Epoch 403/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3318 - val_accuracy: 0.9231\n",
            "Epoch 404/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1482 - accuracy: 0.9560 - val_loss: 0.3323 - val_accuracy: 0.9231\n",
            "Epoch 405/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1490 - accuracy: 0.9588 - val_loss: 0.3332 - val_accuracy: 0.9231\n",
            "Epoch 406/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1482 - accuracy: 0.9588 - val_loss: 0.3332 - val_accuracy: 0.9231\n",
            "Epoch 407/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1489 - accuracy: 0.9560 - val_loss: 0.3350 - val_accuracy: 0.9231\n",
            "Epoch 408/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3331 - val_accuracy: 0.9231\n",
            "Epoch 409/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1494 - accuracy: 0.9560 - val_loss: 0.3341 - val_accuracy: 0.9231\n",
            "Epoch 410/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1485 - accuracy: 0.9533 - val_loss: 0.3323 - val_accuracy: 0.9231\n",
            "Epoch 411/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3334 - val_accuracy: 0.9231\n",
            "Epoch 412/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3321 - val_accuracy: 0.9231\n",
            "Epoch 413/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9560 - val_loss: 0.3328 - val_accuracy: 0.9231\n",
            "Epoch 414/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1486 - accuracy: 0.9560 - val_loss: 0.3320 - val_accuracy: 0.9231\n",
            "Epoch 415/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.9560 - val_loss: 0.3318 - val_accuracy: 0.9231\n",
            "Epoch 416/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1484 - accuracy: 0.9560 - val_loss: 0.3329 - val_accuracy: 0.9231\n",
            "Epoch 417/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1484 - accuracy: 0.9560 - val_loss: 0.3348 - val_accuracy: 0.9231\n",
            "Epoch 418/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9533 - val_loss: 0.3355 - val_accuracy: 0.9231\n",
            "Epoch 419/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9588 - val_loss: 0.3332 - val_accuracy: 0.9231\n",
            "Epoch 420/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1477 - accuracy: 0.9588 - val_loss: 0.3329 - val_accuracy: 0.9231\n",
            "Epoch 421/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1479 - accuracy: 0.9588 - val_loss: 0.3351 - val_accuracy: 0.9231\n",
            "Epoch 422/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1481 - accuracy: 0.9533 - val_loss: 0.3361 - val_accuracy: 0.9231\n",
            "Epoch 423/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1483 - accuracy: 0.9588 - val_loss: 0.3354 - val_accuracy: 0.9231\n",
            "Epoch 424/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1480 - accuracy: 0.9588 - val_loss: 0.3358 - val_accuracy: 0.9231\n",
            "Epoch 425/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1493 - accuracy: 0.9560 - val_loss: 0.3373 - val_accuracy: 0.9231\n",
            "Epoch 426/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1484 - accuracy: 0.9615 - val_loss: 0.3335 - val_accuracy: 0.9231\n",
            "Epoch 427/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1480 - accuracy: 0.9588 - val_loss: 0.3349 - val_accuracy: 0.9231\n",
            "Epoch 428/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.9588 - val_loss: 0.3350 - val_accuracy: 0.9231\n",
            "Epoch 429/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1483 - accuracy: 0.9588 - val_loss: 0.3326 - val_accuracy: 0.9231\n",
            "Epoch 430/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3346 - val_accuracy: 0.9231\n",
            "Epoch 431/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1488 - accuracy: 0.9533 - val_loss: 0.3393 - val_accuracy: 0.9231\n",
            "Epoch 432/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1477 - accuracy: 0.9588 - val_loss: 0.3392 - val_accuracy: 0.9231\n",
            "Epoch 433/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1482 - accuracy: 0.9588 - val_loss: 0.3366 - val_accuracy: 0.9231\n",
            "Epoch 434/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1480 - accuracy: 0.9560 - val_loss: 0.3372 - val_accuracy: 0.9231\n",
            "Epoch 435/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3398 - val_accuracy: 0.9231\n",
            "Epoch 436/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1480 - accuracy: 0.9560 - val_loss: 0.3377 - val_accuracy: 0.9231\n",
            "Epoch 437/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9560 - val_loss: 0.3362 - val_accuracy: 0.9231\n",
            "Epoch 438/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1484 - accuracy: 0.9533 - val_loss: 0.3354 - val_accuracy: 0.9231\n",
            "Epoch 439/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1478 - accuracy: 0.9588 - val_loss: 0.3382 - val_accuracy: 0.9231\n",
            "Epoch 440/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3379 - val_accuracy: 0.9231\n",
            "Epoch 441/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.9560 - val_loss: 0.3353 - val_accuracy: 0.9231\n",
            "Epoch 442/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1484 - accuracy: 0.9560 - val_loss: 0.3367 - val_accuracy: 0.9231\n",
            "Epoch 443/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1478 - accuracy: 0.9588 - val_loss: 0.3383 - val_accuracy: 0.9231\n",
            "Epoch 444/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.3404 - val_accuracy: 0.9231\n",
            "Epoch 445/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3412 - val_accuracy: 0.9231\n",
            "Epoch 446/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1482 - accuracy: 0.9588 - val_loss: 0.3434 - val_accuracy: 0.9231\n",
            "Epoch 447/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1480 - accuracy: 0.9588 - val_loss: 0.3401 - val_accuracy: 0.9231\n",
            "Epoch 448/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1478 - accuracy: 0.9560 - val_loss: 0.3391 - val_accuracy: 0.9231\n",
            "Epoch 449/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1477 - accuracy: 0.9588 - val_loss: 0.3394 - val_accuracy: 0.9231\n",
            "Epoch 450/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1481 - accuracy: 0.9560 - val_loss: 0.3387 - val_accuracy: 0.9231\n",
            "Epoch 451/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1479 - accuracy: 0.9588 - val_loss: 0.3386 - val_accuracy: 0.9231\n",
            "Epoch 452/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1488 - accuracy: 0.9560 - val_loss: 0.3409 - val_accuracy: 0.9231\n",
            "Epoch 453/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1483 - accuracy: 0.9560 - val_loss: 0.3426 - val_accuracy: 0.9231\n",
            "Epoch 454/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1479 - accuracy: 0.9560 - val_loss: 0.3419 - val_accuracy: 0.9231\n",
            "Epoch 455/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1479 - accuracy: 0.9588 - val_loss: 0.3415 - val_accuracy: 0.9231\n",
            "Epoch 456/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1480 - accuracy: 0.9560 - val_loss: 0.3427 - val_accuracy: 0.9231\n",
            "Epoch 457/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9560 - val_loss: 0.3385 - val_accuracy: 0.9231\n",
            "Epoch 458/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1474 - accuracy: 0.9588 - val_loss: 0.3398 - val_accuracy: 0.9231\n",
            "Epoch 459/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3422 - val_accuracy: 0.9231\n",
            "Epoch 460/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3422 - val_accuracy: 0.9231\n",
            "Epoch 461/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3436 - val_accuracy: 0.9231\n",
            "Epoch 462/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1485 - accuracy: 0.9533 - val_loss: 0.3418 - val_accuracy: 0.9231\n",
            "Epoch 463/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1476 - accuracy: 0.9560 - val_loss: 0.3412 - val_accuracy: 0.9231\n",
            "Epoch 464/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1477 - accuracy: 0.9588 - val_loss: 0.3427 - val_accuracy: 0.9231\n",
            "Epoch 465/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1479 - accuracy: 0.9588 - val_loss: 0.3434 - val_accuracy: 0.9231\n",
            "Epoch 466/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1494 - accuracy: 0.9505 - val_loss: 0.3447 - val_accuracy: 0.9231\n",
            "Epoch 467/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1480 - accuracy: 0.9560 - val_loss: 0.3455 - val_accuracy: 0.9231\n",
            "Epoch 468/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1497 - accuracy: 0.9560 - val_loss: 0.3427 - val_accuracy: 0.9231\n",
            "Epoch 469/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1487 - accuracy: 0.9560 - val_loss: 0.3410 - val_accuracy: 0.9231\n",
            "Epoch 470/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1479 - accuracy: 0.9588 - val_loss: 0.3413 - val_accuracy: 0.9231\n",
            "Epoch 471/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3436 - val_accuracy: 0.9231\n",
            "Epoch 472/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1496 - accuracy: 0.9588 - val_loss: 0.3436 - val_accuracy: 0.9231\n",
            "Epoch 473/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3438 - val_accuracy: 0.9231\n",
            "Epoch 474/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.3402 - val_accuracy: 0.9231\n",
            "Epoch 475/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1489 - accuracy: 0.9588 - val_loss: 0.3396 - val_accuracy: 0.9231\n",
            "Epoch 476/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1478 - accuracy: 0.9615 - val_loss: 0.3421 - val_accuracy: 0.9231\n",
            "Epoch 477/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1496 - accuracy: 0.9588 - val_loss: 0.3445 - val_accuracy: 0.9231\n",
            "Epoch 478/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1477 - accuracy: 0.9588 - val_loss: 0.3417 - val_accuracy: 0.9231\n",
            "Epoch 479/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1483 - accuracy: 0.9588 - val_loss: 0.3433 - val_accuracy: 0.9231\n",
            "Epoch 480/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1476 - accuracy: 0.9588 - val_loss: 0.3447 - val_accuracy: 0.9231\n",
            "Epoch 481/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1478 - accuracy: 0.9588 - val_loss: 0.3473 - val_accuracy: 0.9231\n",
            "Epoch 482/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3497 - val_accuracy: 0.9231\n",
            "Epoch 483/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1488 - accuracy: 0.9588 - val_loss: 0.3455 - val_accuracy: 0.9231\n",
            "Epoch 484/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1478 - accuracy: 0.9588 - val_loss: 0.3433 - val_accuracy: 0.9231\n",
            "Epoch 485/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3412 - val_accuracy: 0.9231\n",
            "Epoch 486/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1479 - accuracy: 0.9560 - val_loss: 0.3436 - val_accuracy: 0.9231\n",
            "Epoch 487/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3425 - val_accuracy: 0.9231\n",
            "Epoch 488/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3434 - val_accuracy: 0.9231\n",
            "Epoch 489/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1475 - accuracy: 0.9588 - val_loss: 0.3441 - val_accuracy: 0.9231\n",
            "Epoch 490/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3442 - val_accuracy: 0.9231\n",
            "Epoch 491/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1482 - accuracy: 0.9588 - val_loss: 0.3459 - val_accuracy: 0.9231\n",
            "Epoch 492/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1479 - accuracy: 0.9588 - val_loss: 0.3442 - val_accuracy: 0.9231\n",
            "Epoch 493/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1478 - accuracy: 0.9560 - val_loss: 0.3465 - val_accuracy: 0.9231\n",
            "Epoch 494/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1483 - accuracy: 0.9588 - val_loss: 0.3472 - val_accuracy: 0.9231\n",
            "Epoch 495/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1480 - accuracy: 0.9560 - val_loss: 0.3456 - val_accuracy: 0.9231\n",
            "Epoch 496/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1477 - accuracy: 0.9588 - val_loss: 0.3453 - val_accuracy: 0.9231\n",
            "Epoch 497/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1482 - accuracy: 0.9560 - val_loss: 0.3455 - val_accuracy: 0.9231\n",
            "Epoch 498/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1482 - accuracy: 0.9560 - val_loss: 0.3461 - val_accuracy: 0.9231\n",
            "Epoch 499/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1480 - accuracy: 0.9560 - val_loss: 0.3476 - val_accuracy: 0.9231\n",
            "Epoch 500/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1481 - accuracy: 0.9588 - val_loss: 0.3458 - val_accuracy: 0.9231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred_DNN = (model.predict(DNNX) > 0.5).astype(\"int32\")\n",
        "DTr = model.evaluate(DX_train, Dy_train,verbose=0)[1]\n",
        "DTe = model.evaluate(DX_test, Dy_test,verbose=0)[1]\n",
        "DTr,DTe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhG0YCbhASH9",
        "outputId": "4ef2b4f1-b25e-41b4-b94a-6d167aa7171a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9516483545303345, 0.9210526347160339)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_DNN = (model.predict(DX_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKbxrS57kOBE",
        "outputId": "118e8aee-fa2c-41e4-c989-64ccdd16be11"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"DNN\":[DTr,DTe]})\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "v_YjlCdOO1JY",
        "outputId": "41902c63-3152-4adf-aca1-4874c6c28c45"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        train      test\n",
              "SVM  0.951648  0.956140\n",
              "KNN  0.980220  0.991228\n",
              "RF   0.975824  0.991228\n",
              "LR   0.980220  0.973684\n",
              "ANN  0.947253  0.973684\n",
              "XGB  0.989011  0.982456\n",
              "DNN  0.951648  0.921053"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb467d55-b212-4a1d-b81e-c53798c89e52\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.991228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>0.975824</td>\n",
              "      <td>0.991228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.973684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>0.947253</td>\n",
              "      <td>0.973684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>0.989011</td>\n",
              "      <td>0.982456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.921053</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb467d55-b212-4a1d-b81e-c53798c89e52')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb467d55-b212-4a1d-b81e-c53798c89e52 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb467d55-b212-4a1d-b81e-c53798c89e52');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AutoML Individual and AutoML DNN**"
      ],
      "metadata": {
        "id": "nJaAMLDKKfKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H2O AutoML"
      ],
      "metadata": {
        "id": "EhNt_UkjDKcV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h2o\n",
        "import h2o\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "h2o.init()\n",
        "from h2o.model.segment_models import H2OFrame\n",
        "from h2o.automl import H2OAutoML\n",
        "print(\"All Library Loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "iwMF9ROeDOzF",
        "outputId": "53992e83-84b2-4752-cdd9-c8d75b32c984"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h2o\n",
            "  Downloading h2o-3.40.0.4.tar.gz (177.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.8.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from h2o) (0.18.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.4)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.40.0.4-py2.py3-none-any.whl size=177697886 sha256=2697b85050f7a007db88f192ae9a559fe7f274bf6b818306a1c710bbb8019964\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/f2/b0/5bb4d702a0467e82d77c45088db3eef25114c26b0eec8e7f6a\n",
            "Successfully built h2o\n",
            "Installing collected packages: h2o\n",
            "Successfully installed h2o-3.40.0.4\n",
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.19\" 2023-04-18; OpenJDK Runtime Environment (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1); OpenJDK 64-Bit Server VM (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.10/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpjqbdkg5b\n",
            "  JVM stdout: /tmp/tmpjqbdkg5b/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpjqbdkg5b/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         02 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.40.0.4\n",
              "H2O_cluster_version_age:    2 months and 5 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_h3p6sm\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.170 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-1.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-1 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-1 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-1 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table th,\n",
              "#h2o-table-1 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>02 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.40.0.4</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>2 months and 5 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_h3p6sm</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.170 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Library Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWdoOLbsF2qs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-QiglXeF_XE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8P1HTOHbugE-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asKywHvIu83f"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5wVvB0pvWHe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train, valid = hdf.split_frame(ratios=[.8], seed=123)\n",
        "#hdf = h2o.H2OFrame(df)\n",
        "#hdf[\"diagnosis\"] = hdf[\"diagnosis\"].asfactor()\n",
        "hy = \"diagnosis\"\n",
        "hx = list(df.columns)\n",
        "hx.remove(hy)\n",
        "hdf  = df.copy()\n",
        "hdf.iloc[:,1:] = StandardScaler().fit_transform(hdf.iloc[:,1:])\n",
        "hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n",
        "hdf.iloc[:,0] = hdf.iloc[:,0].astype('category')\n",
        "train1, valid1 = train_test_split(hdf, test_size=0.2,random_state=123)\n",
        "train = h2o.H2OFrame(train1)\n",
        "valid = h2o.H2OFrame(valid1)\n",
        "train[\"diagnosis\"] = train[\"diagnosis\"].asfactor()\n",
        "valid[\"diagnosis\"] = valid[\"diagnosis\"].asfactor()"
      ],
      "metadata": {
        "id": "NVoFNbYCGxGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e98fef-4ae1-41aa-fda1-1401a0780ad5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-460708a37676>:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = time.time()\n",
        "aml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\",\n",
        "                nfolds=10, sort_metric='accuracy')\n",
        "aml.train(x = hx, y = hy, training_frame = train,\n",
        "          validation_frame = valid)\n",
        "autoend = time.time() - st"
      ],
      "metadata": {
        "id": "7JNO6XnVHH5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bde204c-3763-46d3-cb51-97971cb41f63"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoML progress: |\n",
            "21:56:47.141: Project: AutoML_1_20230703_215647\n",
            "21:56:47.143: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "21:56:47.145: Setting stopping tolerance adaptively based on the training frame: 0.04688072309384954\n",
            "21:56:47.145: Build control seed: 123\n",
            "21:56:47.146: training frame: Frame key: AutoML_1_20230703_215647_training_py_1_sid_8a29    cols: 31    rows: 455  chunks: 1    size: 111919  checksum: -6999049652048799504\n",
            "21:56:47.149: validation frame: Frame key: py_2_sid_8a29    cols: 31    rows: 114  chunks: 1    size: 30037  checksum: 1244986154056216624\n",
            "21:56:47.149: leaderboard frame: NULL\n",
            "21:56:47.149: blending frame: NULL\n",
            "21:56:47.149: response column: diagnosis\n",
            "21:56:47.150: fold column: null\n",
            "21:56:47.150: weights column: null\n",
            "21:56:47.171: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "21:56:47.198: AutoML job created: 2023.07.03 21:56:47.97\n",
            "21:56:47.199: AutoML build started: 2023.07.03 21:56:47.199\n",
            "21:56:47.247: AutoML: starting XGBoost_1_AutoML_1_20230703_215647 model training\n",
            "\n",
            "█\n",
            "21:56:52.117: New leader: XGBoost_1_AutoML_1_20230703_215647, accuracy: 0.9428571428571428\n",
            "21:56:52.122: AutoML: starting GLM_1_AutoML_1_20230703_215647 model training\n",
            "\n",
            "█\n",
            "21:56:54.745: AutoML: starting GBM_1_AutoML_1_20230703_215647 model training\n",
            "\n",
            "███\n",
            "21:57:08.19: AutoML: starting XGBoost_2_AutoML_1_20230703_215647 model training\n",
            "\n",
            "█\n",
            "21:57:10.623: AutoML: starting DRF_1_AutoML_1_20230703_215647 model training\n",
            "\n",
            "\n",
            "21:57:13.507: AutoML: starting GBM_2_AutoML_1_20230703_215647 model training\n",
            "\n",
            "███\n",
            "21:57:22.541: AutoML: starting GBM_3_AutoML_1_20230703_215647 model training\n",
            "\n",
            "█\n",
            "21:57:30.496: AutoML: starting GBM_4_AutoML_1_20230703_215647 model training\n",
            "\n",
            "█\n",
            "21:57:37.322: AutoML: starting XGBoost_3_AutoML_1_20230703_215647 model training\n",
            "\n",
            "█\n",
            "21:57:41.355: AutoML: starting XRT_1_AutoML_1_20230703_215647 model training\n",
            "\n",
            "█\n",
            "21:57:44.948: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "21:57:44.966: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_1_20230703_215647 model training\n",
            "\n",
            "██\n",
            "21:57:47.109: AutoML: starting StackedEnsemble_AllModels_1_AutoML_1_20230703_215647 model training\n",
            "21:57:49.679: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "21:57:49.679: AutoML build stopped: 2023.07.03 21:57:49.679\n",
            "21:57:49.679: AutoML build done: built 10 models\n",
            "21:57:49.679: AutoML duration:  1 min  2.480 sec\n",
            "\n",
            "████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lbo606kFH4Zc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lb = aml.leaderboard\n",
        "lb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "gmCXw_D_4y1D",
        "outputId": "aefc96e6-7f0e-42e5-dbaf-c88deeb702d1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_id                              accuracy       auc    logloss     aucpr    mean_per_class_error      rmse        mse\n",
              "----------------------------------  ----------  --------  ---------  --------  ----------------------  --------  ---------\n",
              "XGBoost_1_AutoML_1_20230703_215647    0.942857  0.984114  0.166379   0.979636               0.0539185  0.210251  0.0442054\n",
              "DRF_1_AutoML_1_20230703_215647        0.96044   0.987058  0.249487   0.985869               0.0421609  0.18258   0.0333355\n",
              "XRT_1_AutoML_1_20230703_215647        0.96044   0.991619  0.116734   0.98782                0.0421609  0.184096  0.0338912\n",
              "XGBoost_2_AutoML_1_20230703_215647    0.962637  0.990394  0.11331    0.987678               0.0392369  0.174879  0.0305826\n",
              "GBM_4_AutoML_1_20230703_215647        0.967033  0.993246  0.0998609  0.990397               0.0333889  0.170342  0.0290164\n",
              "GBM_2_AutoML_1_20230703_215647        0.967033  0.994255  0.0933299  0.991681               0.0380426  0.162663  0.0264594\n",
              "XGBoost_3_AutoML_1_20230703_215647    0.969231  0.992999  0.0965668  0.990609               0.0316284  0.161568  0.0261043\n",
              "GBM_1_AutoML_1_20230703_215647        0.971429  0.994461  0.0911605  0.992401               0.0287044  0.158892  0.0252468\n",
              "GBM_3_AutoML_1_20230703_215647        0.971429  0.994461  0.0902413  0.991937               0.027541   0.157908  0.0249349\n",
              "GLM_1_AutoML_1_20230703_215647        0.978022  0.996479  0.0739483  0.994833               0.0269129  0.1449    0.0209961\n",
              "[10 rows x 8 columns]\n"
            ],
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th>model_id                          </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">      mse</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>XGBoost_1_AutoML_1_20230703_215647</td><td style=\"text-align: right;\">  0.942857</td><td style=\"text-align: right;\">0.984114</td><td style=\"text-align: right;\">0.166379 </td><td style=\"text-align: right;\">0.979636</td><td style=\"text-align: right;\">             0.0539185</td><td style=\"text-align: right;\">0.210251</td><td style=\"text-align: right;\">0.0442054</td></tr>\n",
              "<tr><td>DRF_1_AutoML_1_20230703_215647    </td><td style=\"text-align: right;\">  0.96044 </td><td style=\"text-align: right;\">0.987058</td><td style=\"text-align: right;\">0.249487 </td><td style=\"text-align: right;\">0.985869</td><td style=\"text-align: right;\">             0.0421609</td><td style=\"text-align: right;\">0.18258 </td><td style=\"text-align: right;\">0.0333355</td></tr>\n",
              "<tr><td>XRT_1_AutoML_1_20230703_215647    </td><td style=\"text-align: right;\">  0.96044 </td><td style=\"text-align: right;\">0.991619</td><td style=\"text-align: right;\">0.116734 </td><td style=\"text-align: right;\">0.98782 </td><td style=\"text-align: right;\">             0.0421609</td><td style=\"text-align: right;\">0.184096</td><td style=\"text-align: right;\">0.0338912</td></tr>\n",
              "<tr><td>XGBoost_2_AutoML_1_20230703_215647</td><td style=\"text-align: right;\">  0.962637</td><td style=\"text-align: right;\">0.990394</td><td style=\"text-align: right;\">0.11331  </td><td style=\"text-align: right;\">0.987678</td><td style=\"text-align: right;\">             0.0392369</td><td style=\"text-align: right;\">0.174879</td><td style=\"text-align: right;\">0.0305826</td></tr>\n",
              "<tr><td>GBM_4_AutoML_1_20230703_215647    </td><td style=\"text-align: right;\">  0.967033</td><td style=\"text-align: right;\">0.993246</td><td style=\"text-align: right;\">0.0998609</td><td style=\"text-align: right;\">0.990397</td><td style=\"text-align: right;\">             0.0333889</td><td style=\"text-align: right;\">0.170342</td><td style=\"text-align: right;\">0.0290164</td></tr>\n",
              "<tr><td>GBM_2_AutoML_1_20230703_215647    </td><td style=\"text-align: right;\">  0.967033</td><td style=\"text-align: right;\">0.994255</td><td style=\"text-align: right;\">0.0933299</td><td style=\"text-align: right;\">0.991681</td><td style=\"text-align: right;\">             0.0380426</td><td style=\"text-align: right;\">0.162663</td><td style=\"text-align: right;\">0.0264594</td></tr>\n",
              "<tr><td>XGBoost_3_AutoML_1_20230703_215647</td><td style=\"text-align: right;\">  0.969231</td><td style=\"text-align: right;\">0.992999</td><td style=\"text-align: right;\">0.0965668</td><td style=\"text-align: right;\">0.990609</td><td style=\"text-align: right;\">             0.0316284</td><td style=\"text-align: right;\">0.161568</td><td style=\"text-align: right;\">0.0261043</td></tr>\n",
              "<tr><td>GBM_1_AutoML_1_20230703_215647    </td><td style=\"text-align: right;\">  0.971429</td><td style=\"text-align: right;\">0.994461</td><td style=\"text-align: right;\">0.0911605</td><td style=\"text-align: right;\">0.992401</td><td style=\"text-align: right;\">             0.0287044</td><td style=\"text-align: right;\">0.158892</td><td style=\"text-align: right;\">0.0252468</td></tr>\n",
              "<tr><td>GBM_3_AutoML_1_20230703_215647    </td><td style=\"text-align: right;\">  0.971429</td><td style=\"text-align: right;\">0.994461</td><td style=\"text-align: right;\">0.0902413</td><td style=\"text-align: right;\">0.991937</td><td style=\"text-align: right;\">             0.027541 </td><td style=\"text-align: right;\">0.157908</td><td style=\"text-align: right;\">0.0249349</td></tr>\n",
              "<tr><td>GLM_1_AutoML_1_20230703_215647    </td><td style=\"text-align: right;\">  0.978022</td><td style=\"text-align: right;\">0.996479</td><td style=\"text-align: right;\">0.0739483</td><td style=\"text-align: right;\">0.994833</td><td style=\"text-align: right;\">             0.0269129</td><td style=\"text-align: right;\">0.1449  </td><td style=\"text-align: right;\">0.0209961</td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10 rows x 8 columns]</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "best_model"
      ],
      "metadata": {
        "id": "vlH8zwtisQU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21a2f448-afa6-45e3-9cb9-02f96a49e748"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230703_215647\n",
              "\n",
              "\n",
              "Model Summary: \n",
              "    number_of_trees\n",
              "--  -----------------\n",
              "    36\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.029074514855628383\n",
              "RMSE: 0.17051250644931704\n",
              "LogLoss: 0.12394929039565182\n",
              "Mean Per-Class Error: 0.029270653158718393\n",
              "AUC: 0.9940799769376494\n",
              "AUCPR: 0.992056368504806\n",
              "Gini: 0.9881599538752988\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4925362765789032\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  ------------\n",
              "0      279  5    0.0176   (5.0/284.0)\n",
              "1      7    164  0.0409   (7.0/171.0)\n",
              "Total  286  169  0.0264   (12.0/455.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.492536     0.964706  38\n",
              "max f2                       0.317989     0.971098  49\n",
              "max f0point5                 0.514766     0.971395  37\n",
              "max accuracy                 0.514766     0.973626  37\n",
              "max precision                0.968002     1         0\n",
              "max recall                   0.0588847    1         100\n",
              "max specificity              0.968002     1         0\n",
              "max absolute_mcc             0.492536     0.943696  38\n",
              "max min_per_class_accuracy   0.402806     0.964912  42\n",
              "max mean_per_class_accuracy  0.492536     0.970729  38\n",
              "max tns                      0.968002     284       0\n",
              "max fns                      0.968002     110       0\n",
              "max fps                      0.0148769    284       134\n",
              "max tps                      0.0588847    171       100\n",
              "max tnr                      0.968002     1         0\n",
              "max fnr                      0.968002     0.643275  0\n",
              "max fpr                      0.0148769    1         134\n",
              "max tpr                      0.0588847    1         100\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.29 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.134066                    0.968002           2.66082    2.66082            1                0.968002   1                           0.968002            0.356725        0.356725                   166.082   166.082            0.356725\n",
              "2        0.162637                    0.957612           2.66082    2.66082            1                0.957612   1                           0.966177            0.0760234       0.432749                   166.082   166.082            0.432749\n",
              "3        0.2                         0.950501           2.66082    2.66082            1                0.953488   1                           0.963806            0.0994152       0.532164                   166.082   166.082            0.532164\n",
              "4        0.301099                    0.807568           2.66082    2.66082            1                0.897179   1                           0.941435            0.269006        0.80117                    166.082   166.082            0.80117\n",
              "5        0.4                         0.314704           1.83301    2.45614            0.688889         0.582323   0.923077                    0.852644            0.181287        0.982456                   83.3008   145.614            0.93316\n",
              "6        0.501099                    0.076693           0.0578439  1.97227            0.0217391        0.161579   0.741228                    0.713218            0.00584795      0.988304                   -94.2156  97.2274            0.780558\n",
              "7        0.624176                    0.0503081          0.0950292  1.60211            0.0357143        0.058398   0.602113                    0.584099            0.0116959       1                          -90.4971  60.2113            0.602113\n",
              "8        0.698901                    0.0279195          0          1.43082            0                0.0365022  0.537736                    0.525551            0               1                          -100      43.0818            0.482394\n",
              "9        0.804396                    0.0177071          0          1.24317            0                0.0225316  0.467213                    0.459581            0               1                          -100      24.3169            0.31338\n",
              "10       0.931868                    0.0175245          0          1.07311            0                0.0175245  0.403302                    0.399111            0               1                          -100      7.31132            0.109155\n",
              "11       1                           0.0148769          0          1                  0                0.0148769  0.375824                    0.372932            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.03762067586537018\n",
              "RMSE: 0.19396050078655236\n",
              "LogLoss: 0.1512542647982658\n",
              "Mean Per-Class Error: 0.03274306715669896\n",
              "AUC: 0.976445038422987\n",
              "AUCPR: 0.9791961628675718\n",
              "Gini: 0.952890076845974\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25368359684944153\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  -----------\n",
              "0      70   3    0.0411   (3.0/73.0)\n",
              "1      1    40   0.0244   (1.0/41.0)\n",
              "Total  71   43   0.0351   (4.0/114.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.253684     0.952381  20\n",
              "max f2                       0.253684     0.966184  20\n",
              "max f0point5                 0.705651     0.972973  15\n",
              "max accuracy                 0.253684     0.964912  20\n",
              "max precision                0.968002     1         0\n",
              "max recall                   0.0175245    1         46\n",
              "max specificity              0.968002     1         0\n",
              "max absolute_mcc             0.253684     0.925285  20\n",
              "max min_per_class_accuracy   0.253684     0.958904  20\n",
              "max mean_per_class_accuracy  0.253684     0.967257  20\n",
              "max tns                      0.968002     73        0\n",
              "max fns                      0.968002     27        0\n",
              "max fps                      0.0148769    73        47\n",
              "max tps                      0.0175245    41        46\n",
              "max tnr                      0.968002     1         0\n",
              "max fnr                      0.968002     0.658537  0\n",
              "max fpr                      0.0148769    1         47\n",
              "max tpr                      0.0175245    1         46\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 35.96 %, avg score: 34.85 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.122807                    0.968002           2.78049    2.78049            1                0.968002   1                           0.968002            0.341463        0.341463                   178.049   178.049            0.341463\n",
              "2        0.157895                    0.954363           2.78049    2.78049            1                0.955175   1                           0.965152            0.097561        0.439024                   178.049   178.049            0.439024\n",
              "3        0.210526                    0.932284           2.78049    2.78049            1                0.935408   1                           0.957716            0.146341        0.585366                   178.049   178.049            0.585366\n",
              "4        0.298246                    0.717271           2.78049    2.78049            1                0.870593   1                           0.932091            0.243902        0.829268                   178.049   178.049            0.829268\n",
              "5        0.403509                    0.169029           1.39024    2.41782            0.5              0.45112    0.869565                    0.806621            0.146341        0.97561                    39.0244   141.782            0.893418\n",
              "6        0.508772                    0.0588847          0          1.91758            0                0.093715   0.689655                    0.659123            0               0.97561                    -100      91.7578            0.729034\n",
              "7        0.596491                    0.0359404          0          1.63558            0                0.0470055  0.588235                    0.569106            0               0.97561                    -100      63.5581            0.592048\n",
              "8        0.701754                    0.023509           0          1.39024            0                0.0319946  0.5                         0.488539            0               0.97561                    -100      39.0244            0.427665\n",
              "9        0.973684                    0.0175245          0.0896932  1.02703            0.0322581        0.0192596  0.369369                    0.357479            0.0243902       1                          -91.0307  2.7027             0.0410959\n",
              "10       1                           0.0148769          0          1                  0                0.0148769  0.359649                    0.348463            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.044205368519838424\n",
              "RMSE: 0.21025072775103162\n",
              "LogLoss: 0.16637884924436225\n",
              "Mean Per-Class Error: 0.05391854048266205\n",
              "AUC: 0.9841137468083354\n",
              "AUCPR: 0.9796363289807396\n",
              "Gini: 0.9682274936166708\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35231223702430725\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  ------------\n",
              "0      265  19   0.0669   (19.0/284.0)\n",
              "1      7    164  0.0409   (7.0/171.0)\n",
              "Total  272  183  0.0571   (26.0/455.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.352312     0.926554  103\n",
              "max f2                       0.352312     0.94579   103\n",
              "max f0point5                 0.648854     0.953307  71\n",
              "max accuracy                 0.624354     0.942857  74\n",
              "max precision                0.968105     1         0\n",
              "max recall                   0.0317377    1         212\n",
              "max specificity              0.968105     1         0\n",
              "max absolute_mcc             0.352312     0.881234  103\n",
              "max min_per_class_accuracy   0.393351     0.940141  98\n",
              "max mean_per_class_accuracy  0.352312     0.946081  103\n",
              "max tns                      0.968105     284       0\n",
              "max fns                      0.968105     168       0\n",
              "max fps                      0.0161675    284       239\n",
              "max tps                      0.0317377    171       212\n",
              "max tnr                      0.968105     1         0\n",
              "max fnr                      0.968105     0.982456  0\n",
              "max fpr                      0.0161675    1         239\n",
              "max tpr                      0.0317377    1         212\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.58 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.0153846                   0.964642           2.66082    2.66082            1                0.966126   1                           0.966126            0.0409357       0.0409357                  166.082   166.082            0.0409357\n",
              "2        0.032967                    0.964165           2.66082    2.66082            1                0.964165   1                           0.96508             0.0467836       0.0877193                  166.082   166.082            0.0877193\n",
              "3        0.0461538                   0.96322            2.66082    2.66082            1                0.96322    1                           0.964549            0.0350877       0.122807                   166.082   166.082            0.122807\n",
              "4        0.0659341                   0.961647           2.66082    2.66082            1                0.961647   1                           0.963678            0.0526316       0.175439                   166.082   166.082            0.175439\n",
              "5        0.101099                    0.959238           2.66082    2.66082            1                0.960176   1                           0.96246             0.0935673       0.269006                   166.082   166.082            0.269006\n",
              "6        0.151648                    0.95194            2.66082    2.66082            1                0.954832   1                           0.959917            0.134503        0.403509                   166.082   166.082            0.403509\n",
              "7        0.2                         0.934824           2.66082    2.66082            1                0.944351   1                           0.956154            0.128655        0.532164                   166.082   166.082            0.532164\n",
              "8        0.301099                    0.781079           2.54513    2.62197            0.956522         0.887324   0.985401                    0.933043            0.25731         0.789474                   154.513   162.197            0.782431\n",
              "9        0.4                         0.354154           1.65562    2.38304            0.622222         0.566931   0.895604                    0.842521            0.163743        0.953216                   65.5621   138.304            0.886315\n",
              "10       0.501099                    0.0901423          0.289219   1.9606             0.108696         0.19796    0.736842                    0.712478            0.0292398       0.982456                   -71.0781  96.0603            0.771189\n",
              "11       0.6                         0.0496738          0.0591293  1.64717            0.0222222        0.0669556  0.619048                    0.606073            0.00584795      0.988304                   -94.0871  64.7173            0.622107\n",
              "12       0.698901                    0.0362709          0.0591293  1.42245            0.0222222        0.0437602  0.534591                    0.526501            0.00584795      0.994152                   -94.0871  42.245             0.473025\n",
              "13       0.8                         0.0304273          0.0578439  1.25               0.0217391        0.0321457  0.46978                     0.464027            0.00584795      1                          -94.2156  25                 0.320423\n",
              "14       0.907692                    0.0233827          0          1.10169            0                0.0257063  0.414044                    0.412023            0               1                          -100      10.1695            0.147887\n",
              "15       1                           0.0161675          0          1                  0                0.0195924  0.375824                    0.375799            0               1                          -100      0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                         mean       sd         cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid    cv_6_valid    cv_7_valid    cv_8_valid    cv_9_valid    cv_10_valid\n",
              "-----------------------  ---------  ---------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  -------------\n",
              "accuracy                 0.96715    0.0235589  0.934783      0.978261      0.978261      0.956522      0.934783      0.955556      1             0.955556      1             0.977778\n",
              "auc                      0.983879   0.0178793  0.960938      0.995833      0.997972      0.984615      0.975446      0.980159      1             0.948276      1             0.995556\n",
              "err                      0.0328502  0.0235589  0.0652174     0.0217391     0.0217391     0.0434783     0.0652174     0.0444444     0             0.0444444     0             0.0222222\n",
              "err_count                1.5        1.08012    3             1             1             2             3             2             0             2             0             1\n",
              "f0point5                 0.958851   0.0287485  0.948276      0.986842      0.955056      0.95          0.909091      0.952381      1             0.9375        1             0.949367\n",
              "f1                       0.951568   0.0406347  0.88          0.967742      0.971429      0.95          0.888889      0.952381      1             0.9375        1             0.967742\n",
              "f2                       0.945492   0.0586199  0.820896      0.949367      0.988372      0.95          0.869565      0.952381      1             0.9375        1             0.986842\n",
              "lift_top_group           2.7497     0.495565   3.28571       2.875         2.70588       2.3           3.28571       2.14286       1.875         2.8125        3.21429       3\n",
              "logloss                  0.166222   0.0649039  0.212998      0.147144      0.106434      0.204719      0.231138      0.199252      0.0746679     0.255789      0.0740798     0.156001\n",
              "max_per_class_error      0.0647578  0.0660564  0.214286      0.0625        0.0344828     0.05          0.142857      0.0476191     0             0.0625        0             0.0333333\n",
              "mcc                      0.927621   0.0547608  0.847566      0.952501      0.954923      0.911539      0.843984      0.910714      1             0.903017      1             0.951972\n",
              "mean_per_class_accuracy  0.960328   0.035232   0.892857      0.96875       0.982759      0.955769      0.912946      0.955357      1             0.951509      1             0.983333\n",
              "mean_per_class_error     0.0396719  0.035232   0.107143      0.03125       0.0172414     0.0442308     0.0870536     0.0446429     0             0.0484914     0             0.0166667\n",
              "mse                      0.0441377  0.0226749  0.0620417     0.0377012     0.0215032     0.0596129     0.0706184     0.0543299     0.0114468     0.0693428     0.0124989     0.0422811\n",
              "pr_auc                   0.977882   0.023669   0.938531      0.992855      0.996638      0.980331      0.952928      0.982007      1             0.944435      1             0.991098\n",
              "precision                0.96449    0.0315774  1             1             0.944444      0.95          0.923077      0.952381      1             0.9375        1             0.9375\n",
              "r2                       0.805685   0.102998   0.706964      0.833801      0.907706      0.757421      0.666454      0.78171       0.954008      0.697372      0.941681      0.809735\n",
              "recall                   0.942024   0.0709415  0.785714      0.9375        1             0.95          0.857143      0.952381      1             0.9375        1             1\n",
              "rmse                     0.202062   0.0606329  0.249082      0.194168      0.14664       0.244158      0.265741      0.233088      0.10699       0.26333       0.111799      0.205624\n",
              "specificity              0.978632   0.0186053  1             1             0.965517      0.961538      0.96875       0.958333      1             0.965517      1             0.966667\n",
              "\n",
              "Scoring History: \n",
              "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
              "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
              "    2023-07-03 21:56:51  4.526 sec   0                  0.5              0.693147            0.5             0.375824           1                0.624176                         0.5                0.693147              0.5               0.359649             1                  0.640351\n",
              "    2023-07-03 21:56:51  4.550 sec   5                  0.225101         0.220985            0.991465        0.987958           2.66082          0.0307692                        0.225428           0.222521              0.976779          0.978796             2.78049            0.0438596\n",
              "    2023-07-03 21:56:51  4.571 sec   10                 0.182175         0.144104            0.993503        0.991125           2.66082          0.0307692                        0.194942           0.160143              0.97728           0.979975             2.78049            0.0350877\n",
              "    2023-07-03 21:56:51  4.599 sec   15                 0.172352         0.130611            0.994533        0.992725           2.66082          0.0241758                        0.191048           0.152637              0.977113          0.980463             2.78049            0.0350877\n",
              "    2023-07-03 21:56:51  4.619 sec   20                 0.170296         0.1239              0.99408         0.992056           2.66082          0.0263736                        0.193492           0.150659              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-07-03 21:56:51  4.646 sec   25                 0.170296         0.1239              0.99408         0.992056           2.66082          0.0263736                        0.193495           0.150663              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-07-03 21:56:51  4.670 sec   30                 0.170295         0.123902            0.99408         0.992056           2.66082          0.0263736                        0.193487           0.150653              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-07-03 21:56:51  4.693 sec   35                 0.17029          0.123951            0.99408         0.992056           2.66082          0.0263736                        0.193367           0.150506              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-07-03 21:56:51  4.713 sec   36                 0.170513         0.123949            0.99408         0.992056           2.66082          0.0263736                        0.193961           0.151254              0.976445          0.979196             2.78049            0.0350877\n",
              "\n",
              "Variable Importances: \n",
              "variable              relative_importance    scaled_importance    percentage\n",
              "--------------------  ---------------------  -------------------  ------------\n",
              "concave points_worst  204.282                1                    0.341069\n",
              "area_worst            111.787                0.547218             0.186639\n",
              "concave points_mean   80.5573                0.394343             0.134498\n",
              "radius_worst          72.7003                0.355881             0.12138\n",
              "perimeter_worst       63.8343                0.312481             0.106577\n",
              "texture_worst         22.0758                0.108065             0.0368576\n",
              "concavity_worst       20.4532                0.100122             0.0341486\n",
              "texture_mean          10.5059                0.0514284            0.0175406\n",
              "radius_mean           8.47355                0.0414796            0.0141474\n",
              "concavity_mean        3.36487                0.0164716            0.00561796\n",
              "compactness_se        0.913376               0.00447114           0.00152497\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ],
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230703_215647\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-2.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-2 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-2 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-2 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table th,\n",
              "#h2o-table-2 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>number_of_trees</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>36.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.029074514855628383\n",
              "RMSE: 0.17051250644931704\n",
              "LogLoss: 0.12394929039565182\n",
              "Mean Per-Class Error: 0.029270653158718393\n",
              "AUC: 0.9940799769376494\n",
              "AUCPR: 0.992056368504806\n",
              "Gini: 0.9881599538752988</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-3.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-3 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-3 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-3 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table th,\n",
              "#h2o-table-3 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4925362765789032</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>279.0</td>\n",
              "<td>5.0</td>\n",
              "<td>0.0176</td>\n",
              "<td> (5.0/284.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>7.0</td>\n",
              "<td>164.0</td>\n",
              "<td>0.0409</td>\n",
              "<td> (7.0/171.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>286.0</td>\n",
              "<td>169.0</td>\n",
              "<td>0.0264</td>\n",
              "<td> (12.0/455.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-4.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-4 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-4 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-4 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table th,\n",
              "#h2o-table-4 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-4 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-4\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9647059</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.3179893</td>\n",
              "<td>0.9710983</td>\n",
              "<td>49.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.5147656</td>\n",
              "<td>0.9713945</td>\n",
              "<td>37.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.5147656</td>\n",
              "<td>0.9736264</td>\n",
              "<td>37.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0588847</td>\n",
              "<td>1.0</td>\n",
              "<td>100.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9436960</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.4028056</td>\n",
              "<td>0.9649123</td>\n",
              "<td>42.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9707293</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>284.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>110.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0148769</td>\n",
              "<td>284.0</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0588847</td>\n",
              "<td>171.0</td>\n",
              "<td>100.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.6432749</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0148769</td>\n",
              "<td>1.0</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0588847</td>\n",
              "<td>1.0</td>\n",
              "<td>100.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-5.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-5 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-5 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-5 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table th,\n",
              "#h2o-table-5 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.29 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.1340659</td>\n",
              "<td>0.9680020</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.3567251</td>\n",
              "<td>0.3567251</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.3567251</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.1626374</td>\n",
              "<td>0.9576122</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9576122</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661767</td>\n",
              "<td>0.0760234</td>\n",
              "<td>0.4327485</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.4327485</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.2</td>\n",
              "<td>0.9505008</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9534877</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9638063</td>\n",
              "<td>0.0994152</td>\n",
              "<td>0.5321637</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.5321637</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.3010989</td>\n",
              "<td>0.8075677</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.8971791</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9414351</td>\n",
              "<td>0.2690058</td>\n",
              "<td>0.8011696</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.8011696</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.4</td>\n",
              "<td>0.3147044</td>\n",
              "<td>1.8330084</td>\n",
              "<td>2.4561404</td>\n",
              "<td>0.6888889</td>\n",
              "<td>0.5823229</td>\n",
              "<td>0.9230769</td>\n",
              "<td>0.8526436</td>\n",
              "<td>0.1812865</td>\n",
              "<td>0.9824561</td>\n",
              "<td>83.3008447</td>\n",
              "<td>145.6140351</td>\n",
              "<td>0.9331604</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.5010989</td>\n",
              "<td>0.0766930</td>\n",
              "<td>0.0578439</td>\n",
              "<td>1.9722735</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.1615786</td>\n",
              "<td>0.7412281</td>\n",
              "<td>0.7132182</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9883041</td>\n",
              "<td>-94.2156115</td>\n",
              "<td>97.2273520</td>\n",
              "<td>0.7805576</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.6241758</td>\n",
              "<td>0.0503081</td>\n",
              "<td>0.0950292</td>\n",
              "<td>1.6021127</td>\n",
              "<td>0.0357143</td>\n",
              "<td>0.0583980</td>\n",
              "<td>0.6021127</td>\n",
              "<td>0.5840988</td>\n",
              "<td>0.0116959</td>\n",
              "<td>1.0</td>\n",
              "<td>-90.4970760</td>\n",
              "<td>60.2112676</td>\n",
              "<td>0.6021127</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.6989011</td>\n",
              "<td>0.0279195</td>\n",
              "<td>0.0</td>\n",
              "<td>1.4308176</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0365022</td>\n",
              "<td>0.5377358</td>\n",
              "<td>0.5255507</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>43.0817610</td>\n",
              "<td>0.4823944</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.8043956</td>\n",
              "<td>0.0177071</td>\n",
              "<td>0.0</td>\n",
              "<td>1.2431694</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0225316</td>\n",
              "<td>0.4672131</td>\n",
              "<td>0.4595810</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>24.3169399</td>\n",
              "<td>0.3133803</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.9318681</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0731132</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.4033019</td>\n",
              "<td>0.3991110</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>7.3113208</td>\n",
              "<td>0.1091549</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.3758242</td>\n",
              "<td>0.3729324</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.03762067586537018\n",
              "RMSE: 0.19396050078655236\n",
              "LogLoss: 0.1512542647982658\n",
              "Mean Per-Class Error: 0.03274306715669896\n",
              "AUC: 0.976445038422987\n",
              "AUCPR: 0.9791961628675718\n",
              "Gini: 0.952890076845974</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-6.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-6 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-6 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-6 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table th,\n",
              "#h2o-table-6 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25368359684944153</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>70.0</td>\n",
              "<td>3.0</td>\n",
              "<td>0.0411</td>\n",
              "<td> (3.0/73.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>1.0</td>\n",
              "<td>40.0</td>\n",
              "<td>0.0244</td>\n",
              "<td> (1.0/41.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>71.0</td>\n",
              "<td>43.0</td>\n",
              "<td>0.0351</td>\n",
              "<td> (4.0/114.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-7.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-7 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-7 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-7 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table th,\n",
              "#h2o-table-7 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-7 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-7\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9523810</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9661836</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.7056506</td>\n",
              "<td>0.9729730</td>\n",
              "<td>15.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9649123</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0175245</td>\n",
              "<td>1.0</td>\n",
              "<td>46.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9252854</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9589041</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9672569</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>73.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>27.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0148769</td>\n",
              "<td>73.0</td>\n",
              "<td>47.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0175245</td>\n",
              "<td>41.0</td>\n",
              "<td>46.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.6585366</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0148769</td>\n",
              "<td>1.0</td>\n",
              "<td>47.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0175245</td>\n",
              "<td>1.0</td>\n",
              "<td>46.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-8.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-8 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-8 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-8 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table th,\n",
              "#h2o-table-8 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-8 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-8\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 35.96 %, avg score: 34.85 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.1228070</td>\n",
              "<td>0.9680020</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.3414634</td>\n",
              "<td>0.3414634</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.3414634</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.1578947</td>\n",
              "<td>0.9543629</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9551752</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9651516</td>\n",
              "<td>0.0975610</td>\n",
              "<td>0.4390244</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.4390244</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.2105263</td>\n",
              "<td>0.9322835</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9354083</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9577157</td>\n",
              "<td>0.1463415</td>\n",
              "<td>0.5853659</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.5853659</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.2982456</td>\n",
              "<td>0.7172713</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.8705932</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9320915</td>\n",
              "<td>0.2439024</td>\n",
              "<td>0.8292683</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.8292683</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.4035088</td>\n",
              "<td>0.1690287</td>\n",
              "<td>1.3902439</td>\n",
              "<td>2.4178155</td>\n",
              "<td>0.5</td>\n",
              "<td>0.4511201</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.8066207</td>\n",
              "<td>0.1463415</td>\n",
              "<td>0.9756098</td>\n",
              "<td>39.0243902</td>\n",
              "<td>141.7815483</td>\n",
              "<td>0.8934180</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.5087719</td>\n",
              "<td>0.0588847</td>\n",
              "<td>0.0</td>\n",
              "<td>1.9175778</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0937150</td>\n",
              "<td>0.6896552</td>\n",
              "<td>0.6591230</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>91.7577796</td>\n",
              "<td>0.7290344</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.5964912</td>\n",
              "<td>0.0359404</td>\n",
              "<td>0.0</td>\n",
              "<td>1.6355811</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0470055</td>\n",
              "<td>0.5882353</td>\n",
              "<td>0.5691057</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>63.5581062</td>\n",
              "<td>0.5920481</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.7017544</td>\n",
              "<td>0.0235090</td>\n",
              "<td>0.0</td>\n",
              "<td>1.3902439</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0319946</td>\n",
              "<td>0.5</td>\n",
              "<td>0.4885390</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>39.0243902</td>\n",
              "<td>0.4276646</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.9736842</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.0896932</td>\n",
              "<td>1.0270270</td>\n",
              "<td>0.0322581</td>\n",
              "<td>0.0192596</td>\n",
              "<td>0.3693694</td>\n",
              "<td>0.3574790</td>\n",
              "<td>0.0243902</td>\n",
              "<td>1.0</td>\n",
              "<td>-91.0306845</td>\n",
              "<td>2.7027027</td>\n",
              "<td>0.0410959</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.3596491</td>\n",
              "<td>0.3484632</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.044205368519838424\n",
              "RMSE: 0.21025072775103162\n",
              "LogLoss: 0.16637884924436225\n",
              "Mean Per-Class Error: 0.05391854048266205\n",
              "AUC: 0.9841137468083354\n",
              "AUCPR: 0.9796363289807396\n",
              "Gini: 0.9682274936166708</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-9.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-9 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-9 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-9 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table th,\n",
              "#h2o-table-9 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-9 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-9\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35231223702430725</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>265.0</td>\n",
              "<td>19.0</td>\n",
              "<td>0.0669</td>\n",
              "<td> (19.0/284.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>7.0</td>\n",
              "<td>164.0</td>\n",
              "<td>0.0409</td>\n",
              "<td> (7.0/171.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>272.0</td>\n",
              "<td>183.0</td>\n",
              "<td>0.0571</td>\n",
              "<td> (26.0/455.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-10.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-10 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-10 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-10 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table th,\n",
              "#h2o-table-10 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-10 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-10\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9265537</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9457901</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6488545</td>\n",
              "<td>0.9533074</td>\n",
              "<td>71.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.6243538</td>\n",
              "<td>0.9428571</td>\n",
              "<td>74.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0317377</td>\n",
              "<td>1.0</td>\n",
              "<td>212.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.8812343</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3933513</td>\n",
              "<td>0.9401408</td>\n",
              "<td>98.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9460815</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9681047</td>\n",
              "<td>284.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9681047</td>\n",
              "<td>168.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0161675</td>\n",
              "<td>284.0</td>\n",
              "<td>239.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0317377</td>\n",
              "<td>171.0</td>\n",
              "<td>212.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9681047</td>\n",
              "<td>0.9824561</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0161675</td>\n",
              "<td>1.0</td>\n",
              "<td>239.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0317377</td>\n",
              "<td>1.0</td>\n",
              "<td>212.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-11.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-11 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-11 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-11 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table th,\n",
              "#h2o-table-11 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-11 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-11\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.58 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.0153846</td>\n",
              "<td>0.9646416</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661258</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661258</td>\n",
              "<td>0.0409357</td>\n",
              "<td>0.0409357</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.0409357</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.0329670</td>\n",
              "<td>0.9641652</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9641652</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9650801</td>\n",
              "<td>0.0467836</td>\n",
              "<td>0.0877193</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.0877193</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.0461538</td>\n",
              "<td>0.9632205</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9632205</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9645488</td>\n",
              "<td>0.0350877</td>\n",
              "<td>0.1228070</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.1228070</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.0659341</td>\n",
              "<td>0.9616467</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9616467</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9636782</td>\n",
              "<td>0.0526316</td>\n",
              "<td>0.1754386</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.1754386</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.1010989</td>\n",
              "<td>0.9592377</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9601760</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9624600</td>\n",
              "<td>0.0935673</td>\n",
              "<td>0.2690058</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.2690058</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1516484</td>\n",
              "<td>0.9519403</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9548322</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9599174</td>\n",
              "<td>0.1345029</td>\n",
              "<td>0.4035088</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.4035088</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.2</td>\n",
              "<td>0.9348235</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9443512</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9561542</td>\n",
              "<td>0.1286550</td>\n",
              "<td>0.5321637</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.5321637</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.3010989</td>\n",
              "<td>0.7810790</td>\n",
              "<td>2.5451309</td>\n",
              "<td>2.6219746</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.8873240</td>\n",
              "<td>0.9854015</td>\n",
              "<td>0.9330433</td>\n",
              "<td>0.2573099</td>\n",
              "<td>0.7894737</td>\n",
              "<td>154.5130943</td>\n",
              "<td>162.1974645</td>\n",
              "<td>0.7824314</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.4</td>\n",
              "<td>0.3541536</td>\n",
              "<td>1.6556205</td>\n",
              "<td>2.3830409</td>\n",
              "<td>0.6222222</td>\n",
              "<td>0.5669310</td>\n",
              "<td>0.8956044</td>\n",
              "<td>0.8425210</td>\n",
              "<td>0.1637427</td>\n",
              "<td>0.9532164</td>\n",
              "<td>65.5620533</td>\n",
              "<td>138.3040936</td>\n",
              "<td>0.8863150</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.5010989</td>\n",
              "<td>0.0901423</td>\n",
              "<td>0.2892194</td>\n",
              "<td>1.9606033</td>\n",
              "<td>0.1086957</td>\n",
              "<td>0.1979604</td>\n",
              "<td>0.7368421</td>\n",
              "<td>0.7124781</td>\n",
              "<td>0.0292398</td>\n",
              "<td>0.9824561</td>\n",
              "<td>-71.0780575</td>\n",
              "<td>96.0603263</td>\n",
              "<td>0.7711885</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.6</td>\n",
              "<td>0.0496738</td>\n",
              "<td>0.0591293</td>\n",
              "<td>1.6471735</td>\n",
              "<td>0.0222222</td>\n",
              "<td>0.0669556</td>\n",
              "<td>0.6190476</td>\n",
              "<td>0.6060733</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9883041</td>\n",
              "<td>-94.0870695</td>\n",
              "<td>64.7173489</td>\n",
              "<td>0.6221069</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6989011</td>\n",
              "<td>0.0362709</td>\n",
              "<td>0.0591293</td>\n",
              "<td>1.4224503</td>\n",
              "<td>0.0222222</td>\n",
              "<td>0.0437602</td>\n",
              "<td>0.5345912</td>\n",
              "<td>0.5265007</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9941520</td>\n",
              "<td>-94.0870695</td>\n",
              "<td>42.2450256</td>\n",
              "<td>0.4730253</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0304273</td>\n",
              "<td>0.0578439</td>\n",
              "<td>1.25</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0321457</td>\n",
              "<td>0.4697802</td>\n",
              "<td>0.4640272</td>\n",
              "<td>0.0058480</td>\n",
              "<td>1.0</td>\n",
              "<td>-94.2156115</td>\n",
              "<td>25.0</td>\n",
              "<td>0.3204225</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.9076923</td>\n",
              "<td>0.0233827</td>\n",
              "<td>0.0</td>\n",
              "<td>1.1016949</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0257063</td>\n",
              "<td>0.4140436</td>\n",
              "<td>0.4120231</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>10.1694915</td>\n",
              "<td>0.1478873</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0161675</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0195924</td>\n",
              "<td>0.3758242</td>\n",
              "<td>0.3757987</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-12.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-12 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-12 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-12 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table th,\n",
              "#h2o-table-12 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-12 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-12\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th>\n",
              "<th>cv_6_valid</th>\n",
              "<th>cv_7_valid</th>\n",
              "<th>cv_8_valid</th>\n",
              "<th>cv_9_valid</th>\n",
              "<th>cv_10_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.9671497</td>\n",
              "<td>0.0235589</td>\n",
              "<td>0.9347826</td>\n",
              "<td>0.9782609</td>\n",
              "<td>0.9782609</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.9347826</td>\n",
              "<td>0.9555556</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9555556</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9777778</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.9838794</td>\n",
              "<td>0.0178793</td>\n",
              "<td>0.9609375</td>\n",
              "<td>0.9958333</td>\n",
              "<td>0.9979716</td>\n",
              "<td>0.9846154</td>\n",
              "<td>0.9754464</td>\n",
              "<td>0.9801587</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9482759</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9955556</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.0328502</td>\n",
              "<td>0.0235589</td>\n",
              "<td>0.0652174</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0434783</td>\n",
              "<td>0.0652174</td>\n",
              "<td>0.0444444</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0444444</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0222222</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>1.5</td>\n",
              "<td>1.0801234</td>\n",
              "<td>3.0</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>2.0</td>\n",
              "<td>3.0</td>\n",
              "<td>2.0</td>\n",
              "<td>0.0</td>\n",
              "<td>2.0</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.9588513</td>\n",
              "<td>0.0287485</td>\n",
              "<td>0.9482759</td>\n",
              "<td>0.9868421</td>\n",
              "<td>0.9550562</td>\n",
              "<td>0.95</td>\n",
              "<td>0.9090909</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9493671</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.9515683</td>\n",
              "<td>0.0406347</td>\n",
              "<td>0.88</td>\n",
              "<td>0.9677419</td>\n",
              "<td>0.9714286</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8888889</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9677419</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.9454923</td>\n",
              "<td>0.0586199</td>\n",
              "<td>0.8208955</td>\n",
              "<td>0.9493671</td>\n",
              "<td>0.9883721</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9868421</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.7496953</td>\n",
              "<td>0.4955653</td>\n",
              "<td>3.2857144</td>\n",
              "<td>2.875</td>\n",
              "<td>2.7058823</td>\n",
              "<td>2.3</td>\n",
              "<td>3.2857144</td>\n",
              "<td>2.142857</td>\n",
              "<td>1.875</td>\n",
              "<td>2.8125</td>\n",
              "<td>3.2142856</td>\n",
              "<td>3.0</td></tr>\n",
              "<tr><td>logloss</td>\n",
              "<td>0.1662221</td>\n",
              "<td>0.0649039</td>\n",
              "<td>0.2129980</td>\n",
              "<td>0.1471438</td>\n",
              "<td>0.1064336</td>\n",
              "<td>0.2047186</td>\n",
              "<td>0.2311378</td>\n",
              "<td>0.199252</td>\n",
              "<td>0.0746679</td>\n",
              "<td>0.2557887</td>\n",
              "<td>0.0740798</td>\n",
              "<td>0.1560008</td></tr>\n",
              "<tr><td>max_per_class_error</td>\n",
              "<td>0.0647578</td>\n",
              "<td>0.0660564</td>\n",
              "<td>0.2142857</td>\n",
              "<td>0.0625</td>\n",
              "<td>0.0344828</td>\n",
              "<td>0.05</td>\n",
              "<td>0.1428571</td>\n",
              "<td>0.0476191</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0625</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0333333</td></tr>\n",
              "<tr><td>mcc</td>\n",
              "<td>0.9276215</td>\n",
              "<td>0.0547608</td>\n",
              "<td>0.8475655</td>\n",
              "<td>0.9525009</td>\n",
              "<td>0.9549227</td>\n",
              "<td>0.9115385</td>\n",
              "<td>0.8439837</td>\n",
              "<td>0.9107143</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9030172</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9519716</td></tr>\n",
              "<tr><td>mean_per_class_accuracy</td>\n",
              "<td>0.9603280</td>\n",
              "<td>0.0352320</td>\n",
              "<td>0.8928571</td>\n",
              "<td>0.96875</td>\n",
              "<td>0.9827586</td>\n",
              "<td>0.9557692</td>\n",
              "<td>0.9129464</td>\n",
              "<td>0.9553571</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9515086</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9833333</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.0396719</td>\n",
              "<td>0.0352320</td>\n",
              "<td>0.1071429</td>\n",
              "<td>0.03125</td>\n",
              "<td>0.0172414</td>\n",
              "<td>0.0442308</td>\n",
              "<td>0.0870536</td>\n",
              "<td>0.0446429</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0484914</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0166667</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.0441377</td>\n",
              "<td>0.0226749</td>\n",
              "<td>0.0620417</td>\n",
              "<td>0.0377012</td>\n",
              "<td>0.0215032</td>\n",
              "<td>0.0596129</td>\n",
              "<td>0.0706184</td>\n",
              "<td>0.0543299</td>\n",
              "<td>0.0114468</td>\n",
              "<td>0.0693428</td>\n",
              "<td>0.0124989</td>\n",
              "<td>0.0422811</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.9778822</td>\n",
              "<td>0.0236690</td>\n",
              "<td>0.9385305</td>\n",
              "<td>0.9928552</td>\n",
              "<td>0.9966378</td>\n",
              "<td>0.9803309</td>\n",
              "<td>0.9529279</td>\n",
              "<td>0.9820066</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9444352</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9910979</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.9644902</td>\n",
              "<td>0.0315774</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9444444</td>\n",
              "<td>0.95</td>\n",
              "<td>0.9230769</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.8056853</td>\n",
              "<td>0.1029978</td>\n",
              "<td>0.7069637</td>\n",
              "<td>0.8338005</td>\n",
              "<td>0.9077061</td>\n",
              "<td>0.7574214</td>\n",
              "<td>0.6664543</td>\n",
              "<td>0.7817103</td>\n",
              "<td>0.9540083</td>\n",
              "<td>0.6973723</td>\n",
              "<td>0.9416813</td>\n",
              "<td>0.8097349</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.9420238</td>\n",
              "<td>0.0709415</td>\n",
              "<td>0.7857143</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8571429</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.2020618</td>\n",
              "<td>0.0606329</td>\n",
              "<td>0.2490817</td>\n",
              "<td>0.1941680</td>\n",
              "<td>0.1466398</td>\n",
              "<td>0.2441575</td>\n",
              "<td>0.2657412</td>\n",
              "<td>0.2330877</td>\n",
              "<td>0.1069898</td>\n",
              "<td>0.2633303</td>\n",
              "<td>0.1117986</td>\n",
              "<td>0.2056237</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.9786323</td>\n",
              "<td>0.0186053</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9655172</td>\n",
              "<td>0.9615384</td>\n",
              "<td>0.96875</td>\n",
              "<td>0.9583333</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9655172</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9666666</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-13.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-13 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-13 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-13 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table th,\n",
              "#h2o-table-13 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-13 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-13\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Scoring History: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>timestamp</th>\n",
              "<th>duration</th>\n",
              "<th>number_of_trees</th>\n",
              "<th>training_rmse</th>\n",
              "<th>training_logloss</th>\n",
              "<th>training_auc</th>\n",
              "<th>training_pr_auc</th>\n",
              "<th>training_lift</th>\n",
              "<th>training_classification_error</th>\n",
              "<th>validation_rmse</th>\n",
              "<th>validation_logloss</th>\n",
              "<th>validation_auc</th>\n",
              "<th>validation_pr_auc</th>\n",
              "<th>validation_lift</th>\n",
              "<th>validation_classification_error</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>2023-07-03 21:56:51</td>\n",
              "<td> 4.526 sec</td>\n",
              "<td>0.0</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3758242</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6241758</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3596491</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6403509</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-03 21:56:51</td>\n",
              "<td> 4.550 sec</td>\n",
              "<td>5.0</td>\n",
              "<td>0.2251009</td>\n",
              "<td>0.2209849</td>\n",
              "<td>0.9914649</td>\n",
              "<td>0.9879580</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0307692</td>\n",
              "<td>0.2254283</td>\n",
              "<td>0.2225214</td>\n",
              "<td>0.9767792</td>\n",
              "<td>0.9787964</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0438596</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-03 21:56:51</td>\n",
              "<td> 4.571 sec</td>\n",
              "<td>10.0</td>\n",
              "<td>0.1821748</td>\n",
              "<td>0.1441039</td>\n",
              "<td>0.9935034</td>\n",
              "<td>0.9911247</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0307692</td>\n",
              "<td>0.1949417</td>\n",
              "<td>0.1601430</td>\n",
              "<td>0.9772803</td>\n",
              "<td>0.9799751</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-03 21:56:51</td>\n",
              "<td> 4.599 sec</td>\n",
              "<td>15.0</td>\n",
              "<td>0.1723523</td>\n",
              "<td>0.1306105</td>\n",
              "<td>0.9945330</td>\n",
              "<td>0.9927252</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0241758</td>\n",
              "<td>0.1910482</td>\n",
              "<td>0.1526369</td>\n",
              "<td>0.9771133</td>\n",
              "<td>0.9804633</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-03 21:56:51</td>\n",
              "<td> 4.619 sec</td>\n",
              "<td>20.0</td>\n",
              "<td>0.1702960</td>\n",
              "<td>0.1239005</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934925</td>\n",
              "<td>0.1506594</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-03 21:56:51</td>\n",
              "<td> 4.646 sec</td>\n",
              "<td>25.0</td>\n",
              "<td>0.1702965</td>\n",
              "<td>0.1238999</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934951</td>\n",
              "<td>0.1506626</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-03 21:56:51</td>\n",
              "<td> 4.670 sec</td>\n",
              "<td>30.0</td>\n",
              "<td>0.1702950</td>\n",
              "<td>0.1239018</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934870</td>\n",
              "<td>0.1506526</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-03 21:56:51</td>\n",
              "<td> 4.693 sec</td>\n",
              "<td>35.0</td>\n",
              "<td>0.1702896</td>\n",
              "<td>0.1239514</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1933670</td>\n",
              "<td>0.1505057</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-03 21:56:51</td>\n",
              "<td> 4.713 sec</td>\n",
              "<td>36.0</td>\n",
              "<td>0.1705125</td>\n",
              "<td>0.1239493</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1939605</td>\n",
              "<td>0.1512543</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-14.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-14 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-14 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-14 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table th,\n",
              "#h2o-table-14 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-14 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-14\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Variable Importances: </caption>\n",
              "    <thead><tr><th>variable</th>\n",
              "<th>relative_importance</th>\n",
              "<th>scaled_importance</th>\n",
              "<th>percentage</th></tr></thead>\n",
              "    <tbody><tr><td>concave points_worst</td>\n",
              "<td>204.2824097</td>\n",
              "<td>1.0</td>\n",
              "<td>0.3410687</td></tr>\n",
              "<tr><td>area_worst</td>\n",
              "<td>111.7869415</td>\n",
              "<td>0.5472177</td>\n",
              "<td>0.1866388</td></tr>\n",
              "<tr><td>concave points_mean</td>\n",
              "<td>80.5573425</td>\n",
              "<td>0.3943430</td>\n",
              "<td>0.1344981</td></tr>\n",
              "<tr><td>radius_worst</td>\n",
              "<td>72.7002869</td>\n",
              "<td>0.3558813</td>\n",
              "<td>0.1213800</td></tr>\n",
              "<tr><td>perimeter_worst</td>\n",
              "<td>63.8343277</td>\n",
              "<td>0.3124808</td>\n",
              "<td>0.1065774</td></tr>\n",
              "<tr><td>texture_worst</td>\n",
              "<td>22.0757656</td>\n",
              "<td>0.1080649</td>\n",
              "<td>0.0368576</td></tr>\n",
              "<tr><td>concavity_worst</td>\n",
              "<td>20.4532471</td>\n",
              "<td>0.1001224</td>\n",
              "<td>0.0341486</td></tr>\n",
              "<tr><td>texture_mean</td>\n",
              "<td>10.5059090</td>\n",
              "<td>0.0514284</td>\n",
              "<td>0.0175406</td></tr>\n",
              "<tr><td>radius_mean</td>\n",
              "<td>8.4735489</td>\n",
              "<td>0.0414796</td>\n",
              "<td>0.0141474</td></tr>\n",
              "<tr><td>concavity_mean</td>\n",
              "<td>3.3648682</td>\n",
              "<td>0.0164716</td>\n",
              "<td>0.0056180</td></tr>\n",
              "<tr><td>compactness_se</td>\n",
              "<td>0.9133759</td>\n",
              "<td>0.0044711</td>\n",
              "<td>0.0015250</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.model_performance(train).accuracy()"
      ],
      "metadata": {
        "id": "G_PyYFNMs97_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6c5d69-8664-4e6d-efde-4464e8c6e363"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.5147656202316284, 0.9736263736263736]]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "HATr  = best_model.model_performance(train)\n",
        "HATe  = best_model.model_performance(valid)"
      ],
      "metadata": {
        "id": "jKquKjVVJBL9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_h2o = pd.DataFrame(h2o.as_list(best_model.predict(valid)))['predict']\n",
        "y_test_h2o = np.array(valid1['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-caBWGPlp8P",
        "outputId": "f4849ed8-d028-4390-fbf1-adf7395933b5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SFOLD DATA AUTOML\n",
        "#strain, svalid = shdf.split_frame(ratios=[.8], seed=123)\n",
        "shdf  = newdata.copy()\n",
        "#shdf['y_test'] = shdf['y_test'].replace(0,\"B\")\n",
        "#shdf['y_test'] = shdf['y_test'].replace(1,\"M\")\n",
        "shy = \"y_test\"\n",
        "shx = list(shdf.columns)\n",
        "shx.remove(shy)\n",
        "\n",
        "shdf.iloc[:,0:6] = StandardScaler().fit_transform(shdf.iloc[:,0:6])\n",
        "#shdf.iloc[:,-1] = LabelEncoder().fit_transform(shdf.iloc[:,-1])\n",
        "strain1, svalid1 = train_test_split(shdf, test_size=0.2,random_state=123)\n",
        "strain = h2o.H2OFrame(strain1)\n",
        "svalid = h2o.H2OFrame(svalid1)\n",
        "strain[\"y_test\"] = strain[\"y_test\"].asfactor()\n",
        "svalid[\"y_test\"] = svalid[\"y_test\"].asfactor()\n",
        "\n",
        "st = time.time()\n",
        "saml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\", nfolds=10, sort_metric='accuracy')\n",
        "saml.train(x = shx, y = shy, training_frame = strain, validation_frame = svalid)\n",
        "sautoend = time.time() - st\n",
        "sbest_model = saml.get_best_model()\n",
        "sHATr  = sbest_model.model_performance(strain)\n",
        "sHATe  = sbest_model.model_performance(svalid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMDqylVK7svS",
        "outputId": "6a258e85-5704-4af3-8789-e895106598af"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "AutoML progress: |\n",
            "21:57:54.471: Project: AutoML_2_20230703_215754\n",
            "21:57:54.471: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "21:57:54.473: Setting stopping tolerance adaptively based on the training frame: 0.04688072309384954\n",
            "21:57:54.473: Build control seed: 123\n",
            "21:57:54.473: training frame: Frame key: AutoML_2_20230703_215754_training_py_11_sid_8a29    cols: 7    rows: 455  chunks: 1    size: 6691  checksum: -8069984204909073984\n",
            "21:57:54.474: validation frame: Frame key: py_12_sid_8a29    cols: 7    rows: 114  chunks: 1    size: 2557  checksum: -5131178475415970944\n",
            "21:57:54.474: leaderboard frame: NULL\n",
            "21:57:54.474: blending frame: NULL\n",
            "21:57:54.474: response column: y_test\n",
            "21:57:54.474: fold column: null\n",
            "21:57:54.474: weights column: null\n",
            "21:57:54.475: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "21:57:54.477: AutoML job created: 2023.07.03 21:57:54.471\n",
            "21:57:54.483: AutoML build started: 2023.07.03 21:57:54.483\n",
            "21:57:54.484: AutoML: starting XGBoost_1_AutoML_2_20230703_215754 model training\n",
            "\n",
            "\n",
            "21:57:56.134: New leader: XGBoost_1_AutoML_2_20230703_215754, accuracy: 0.9296703296703297\n",
            "21:57:56.135: AutoML: starting GLM_1_AutoML_2_20230703_215754 model training\n",
            "\n",
            "█\n",
            "21:57:56.782: AutoML: starting GBM_1_AutoML_2_20230703_215754 model training\n",
            "\n",
            "█\n",
            "21:57:58.204: AutoML: starting XGBoost_2_AutoML_2_20230703_215754 model training\n",
            "\n",
            "██\n",
            "21:58:00.264: AutoML: starting DRF_1_AutoML_2_20230703_215754 model training\n",
            "\n",
            "█\n",
            "21:58:01.617: AutoML: starting GBM_2_AutoML_2_20230703_215754 model training\n",
            "\n",
            "█\n",
            "21:58:03.117: AutoML: starting GBM_3_AutoML_2_20230703_215754 model training\n",
            "\n",
            "██\n",
            "21:58:04.649: AutoML: starting GBM_4_AutoML_2_20230703_215754 model training\n",
            "\n",
            "█\n",
            "21:58:06.76: AutoML: starting XGBoost_3_AutoML_2_20230703_215754 model training\n",
            "\n",
            "███\n",
            "21:58:07.755: AutoML: starting XRT_1_AutoML_2_20230703_215754 model training\n",
            "21:58:09.308: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "21:58:09.310: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_2_20230703_215754 model training\n",
            "\n",
            "███████████████████████████████████████████████████| (done) 100%\n",
            "\n",
            "21:58:11.201: AutoML: starting StackedEnsemble_AllModels_1_AutoML_2_20230703_215754 model training\n",
            "21:58:12.953: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "21:58:12.953: AutoML build stopped: 2023.07.03 21:58:12.953\n",
            "21:58:12.953: AutoML build done: built 10 models\n",
            "21:58:12.953: AutoML duration: 18.470 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_sh2o = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))['predict']\n",
        "y_test_sh2o = np.array(svalid1['y_test']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivMaaDNNmYY2",
        "outputId": "c0ffe2dc-e21f-4a17-c823-6e06d88eeb6b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "nJrcI3Gn8we2",
        "outputId": "b80777ed-9013-4c4e-998e-0f05e3211638"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 7 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]\n",
        "\n",
        "label = [\"ArtificialNeuralNetwork\", 'DeepNeuralNetwork',\n",
        "         'KNearestNeighborsClassifier', 'LogisticRegression',\n",
        "         'RandomForestClassifier', 'SupportVectorClassifier',\n",
        "         'XGBoost', type(best_model).__name__, type(sbest_model).__name__ ]\n",
        "\n",
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"DNN\":[DTr,DTe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"H_OD\":[HATr.accuracy()[0][1],HATe.accuracy()[0][1]],\n",
        "    \"H_SOD\":[sHATr.accuracy()[0][1],sHATe.accuracy()[0][1]]\n",
        "    })\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc['Model'] = label\n",
        "\n",
        "acc = acc[['Model', 'train', 'test']]\n",
        "acc['avg'] = round((acc['train'] + acc['test'])/2, 6)\n",
        "acc[acc[\"avg\"] == acc[\"avg\"].max()]\n",
        "acc['BestModel'] = 0\n",
        "for i in range(len(acc)):\n",
        "  if acc['avg'][i] >= 90 and acc['avg'][i] < acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"good\"\n",
        "  elif acc['avg'][i] == acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"best\"\n",
        "  else:\n",
        "    acc.iloc[i,-1] = \"not good\"\n",
        "\n",
        "acc[\"Precision\"] = np.zeros(len(acc))\n",
        "acc[\"Recall\"]    = np.zeros(len(acc))\n",
        "acc[\"F1_Score\"]  = np.zeros(len(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ba6kRO-eI60S"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ANNn = []\n",
        "y_pred_DNNn = []\n",
        "for i in range(len(y_pred_ANN)):\n",
        "  y_pred_ANNn.append(y_pred_ANN[i][0])\n",
        "  y_pred_DNNn.append(y_pred_DNN[i][0])"
      ],
      "metadata": {
        "id": "S7pf8G6ao4oF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [np.array(y_pred_ANNn), np.array(y_pred_DNNn), y_pred_knn,\n",
        "        y_pred_lr, y_pred_rf,\n",
        "        y_pred_svm, y_pred_xgb, y_pred_h2o, y_pred_sh2o]\n",
        "\n",
        "tes  = [y_test_indi_ML, np.array(Dy_test), y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_indi_ML, y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_h2o.copy(), y_test_sh2o.copy()]"
      ],
      "metadata": {
        "id": "D2n2VnW1jKp4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pred)):\n",
        "  p,r,f,_ = precision_recall_fscore_support(tes[i], pred[i],\n",
        "                                            average='macro')\n",
        "  acc.iloc[i,5]= p\n",
        "  acc.iloc[i,6]= r\n",
        "  acc.iloc[i,7]= f\n",
        "  p = 0\n",
        "  r = 0\n",
        "  f = 0\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "2DlSQ29moN9T",
        "outputId": "3ddc1a51-1623-40df-f854-cdd756ffbdd7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Model     train      test       avg BestModel  \\\n",
              "ANN        ArtificialNeuralNetwork  0.947253  0.973684  0.960468  not good   \n",
              "DNN              DeepNeuralNetwork  0.951648  0.921053  0.936350  not good   \n",
              "KNN    KNearestNeighborsClassifier  0.980220  0.991228  0.985724  not good   \n",
              "LR              LogisticRegression  0.980220  0.973684  0.976952  not good   \n",
              "RF          RandomForestClassifier  0.975824  0.991228  0.983526  not good   \n",
              "SVM        SupportVectorClassifier  0.951648  0.956140  0.953894  not good   \n",
              "XGB                        XGBoost  0.989011  0.982456  0.985734      best   \n",
              "H_OD           H2OXGBoostEstimator  0.973626  0.964912  0.969269  not good   \n",
              "H_SOD          H2OXGBoostEstimator  0.936264  0.912281  0.924272  not good   \n",
              "\n",
              "       Precision    Recall  F1_Score  \n",
              "ANN     0.973986  0.968760  0.971277  \n",
              "DNN     0.927350  0.900936  0.911757  \n",
              "KNN     0.993243  0.987805  0.990426  \n",
              "LR      0.980263  0.963415  0.970946  \n",
              "RF      0.993243  0.987805  0.990426  \n",
              "SVM     0.967949  0.939024  0.950976  \n",
              "XGB     0.986667  0.975610  0.980743  \n",
              "H_OD    0.958074  0.967257  0.962302  \n",
              "H_SOD   0.914005  0.894086  0.902564  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab13caa3-acc2-42b9-b983-baa539292d7f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>avg</th>\n",
              "      <th>BestModel</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>ArtificialNeuralNetwork</td>\n",
              "      <td>0.947253</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.960468</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.973986</td>\n",
              "      <td>0.968760</td>\n",
              "      <td>0.971277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>DeepNeuralNetwork</td>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.936350</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.927350</td>\n",
              "      <td>0.900936</td>\n",
              "      <td>0.911757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>KNearestNeighborsClassifier</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.985724</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.976952</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.963415</td>\n",
              "      <td>0.970946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.975824</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.983526</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>SupportVectorClassifier</td>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.953894</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.967949</td>\n",
              "      <td>0.939024</td>\n",
              "      <td>0.950976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.989011</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.985734</td>\n",
              "      <td>best</td>\n",
              "      <td>0.986667</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.980743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_OD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.973626</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.969269</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.958074</td>\n",
              "      <td>0.967257</td>\n",
              "      <td>0.962302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_SOD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.936264</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.924272</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.914005</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.902564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab13caa3-acc2-42b9-b983-baa539292d7f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab13caa3-acc2-42b9-b983-baa539292d7f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab13caa3-acc2-42b9-b983-baa539292d7f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ru4oH6-YzGPw",
        "outputId": "158174a9-6fb6-4072-812a-aac9e6201da9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 9 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApHElEQVR4nO3de1RVdd7H8c8BBTQCvCQoc0a8pOZoYF4Y7KY+GDrlZJmSo6FoltaYRhfFFEpTtMdbM5qmgdqaYbxVjlnZOBQ2JY5PKk+1Hi/jqMmooKQBYWHCfv5ocerE9SDHnxzfr7X2WvI7v98+vy9s3B9+e59zbJZlWQIAADDEy/QEAADAtY0wAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMCoRqYnUBtlZWU6deqUrr/+etlsNtPTAQAAtWBZloqKitSmTRt5eVW9/tEgwsipU6dkt9tNTwMAANRBTk6OfvGLX1T5eIMII9dff72kH4oJCAgwPBsAAFAbhYWFstvtjvN4VRpEGCm/NBMQEEAYAQCgganpFgtuYAUAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUS6HkY8++khDhgxRmzZtZLPZtGXLlhrHZGZm6pZbbpGvr686duyotWvX1mGqAADAE7kcRoqLixUeHq7ly5fXqv+xY8d09913q3///srOztbUqVP18MMP6/3333d5sgAAwPO4/KZngwcP1uDBg2vdf+XKlWrXrp0WLVokSbrpppv08ccfa8mSJYqJiXH16QEAgIdx+z0jWVlZio6OdmqLiYlRVlZWlWNKSkpUWFjotAEAAM/k9jCSm5ur4OBgp7bg4GAVFhbq22+/rXRMSkqKAgMDHRsfkgcAgOe6Kl9Nk5iYqIKCAseWk5NjekoAAMBN3P5BeSEhIcrLy3Nqy8vLU0BAgJo0aVLpGF9fX/n6+rp7agAA4Crg9pWRqKgoZWRkOLXt2LFDUVFR7n5qAADQALgcRr755htlZ2crOztb0g8v3c3OztaJEyck/XCJJS4uztF/4sSJOnr0qJ599lkdPHhQr7zyijZu3Kgnn3yyfioAAFwWm83mlg2oLZfDyKeffqoePXqoR48ekqSEhAT16NFDSUlJkqTTp087gokktWvXTu+884527Nih8PBwLVq0SK+99hov6wUAAJIkm2VZlulJ1KSwsFCBgYEqKChQQECA6ekAgEdx1ypGAzi9wM1qe/52+w2sQH1yx3+apv/D5EQA4Fp3Vb60FwAAXDtYGQHgFp64igXAPQgjAACPRCBuOLhMAwAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAo3mfEQ/H6esA9+N0C6t81H0b4XBAAAMziMg0AADCKMAIAAIwijAAAAKOu+XtGAABoSDzxJmpWRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYVacwsnz5coWFhcnPz0+RkZHas2dPtf2XLl2qzp07q0mTJrLb7XryySf13Xff1WnCAADAs7gcRjZs2KCEhAQlJydr3759Cg8PV0xMjM6cOVNp//T0dE2fPl3Jyck6cOCAUlNTtWHDBs2YMeOyJw8AABo+l8PI4sWLNWHCBMXHx6tr165auXKlmjZtqrS0tEr779q1S7feeqt+97vfKSwsTHfddZdGjhxZ42oKAAC4NrgURi5evKi9e/cqOjr6xx14eSk6OlpZWVmVjunbt6/27t3rCB9Hjx7Vu+++q9/85jeXMW0AAOApGrnSOT8/X6WlpQoODnZqDw4O1sGDBysd87vf/U75+fm67bbbZFmWLl26pIkTJ1Z7maakpEQlJSWOrwsLC12ZJgAAaEDc/mqazMxMzZs3T6+88or27dunN998U++8847mzJlT5ZiUlBQFBgY6Nrvd7u5pAgAAQ1xaGWnZsqW8vb2Vl5fn1J6Xl6eQkJBKx8yaNUsPPfSQHn74YUlS9+7dVVxcrEceeUTPPfecvLwq5qHExEQlJCQ4vi4sLCSQAADgoVxaGfHx8VHPnj2VkZHhaCsrK1NGRoaioqIqHXPhwoUKgcPb21uSZFlWpWN8fX0VEBDgtAEAAM/k0sqIJCUkJGjMmDHq1auX+vTpo6VLl6q4uFjx8fGSpLi4OIWGhiolJUWSNGTIEC1evFg9evRQZGSkjhw5olmzZmnIkCGOUAIAAK5dLoeR2NhYnT17VklJScrNzVVERIS2b9/uuKn1xIkTTishM2fOlM1m08yZM3Xy5EndcMMNGjJkiObOnVt/VQAAgAbLZlV1reQqUlhYqMDAQBUUFNT7JRubzVav+ytn+tvqjrpM1yR5Zl0cg7VnuibJM+viGKw90zVJDauu2p6/+WwaAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgVJ3CyPLlyxUWFiY/Pz9FRkZqz5491fb/+uuv9fjjj6t169by9fVVp06d9O6779ZpwgAAwLM0cnXAhg0blJCQoJUrVyoyMlJLly5VTEyMDh06pFatWlXof/HiRQ0cOFCtWrXS5s2bFRoaqi+//FJBQUH1MX8AANDA2SzLslwZEBkZqd69e2vZsmWSpLKyMtntdk2ePFnTp0+v0H/lypX67//+bx08eFCNGzeu0yQLCwsVGBiogoICBQQE1GkfVbHZbPW6v3IuflvrnTvqMl2T5Jl1cQzWnumaJM+si2Ow9kzXJDWsump7/nbpMs3Fixe1d+9eRUdH/7gDLy9FR0crKyur0jFbt25VVFSUHn/8cQUHB6tbt26aN2+eSktLq3yekpISFRYWOm0AAMAzuRRG8vPzVVpaquDgYKf24OBg5ebmVjrm6NGj2rx5s0pLS/Xuu+9q1qxZWrRokV588cUqnyclJUWBgYGOzW63uzJNAADQgLj91TRlZWVq1aqVVq1apZ49eyo2NlbPPfecVq5cWeWYxMREFRQUOLacnBx3TxMAABji0g2sLVu2lLe3t/Ly8pza8/LyFBISUumY1q1bq3HjxvL29na03XTTTcrNzdXFixfl4+NTYYyvr698fX1dmRoAAGigXFoZ8fHxUc+ePZWRkeFoKysrU0ZGhqKioiodc+utt+rIkSMqKytztB0+fFitW7euNIgAAIBri8uXaRISErR69WqtW7dOBw4c0KRJk1RcXKz4+HhJUlxcnBITEx39J02apHPnzmnKlCk6fPiw3nnnHc2bN0+PP/54/VUBAAAaLJffZyQ2NlZnz55VUlKScnNzFRERoe3btztuaj1x4oS8vH7MOHa7Xe+//76efPJJ3XzzzQoNDdWUKVM0bdq0+qsCAAA0WC6/z4gJvM+I6xrS69Bd4Yl1cQzWnumaJM+si2Ow9kzXJDWsutzyPiMAAAD1jTACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAqDqFkeXLlyssLEx+fn6KjIzUnj17ajVu/fr1stlsGjp0aF2eFgAAeCCXw8iGDRuUkJCg5ORk7du3T+Hh4YqJidGZM2eqHXf8+HE9/fTTuv322+s8WQAA4HlcDiOLFy/WhAkTFB8fr65du2rlypVq2rSp0tLSqhxTWlqqUaNG6YUXXlD79u0va8IAAMCzuBRGLl68qL179yo6OvrHHXh5KTo6WllZWVWOmz17tlq1aqXx48fX6nlKSkpUWFjotAEAAM/kUhjJz89XaWmpgoODndqDg4OVm5tb6ZiPP/5YqampWr16da2fJyUlRYGBgY7Nbre7Mk0AANCAuPXVNEVFRXrooYe0evVqtWzZstbjEhMTVVBQ4NhycnLcOEsAAGBSI1c6t2zZUt7e3srLy3Nqz8vLU0hISIX+//73v3X8+HENGTLE0VZWVvbDEzdqpEOHDqlDhw4Vxvn6+srX19eVqQEAgAbKpZURHx8f9ezZUxkZGY62srIyZWRkKCoqqkL/Ll266PPPP1d2drZj++1vf6v+/fsrOzubyy8AAMC1lRFJSkhI0JgxY9SrVy/16dNHS5cuVXFxseLj4yVJcXFxCg0NVUpKivz8/NStWzen8UFBQZJUoR0AAFybXA4jsbGxOnv2rJKSkpSbm6uIiAht377dcVPriRMn5OXFG7sCAIDasVmWZZmeRE0KCwsVGBiogoICBQQE1Ou+bTZbve6vnOlvqzvqMl2T5Jl1cQzWnumaJM+si2Ow9kzXJDWsump7/mYJAwAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYFSdwsjy5csVFhYmPz8/RUZGas+ePVX2Xb16tW6//XY1a9ZMzZo1U3R0dLX9AQDAtcXlMLJhwwYlJCQoOTlZ+/btU3h4uGJiYnTmzJlK+2dmZmrkyJH68MMPlZWVJbvdrrvuuksnT5687MkDAICGz2ZZluXKgMjISPXu3VvLli2TJJWVlclut2vy5MmaPn16jeNLS0vVrFkzLVu2THFxcbV6zsLCQgUGBqqgoEABAQGuTLdGNputXvdXzsVva71zR12ma5I8sy6OwdozXZPkmXVxDNae6ZqkhlVXbc/fLq2MXLx4UXv37lV0dPSPO/DyUnR0tLKysmq1jwsXLuj7779X8+bNq+xTUlKiwsJCpw0AAHgml8JIfn6+SktLFRwc7NQeHBys3NzcWu1j2rRpatOmjVOg+bmUlBQFBgY6Nrvd7so0AQBAA3JFX00zf/58rV+/Xm+99Zb8/Pyq7JeYmKiCggLHlpOTcwVnCQAArqRGrnRu2bKlvL29lZeX59Sel5enkJCQascuXLhQ8+fP19///nfdfPPN1fb19fWVr6+vK1MDAAANlEsrIz4+PurZs6cyMjIcbWVlZcrIyFBUVFSV41566SXNmTNH27dvV69eveo+WwAA4HFcWhmRpISEBI0ZM0a9evVSnz59tHTpUhUXFys+Pl6SFBcXp9DQUKWkpEiSFixYoKSkJKWnpyssLMxxb4m/v7/8/f3rsRQAANAQuRxGYmNjdfbsWSUlJSk3N1cRERHavn2746bWEydOyMvrxwWXFStW6OLFi3rggQec9pOcnKznn3/+8mYPAAAaPJffZ8QE3mfEdQ3pdeiu8MS6OAZrz3RNkmfWxTFYe6ZrkhpWXW55nxEAAID6RhgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgVJ3CyPLlyxUWFiY/Pz9FRkZqz5491fbftGmTunTpIj8/P3Xv3l3vvvtunSYLAAA8j8thZMOGDUpISFBycrL27dun8PBwxcTE6MyZM5X237Vrl0aOHKnx48dr//79Gjp0qIYOHaovvvjisicPAAAaPptlWZYrAyIjI9W7d28tW7ZMklRWVia73a7Jkydr+vTpFfrHxsaquLhY27Ztc7T9+te/VkREhFauXFmr5ywsLFRgYKAKCgoUEBDgynRrZLPZ6nV/5Vz8ttY7d9RluibJM+viGKw90zVJnlkXx2Dtma5Jalh11fb83ciVnV68eFF79+5VYmKio83Ly0vR0dHKysqqdExWVpYSEhKc2mJiYrRly5Yqn6ekpEQlJSWOrwsKCiT9UFRD0ZDmWlueWJNEXQ2JJ9YkUVdD4ok1Se6rq3y/NYUdl8JIfn6+SktLFRwc7NQeHBysgwcPVjomNze30v65ublVPk9KSopeeOGFCu12u92V6RoVGBhoegr1zhNrkqirIfHEmiTqakg8sSbJ/XUVFRVV+xwuhZErJTEx0Wk1paysTOfOnVOLFi3ctpxYk8LCQtntduXk5NT7pSKTPLEuT6xJoq6GxBNrkqirIblaarIsS0VFRWrTpk21/VwKIy1btpS3t7fy8vKc2vPy8hQSElLpmJCQEJf6S5Kvr698fX2d2oKCglyZqtsEBAR4zMH6U55YlyfWJFFXQ+KJNUnU1ZBcDTXVZtXFpVfT+Pj4qGfPnsrIyHC0lZWVKSMjQ1FRUZWOiYqKcuovSTt27KiyPwAAuLa4fJkmISFBY8aMUa9evdSnTx8tXbpUxcXFio+PlyTFxcUpNDRUKSkpkqQpU6bozjvv1KJFi3T33Xdr/fr1+vTTT7Vq1ar6rQQAADRILoeR2NhYnT17VklJScrNzVVERIS2b9/uuEn1xIkT8vL6ccGlb9++Sk9P18yZMzVjxgzdeOON2rJli7p161Z/VVwBvr6+Sk5OrnD5qKHzxLo8sSaJuhoST6xJoq6GpKHV5PL7jAAAANQnPpsGAAAYRRgBAABGEUYAAIBRhBEAAGDUNR1GsrKy5O3trbvvvtup/fjx47LZbGrVqpWKioqcHouIiNDzzz/v+Lpfv36y2Wxav369U7+lS5cqLCzMXVOvYOzYsbLZbLLZbGrcuLGCg4M1cOBApaWlqayszNEvLCxMNptNu3fvdho/depU9evXz/H1888/L5vNpokTJzr1y87Ols1m0/Hjx91ZjpOxY8dq6NChTm2bN2+Wn5+fFi1a5Kh9/vz5Tn22bNni9I69mZmZstls+tWvfqXS0lKnvkFBQVq7dq27SnBJZfWWK//52Ww2NW3aVN27d9drr712ZSdYBz8/Ptu1a6dnn31W3333naNP+eM/3W677TaDs/7R2bNnNWnSJP3yl7+Ur6+vQkJCFBMTo507d6ply5YVjr1yc+bMUXBwsL7//nutXbtWNptNN910U4V+mzZtks1mu6L/Z5SWlqpv3766//77ndoLCgpkt9v13HPPOdreeOMNDRgwQM2aNVOTJk3UuXNnjRs3Tvv373f0Ka+vfPP391fPnj315ptvurWOqn5fyn/fv/766xr3UVpaqiVLlqh79+7y8/NTs2bNNHjwYH3yySdO/X5ao7e3t5o1a6bIyEjNnj3b8RlqpmpYvXq1wsPD5e/vr6CgIPXo0cPxFhvlzp07p6lTp6pt27by8fFRmzZtNG7cOJ04caLCfGpzPqlP13QYSU1N1eTJk/XRRx/p1KlTFR4vKirSwoULa9yPn5+fZs6cqe+//94d06y1QYMG6fTp0zp+/Ljee+899e/fX1OmTNE999yjS5cuOfr5+flp2rRpNe7Pz89Pqamp+te//uXOabvstdde06hRo7RixQo99dRTkn6Y64IFC3T+/Pkaxx89elSvv/66u6fpNrNnz9bp06f1xRdfaPTo0ZowYYLee+8909OqUfnxefToUS1ZskSvvvqqkpOTnfqsWbNGp0+fdmxbt241NFtnw4YN0/79+7Vu3TodPnxYW7duVb9+/VRQUKDRo0drzZo1FcZYlqW1a9cqLi5OjRs3liRdd911OnPmTIUPFk1NTdUvf/nLK1JLOW9vb61du1bbt2/Xn//8Z0f75MmT1bx5c8fPZtq0aYqNjVVERIS2bt2qQ4cOKT09Xe3bt3f60FTph3f7LP/Z7d+/XzExMRoxYoQOHTp0RWtzhWVZevDBBzV79mxNmTJFBw4cUGZmpux2u/r161fhQ13La/zPf/6jXbt26ZFHHtHrr7+uiIiISs8jV0JaWpqmTp2qJ554QtnZ2frkk0/07LPP6ptvvnH0OXfunH7961/r73//u1auXKkjR45o/fr1OnLkiHr37q2jR4867bO255N6Y12jioqKLH9/f+vgwYNWbGysNXfuXMdjx44dsyRZzzzzjOXv72/l5eU5HgsPD7eSk5MdX995551WfHy81aJFC2v58uWO9iVLllht27a9EqVYlmVZY8aMse69994K7RkZGZYka/Xq1ZZlWVbbtm2tJ554wvLx8bHeeecdR78pU6ZYd955p+Pr5ORkKzw83Bo4cKA1fPhwR/v+/fstSdaxY8fcVUoFP61twYIFlp+fn/Xmm286PX7PPfdYXbp0sZ555hlH+1tvvWX99BD/8MMPHT9Xu91ufffdd47HAgMDrTVr1ri9ltqo6mdpWT/8/JYsWeLU1rx5c+vJJ590/8QuQ2U13X///VaPHj0cX0uy3nrrrSs7sVo4f/68JcnKzMys9PHPPvvMkmT94x//cGovP94OHDhgWZZlrVmzxgoMDLR+//vfWw8//LCjX05OjuXr62tNnz79iv6fUe7ll1+2mjVrZp06dcrasmWL1bhxYys7O9uyLMvKysqyJFkvv/xypWPLysoc/y6v76dKS0utxo0bWxs3bnTb/Kv6fSn//p8/f77a8evXr7ckWVu3bq3w2P3332+1aNHC+uabbyzLqrxGy7KsvLw8q2XLltaoUaPqUsJl13DvvfdaY8eOrbbPxIkTreuuu846ffq0U/uFCxes0NBQa9CgQTXO5+fnk/p0za6MbNy4UV26dFHnzp01evRopaWlVfiI45EjR6pjx46aPXt2tfsKCAjQc889p9mzZ6u4uNid03bZgAEDFB4e7rRU2q5dO02cOFGJiYk1LrnNnz9fb7zxhj799FN3T7VG06ZN05w5c7Rt2zbdd999To95e3tr3rx5+uMf/6j//Oc/1e5n6tSpunTpkv74xz+6c7puV1ZWpjfeeEPnz5+Xj4+P6em45IsvvtCuXbsaxLz9/f3l7++vLVu2qKSkpMLj3bt3V+/evZWWlubUvmbNGvXt21ddunRxah83bpw2btyoCxcuSPph6X/QoEEVPt38Spk8ebLCw8P10EMP6ZFHHlFSUpLCw8MlSX/5y1/k7++vxx57rNKx1X1waWlpqdatWydJuuWWW+p/4vUkPT1dnTp10pAhQyo89tRTT+mrr77Sjh07qt1Hq1atNGrUKG3durXCJeArISQkRLt379aXX35Z6eNlZWVav369Ro0aVeFz4Zo0aaLHHntM77//vs6dO1ft81R2Pqkv12wYSU1N1ejRoyX9sBxVUFCgnTt3OvUpvw9h1apV+ve//13t/h577DH5+flp8eLFbptzXXXp0qXCPR4zZ87UsWPHnJZnK3PLLbdoxIgRtbqs407vvfeeXnrpJf31r3/Vf/3Xf1Xa57777lNERESFpf+fa9q0qZKTk5WSknJZ13lNmTZtmvz9/eXr66sHHnhAzZo108MPP2x6WjXatm2b/P395efnp+7du+vMmTN65plnnPqMHDnScfIvDwCmNWrUSGvXrtW6desUFBSkW2+9VTNmzNBnn33m6DN+/Hht2rTJsSxeVFSkzZs3a9y4cRX216NHD7Vv316bN292XMqprN+VYrPZtGLFCmVkZCg4OFjTp093PHb48GG1b99ejRr9+GbdixcvdvoZ/fR3qKCgwNHu4+OjSZMmadWqVerQoYNbayg/tn66DR48uFZjDx8+XOl9PJIc7YcPH65xP126dFFRUZG++uqr2k/8Jy6nhuTkZAUFBSksLEydO3fW2LFjtXHjRscfm2fPntXXX39dbZ2WZenIkSM1Pldl55P6cE2GkUOHDmnPnj0aOXKkpB/+s4mNjVVqamqFvjExMbrttts0a9asavfp6+ur2bNna+HChcrPz3fLvOvKsqwKf8HccMMNevrpp5WUlKSLFy9WO/7FF1/UP/7xD/3tb39z5zSrdfPNNyssLEzJyclO10F/bsGCBVq3bp0OHDhQ7f7Gjx+vFi1aaMGCBfU9Vbd75plnlJ2drQ8++ECRkZFasmSJOnbsaHpaNerfv7+ys7P1z3/+U2PGjFF8fLyGDRvm1GfJkiXKzs52bAMHDjQ0W2fDhg3TqVOntHXrVg0aNEiZmZm65ZZbHDc9jxw5UqWlpdq4caMkacOGDfLy8lJsbGyl+xs3bpzWrFmjnTt3qri4WL/5zW+uVCmVSktLU9OmTXXs2LEaVxbHjRun7OxsvfrqqyouLnZaUb7++usdP7v9+/dr3rx5mjhxot5++223zr/82Prp5sqN3T9fFa+L8n1Ut1pUncupoXXr1srKytLnn3+uKVOm6NKlSxozZowGDRrktPpdX3XWtcbqXJNhJDU1VZcuXVKbNm3UqFEjNWrUSCtWrNAbb7xR6V/K8+fP14YNG5zuHK/M6NGj1bZtW7344ovumnqdHDhwQO3atavQnpCQoG+//VavvPJKteM7dOigCRMmaPr06fVyMNdFaGioMjMzdfLkSQ0aNKjCq5zK3XHHHYqJialwY93PNWrUSHPnztXLL79s7KazumrZsqU6duyo22+/XZs2bdITTzyh//u//zM9rRpdd9116tixo8LDw5WWlqZ//vOfFf4ACAkJUceOHR3bddddZ2i2Ffn5+WngwIGaNWuWdu3apbFjxzpW4QICAvTAAw84bmRds2aNRowYIX9//0r3NWrUKO3evVvPP/+8HnroIaeVhytt165dWrJkibZt26Y+ffpo/Pjxjt/zG2+8UUePHnW6OT8oKEgdO3ZUaGhohX15eXk5fnY333yzEhIS1K9fP7eH/vJj66dbZfOrTKdOnar846W8vVOnTjXu58CBAwoICFCLFi1qP/GfuJwaynXr1k2PPfaY/vSnP2nHjh3asWOHdu7cqRtuuEFBQUHV1mmz2Wr1R01V55PLdc2FkUuXLun111/XokWLnBLo//7v/6pNmzb6y1/+UmFMnz59dP/99zstX1bGy8tLKSkpWrFixRV96Wt1PvjgA33++ecV/gKVfrgWPmvWLM2dO7fKk3u5pKQkHT58uMJLmK+ktm3baufOncrNza02kMyfP19vv/12hVcs/Nzw4cP1q1/9Si+88II7pntF2O12xcbG1hi+rjZeXl6aMWOGZs6cqW+//db0dOqka9euTveIjR8/Xh9//LG2bdumXbt2afz48VWObd68uX77299q586dRi/RXLhwQWPHjtWkSZPUv39/paamas+ePVq5cqWkH1Z8vvnmmxr/YKmOt7f3Vf0zfvDBB/Wvf/2r0tWbRYsWqUWLFjWu0J05c0bp6ekaOnSo0wfFmtS1a1dJUnFxsby8vDRixAilp6crNzfXqV/5H6QxMTFq3rx5tfus7nxyua6O79oVtG3bNp0/f17jx49Xt27dnLZhw4ZVeqlGkubOnasPPvigxpeo3X333YqMjNSrr77qjulXq6SkRLm5uTp58qT27dunefPm6d5779U999yjuLi4Ssc88sgjCgwMVHp6erX7Dg4OVkJCgv7whz+4Y+q1ZrfblZmZqTNnzigmJkaFhYUV+nTv3l2jRo2q1Vznz5+vtLS0q+7G44KCggpLtjk5OZX2nTJlit5+++2r4iZjVwwfPlze3t5avny56alU66uvvtKAAQP0pz/9SZ999pmOHTumTZs26aWXXtK9997r6HfHHXeoY8eOiouLU5cuXdS3b99q97t27Vrl5+dXuMH1SkpMTJRlWY73SQkLC9PChQv17LPP6vjx44qKitJTTz2lp556SgkJCfr444/15Zdfavfu3UpNTZXNZnM6+VqWpdzcXOXm5urYsWNatWqV3n//fafv09XmwQcf1H333acxY8YoNTVVx48f12effaZHH31UW7du1Wuvvea0Qlde4+nTp3XgwAGlpaWpb9++CgwMrPL9Ztxt0qRJmjNnjj755BPHzycuLk433HCDoqKiJEnz5s1TSEiIBg4cqPfee085OTn66KOPFBMTo++//77C72FdzieX45oLI6mpqYqOjlZgYGCFx4YNG6ZPP/200hNcp06dNG7cOKc3aarKggULatWvvm3fvl2tW7dWWFiYBg0apA8//FB/+MMf9Ne//lXe3t6VjmncuLHmzJlTq/k+/fTTVS47X0m/+MUvlJmZqfz8/CoDyezZs2v15jwDBgzQgAED3PO6+cuQmZmpHj16OG1VreB07dpVd911l5KSkq7wLC9Po0aN9Pvf/14vvfTSVRcGf8rf399xb84dd9yhbt26adasWZowYYKWLVvm6Gez2TRu3DidP3++VqsdTZo0qfOSfn3YuXOnli9frjVr1qhp06aO9kcffVR9+/Z1XK5ZuHCh0tPTtX//ft1zzz268cYbNXz4cJWVlSkrK0sBAQGOsYWFhWrdurVat26tm266SYsWLdLs2bOd3kDtamOz2bRx40bNmDFDS5YsUefOnXX77bfryy+/VGZmZoU3IyuvMTQ0VFFRUXr11Vc1ZswY7d+/X61btzZSQ3R0tHbv3q3hw4erU6dOGjZsmPz8/JSRkeE4xlq0aKHdu3erf//+evTRR9WhQweNGDFCHTp00P/8z/+offv2Tvusy/nkctgsUzcBAAAA6BpcGQEAAFcXwggAwGMNHjy4wvt3lG/z5s0zPb1a8YQaasJlGgCAxzp58mSVr+Zp3rx5ja8guRp4Qg01IYwAAACjuEwDAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMOr/Acmj4508w+z3AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve"
      ],
      "metadata": {
        "id": "Qma6hWrmDBvf"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]"
      ],
      "metadata": {
        "id": "03rwpiYpoAGs"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()"
      ],
      "metadata": {
        "id": "x5wrzn-qPaXw"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "C80ESe__xB5x"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
        "\n",
        "y_pred = ANNmodel.predict(X_test).ravel()\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[0]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = model.predict(DX_test).ravel()\n",
        "y_test = Dy_test.copy()\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[1]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = knn.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[2]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = lr.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[3]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = rf.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[4]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = svm.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[5]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = xgb.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[6]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(best_model.predict(valid)))\n",
        "y_test = h2o.as_list(valid['diagnosis'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(best_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))\n",
        "y_test = h2o.as_list(svalid['y_test'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(sbest_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('AUC ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "4Bi4CMUJm6yH",
        "outputId": "58e188f2-8c21-41fa-abdd-aaba88734c5b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n",
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tm = [aend,dend,kend,lend,rend,send,xend,autoend,sautoend]"
      ],
      "metadata": {
        "id": "6vUubKKMxLIP"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['time'] = tm\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "_1wKoAe9wIPn",
        "outputId": "1adbf3fb-dd33-445a-b74b-4de5bcc29654"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Model     train      test       avg BestModel  \\\n",
              "ANN        ArtificialNeuralNetwork  0.947253  0.973684  0.960468  not good   \n",
              "DNN              DeepNeuralNetwork  0.951648  0.921053  0.936350  not good   \n",
              "KNN    KNearestNeighborsClassifier  0.980220  0.991228  0.985724  not good   \n",
              "LR              LogisticRegression  0.980220  0.973684  0.976952  not good   \n",
              "RF          RandomForestClassifier  0.975824  0.991228  0.983526  not good   \n",
              "SVM        SupportVectorClassifier  0.951648  0.956140  0.953894  not good   \n",
              "XGB                        XGBoost  0.989011  0.982456  0.985734      best   \n",
              "H_OD           H2OXGBoostEstimator  0.973626  0.964912  0.969269  not good   \n",
              "H_SOD          H2OXGBoostEstimator  0.936264  0.912281  0.924272  not good   \n",
              "\n",
              "       Precision    Recall  F1_Score       time  \n",
              "ANN     0.973986  0.968760  0.971277   1.948085  \n",
              "DNN     0.927350  0.900936  0.911757  42.189754  \n",
              "KNN     0.993243  0.987805  0.990426   0.000640  \n",
              "LR      0.980263  0.963415  0.970946   0.005957  \n",
              "RF      0.993243  0.987805  0.990426   0.601784  \n",
              "SVM     0.967949  0.939024  0.950976   0.038267  \n",
              "XGB     0.986667  0.975610  0.980743   0.048157  \n",
              "H_OD    0.958074  0.967257  0.962302  65.580579  \n",
              "H_SOD   0.914005  0.894086  0.902564  20.054657  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f11a3d8-df8b-4f7a-b198-385577a9fb01\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>avg</th>\n",
              "      <th>BestModel</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_Score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>ArtificialNeuralNetwork</td>\n",
              "      <td>0.947253</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.960468</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.973986</td>\n",
              "      <td>0.968760</td>\n",
              "      <td>0.971277</td>\n",
              "      <td>1.948085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>DeepNeuralNetwork</td>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.936350</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.927350</td>\n",
              "      <td>0.900936</td>\n",
              "      <td>0.911757</td>\n",
              "      <td>42.189754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>KNearestNeighborsClassifier</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.985724</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "      <td>0.000640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.976952</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.963415</td>\n",
              "      <td>0.970946</td>\n",
              "      <td>0.005957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.975824</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.983526</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "      <td>0.601784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>SupportVectorClassifier</td>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.953894</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.967949</td>\n",
              "      <td>0.939024</td>\n",
              "      <td>0.950976</td>\n",
              "      <td>0.038267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.989011</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.985734</td>\n",
              "      <td>best</td>\n",
              "      <td>0.986667</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.980743</td>\n",
              "      <td>0.048157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_OD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.973626</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.969269</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.958074</td>\n",
              "      <td>0.967257</td>\n",
              "      <td>0.962302</td>\n",
              "      <td>65.580579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_SOD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.936264</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.924272</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.914005</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.902564</td>\n",
              "      <td>20.054657</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f11a3d8-df8b-4f7a-b198-385577a9fb01')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f11a3d8-df8b-4f7a-b198-385577a9fb01 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f11a3d8-df8b-4f7a-b198-385577a9fb01');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P-K86caJxhvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}