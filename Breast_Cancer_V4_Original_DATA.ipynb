{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashiqurrahmankhan21st/BreastCancer/blob/main/Breast_Cancer_V4_Original_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EUMYU_d6_J-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "#import keras_metrics\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#original data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/data.csv\")\n",
        "del df['id']\n",
        "df"
      ],
      "metadata": {
        "id": "Ntdqq4T_aGXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "27c46037-ad62-4dc0-cf82-337f861c60d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0           M        17.99         10.38          122.80     1001.0   \n",
              "1           M        20.57         17.77          132.90     1326.0   \n",
              "2           M        19.69         21.25          130.00     1203.0   \n",
              "3           M        11.42         20.38           77.58      386.1   \n",
              "4           M        20.29         14.34          135.10     1297.0   \n",
              "..        ...          ...           ...             ...        ...   \n",
              "564         M        21.56         22.39          142.00     1479.0   \n",
              "565         M        20.13         28.25          131.20     1261.0   \n",
              "566         M        16.60         28.08          108.30      858.1   \n",
              "567         M        20.60         29.33          140.10     1265.0   \n",
              "568         B         7.76         24.54           47.92      181.0   \n",
              "\n",
              "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0            0.11840           0.27760         0.30010              0.14710   \n",
              "1            0.08474           0.07864         0.08690              0.07017   \n",
              "2            0.10960           0.15990         0.19740              0.12790   \n",
              "3            0.14250           0.28390         0.24140              0.10520   \n",
              "4            0.10030           0.13280         0.19800              0.10430   \n",
              "..               ...               ...             ...                  ...   \n",
              "564          0.11100           0.11590         0.24390              0.13890   \n",
              "565          0.09780           0.10340         0.14400              0.09791   \n",
              "566          0.08455           0.10230         0.09251              0.05302   \n",
              "567          0.11780           0.27700         0.35140              0.15200   \n",
              "568          0.05263           0.04362         0.00000              0.00000   \n",
              "\n",
              "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
              "0           0.2419  ...        25.380          17.33           184.60   \n",
              "1           0.1812  ...        24.990          23.41           158.80   \n",
              "2           0.2069  ...        23.570          25.53           152.50   \n",
              "3           0.2597  ...        14.910          26.50            98.87   \n",
              "4           0.1809  ...        22.540          16.67           152.20   \n",
              "..             ...  ...           ...            ...              ...   \n",
              "564         0.1726  ...        25.450          26.40           166.10   \n",
              "565         0.1752  ...        23.690          38.25           155.00   \n",
              "566         0.1590  ...        18.980          34.12           126.70   \n",
              "567         0.2397  ...        25.740          39.42           184.60   \n",
              "568         0.1587  ...         9.456          30.37            59.16   \n",
              "\n",
              "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "0        2019.0           0.16220            0.66560           0.7119   \n",
              "1        1956.0           0.12380            0.18660           0.2416   \n",
              "2        1709.0           0.14440            0.42450           0.4504   \n",
              "3         567.7           0.20980            0.86630           0.6869   \n",
              "4        1575.0           0.13740            0.20500           0.4000   \n",
              "..          ...               ...                ...              ...   \n",
              "564      2027.0           0.14100            0.21130           0.4107   \n",
              "565      1731.0           0.11660            0.19220           0.3215   \n",
              "566      1124.0           0.11390            0.30940           0.3403   \n",
              "567      1821.0           0.16500            0.86810           0.9387   \n",
              "568       268.6           0.08996            0.06444           0.0000   \n",
              "\n",
              "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
              "0                  0.2654          0.4601                  0.11890  \n",
              "1                  0.1860          0.2750                  0.08902  \n",
              "2                  0.2430          0.3613                  0.08758  \n",
              "3                  0.2575          0.6638                  0.17300  \n",
              "4                  0.1625          0.2364                  0.07678  \n",
              "..                    ...             ...                      ...  \n",
              "564                0.2216          0.2060                  0.07115  \n",
              "565                0.1628          0.2572                  0.06637  \n",
              "566                0.1418          0.2218                  0.07820  \n",
              "567                0.2650          0.4087                  0.12400  \n",
              "568                0.0000          0.2871                  0.07039  \n",
              "\n",
              "[569 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-343bb675-41c6-4559-bebd-ca381a3c8e3f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>...</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>...</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>...</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>...</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>...</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>...</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>...</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>...</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>...</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>...</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-343bb675-41c6-4559-bebd-ca381a3c8e3f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-343bb675-41c6-4559-bebd-ca381a3c8e3f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-343bb675-41c6-4559-bebd-ca381a3c8e3f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()"
      ],
      "metadata": {
        "id": "4cimS259ti6F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diagnosis'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv9sG-P8M1Fe",
        "outputId": "9bbe79ee-218c-4de8-8be4-77c8cbe412e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    357\n",
              "M    212\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()\n",
        "print(\"Original data : \",df.shape)\n",
        "print(\"tarin         : \",X_train.shape)\n",
        "print(\"test          : \",X_test.shape[0])\n",
        "#print(\"validation    : \",X_val.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpyVwv13CTut",
        "outputId": "2d497803-82b6-40a7-be98-bc4856773ddb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data :  (569, 31)\n",
            "tarin         :  (455, 30)\n",
            "test          :  114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "st = time.time()\n",
        "svm = SVC(C=0.1, gamma='auto', kernel = 'rbf',probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "send = time.time() - st\n",
        "STr = svm.score(X_train, y_train)\n",
        "STe = svm.score(X_test, y_test)\n",
        "y_pred_svm = svm.predict(X_test)"
      ],
      "metadata": {
        "id": "wnRYFOkyEEZ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANN\n",
        "st = time.time()\n",
        "tf.random.set_seed(123)\n",
        "ANNmodel = Sequential()\n",
        "ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "ANNmodel.add(Dense(15, activation='relu'))\n",
        "ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "aend = time.time() - st\n",
        "ATr = ANNmodel.evaluate(X_train,y_train,verbose=0)[1]\n",
        "ATe = ANNmodel.evaluate(X_test,y_test,verbose=0)[1]\n",
        "y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZX7DbN-OIyu",
        "outputId": "999aab11-ab4c-444e-fcd6-e9d5f8cbe2f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 90ms/step - loss: 0.6081 - accuracy: 0.6797 - val_loss: 0.6000 - val_accuracy: 0.7174\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5201 - accuracy: 0.7922 - val_loss: 0.5405 - val_accuracy: 0.7826\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4509 - accuracy: 0.8753 - val_loss: 0.4885 - val_accuracy: 0.8043\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.3948 - accuracy: 0.8973 - val_loss: 0.4449 - val_accuracy: 0.8478\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.3478 - accuracy: 0.9120 - val_loss: 0.4080 - val_accuracy: 0.8696\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3104 - accuracy: 0.9193 - val_loss: 0.3767 - val_accuracy: 0.8696\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.2785 - accuracy: 0.9291 - val_loss: 0.3510 - val_accuracy: 0.8913\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.2528 - accuracy: 0.9364 - val_loss: 0.3294 - val_accuracy: 0.8913\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2310 - accuracy: 0.9438 - val_loss: 0.3108 - val_accuracy: 0.8696\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2124 - accuracy: 0.9462 - val_loss: 0.2945 - val_accuracy: 0.8696\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1969 - accuracy: 0.9462 - val_loss: 0.2800 - val_accuracy: 0.8696\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.1828 - accuracy: 0.9462 - val_loss: 0.2671 - val_accuracy: 0.8696\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.1711 - accuracy: 0.9462 - val_loss: 0.2556 - val_accuracy: 0.8696\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1605 - accuracy: 0.9535 - val_loss: 0.2459 - val_accuracy: 0.8696\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1511 - accuracy: 0.9535 - val_loss: 0.2370 - val_accuracy: 0.8696\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1426 - accuracy: 0.9584 - val_loss: 0.2292 - val_accuracy: 0.8696\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1350 - accuracy: 0.9609 - val_loss: 0.2227 - val_accuracy: 0.8696\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1282 - accuracy: 0.9633 - val_loss: 0.2171 - val_accuracy: 0.8696\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.1219 - accuracy: 0.9682 - val_loss: 0.2122 - val_accuracy: 0.8696\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1165 - accuracy: 0.9682 - val_loss: 0.2077 - val_accuracy: 0.8696\n",
            "4/4 [==============================] - 0s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ATr,ATe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8_62aKT4GzA",
        "outputId": "4bbf4bda-c023-437e-e7a8-612a0db9404d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9604395627975464, 0.9736841917037964)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "st = time.time()\n",
        "xgb = XGBClassifier(objective='binary:logistic',max_depth= 6,alpha= 10,learning_rate= 0.03,n_estimators=250)\n",
        "xgb.fit(X_train, y_train)\n",
        "xend = time.time() - st\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "XTr = accuracy_score(y_train, xgb.predict(X_train))\n",
        "XTe = accuracy_score(y_test, xgb.predict(X_test))\n",
        "XTr,XTe"
      ],
      "metadata": {
        "id": "PCP82YZ9RjgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adab5888-d3e2-4bed-eae4-33631bd5622f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9934065934065934, 0.9824561403508771)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "# Define the range of n_neighbors values to test\n",
        "n_neighbors_values = [1,3, 5, 7, 9, 11]\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_n_neighbors = None\n",
        "\n",
        "for n_neighbors in n_neighbors_values:\n",
        "    print(\"Number of Neighbors:\", n_neighbors)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, knn.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "    print('KNN model train accuracy score: {0:0.4f}'.format(train_accuracy))\n",
        "    print('KNN model test accuracy score: {0:0.4f}'.format(test_accuracy))\n",
        "    print()\n",
        "\n",
        "    # Check if the current test accuracy is better than the previous best\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_n_neighbors = n_neighbors\n",
        "print(\"best neighbours: \", best_n_neighbors)\n",
        "\n",
        "st = time.time()\n",
        "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "knn.fit(X_train, y_train)\n",
        "kend = time.time() - st\n",
        "KTr = accuracy_score(y_train, knn.predict(X_train))\n",
        "KTe = accuracy_score(y_test, knn.predict(X_test))\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "KTr,KTe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y_6b33A1imy",
        "outputId": "e2241047-42d3-400e-da0c-096f9d3c2dbe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Neighbors: 1\n",
            "KNN model train accuracy score: 1.0000\n",
            "KNN model test accuracy score: 0.9737\n",
            "\n",
            "Number of Neighbors: 3\n",
            "KNN model train accuracy score: 0.9802\n",
            "KNN model test accuracy score: 0.9912\n",
            "\n",
            "Number of Neighbors: 5\n",
            "KNN model train accuracy score: 0.9692\n",
            "KNN model test accuracy score: 0.9825\n",
            "\n",
            "Number of Neighbors: 7\n",
            "KNN model train accuracy score: 0.9692\n",
            "KNN model test accuracy score: 0.9737\n",
            "\n",
            "Number of Neighbors: 9\n",
            "KNN model train accuracy score: 0.9692\n",
            "KNN model test accuracy score: 0.9825\n",
            "\n",
            "Number of Neighbors: 11\n",
            "KNN model train accuracy score: 0.9714\n",
            "KNN model test accuracy score: 0.9825\n",
            "\n",
            "best neighbours:  3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9802197802197802, 0.9912280701754386)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RF\n",
        "st = time.time()\n",
        "rf = RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123)\n",
        "rf.fit(X_train, y_train)\n",
        "rend = time.time() - st\n",
        "RTr = accuracy_score(y_train, rf.predict(X_train))\n",
        "RTe = accuracy_score(y_test, rf.predict(X_test))\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "RTr,RTe"
      ],
      "metadata": {
        "id": "k_kyk_E92bSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5486a149-1e8c-42c7-b235-5cc06ffebb9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9758241758241758, 0.9912280701754386)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LR\n",
        "st = time.time()\n",
        "lr = LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123)\n",
        "lr.fit(X_train, y_train)\n",
        "lend = time.time() - st\n",
        "LTr = accuracy_score(y_train, lr.predict(X_train))\n",
        "LTe = accuracy_score(y_test, lr.predict(X_test))\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "LTr,LTe"
      ],
      "metadata": {
        "id": "OWkZmwL23Fm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64cb1e68-b812-4ce8-ac0c-320c5e1ff0a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9802197802197802, 0.9736842105263158)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E9_l6r5f0w5n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CVal(ML):\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/data.csv\")\n",
        "  del df['id']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "\n",
        "    ML.fit(X_train, y_train)\n",
        "    y_pred_ML = ML.predict(X_test)\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ML)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]+1\n",
        "\n",
        "  y_pred_final = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_final.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "XqaJZ1Mb0xAe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CValANN():\n",
        "\n",
        "  df = pd.read_csv(\"https://raw.githubusercontent.com/ashiqurrahmankhan21st/BreastCancer/main/data.csv\")\n",
        "  del df['id']\n",
        "\n",
        "  s = 0\n",
        "  e = round(df.shape[0]*.2)\n",
        "\n",
        "  y_pred = []\n",
        "  y_original = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    test_set  = df.iloc[s:e,:]\n",
        "    train_set = df.drop(test_set.index)\n",
        "\n",
        "    X_train = StandardScaler().fit_transform(train_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_train = encoder.fit_transform(train_set['diagnosis']).copy()\n",
        "    X_test = StandardScaler().fit_transform(test_set.drop(columns=['diagnosis'])).copy()\n",
        "    y_test = encoder.fit_transform(test_set['diagnosis']).copy()\n",
        "\n",
        "    #svm = SVC(C=0.1, gamma='auto', kernel = 'rbf')\n",
        "    tf.random.set_seed(123)\n",
        "    ANNmodel = Sequential()\n",
        "    ANNmodel.add(Dense(30, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    ANNmodel.add(Dense(15, activation='relu'))\n",
        "    ANNmodel.add(Dense(1, activation='sigmoid'))\n",
        "    ANNmodel.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy']) #tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
        "    ANNmodel.fit(X_train,y_train, batch_size = 128, epochs = 20, validation_split = 0.1)\n",
        "    y_pred_ANN = (ANNmodel.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "    y_pred.append(y_pred_ANN)\n",
        "    y_original.append(y_test)\n",
        "\n",
        "    s = e\n",
        "    e = e + round(df.shape[0]*.2)\n",
        "    if e-s < round(df.shape[0]*.2):\n",
        "      e = df.shape[0]\n",
        "\n",
        "  y_pred_fina = []\n",
        "  y_original_final = []\n",
        "\n",
        "  try:\n",
        "    for i in range(5):\n",
        "      for j in range(round(df.shape[0]*.2)):\n",
        "        y_pred_fina.append(y_pred[i][j])\n",
        "        y_original_final.append(y_original[i][j])\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  y_pred_final = []\n",
        "\n",
        "  for i in range(len(y_pred_fina)):\n",
        "    y_pred_final.append(y_pred_fina[i][0])\n",
        "\n",
        "  return y_pred_final"
      ],
      "metadata": {
        "id": "jRa_a7EY0y6B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = pd.DataFrame({\n",
        "    \"SVM\": CVal(SVC(C=0.1, gamma='auto', kernel = 'rbf'))\n",
        "})\n",
        "newdata[\"KNN\"] = CVal(KNeighborsClassifier(n_neighbors=3))\n",
        "newdata[\"RF\"]  = CVal(RandomForestClassifier(n_estimators= 500,max_features = 'sqrt', max_samples = 100, random_state=123))\n",
        "newdata['LR']  = CVal(LogisticRegression(C= 0.1 , penalty='l1', solver='liblinear', max_iter = 1000, random_state=123))\n",
        "newdata[\"ANN\"] = CValANN()\n",
        "newdata[\"XGB\"] = CVal(XGBClassifier(objective='binary:logistic',max_depth= 7,alpha= 10,learning_rate= 1,n_estimators=100))\n",
        "newdata['y_test'] = encoder.fit_transform(df['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CGgdO1V0zA9",
        "outputId": "68eb3886-da0f-4cdc-d5de-2f7ff9aea80e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 78ms/step - loss: 0.5698 - accuracy: 0.7824 - val_loss: 0.5560 - val_accuracy: 0.8696\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.5095 - accuracy: 0.8680 - val_loss: 0.4966 - val_accuracy: 0.9348\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.4599 - accuracy: 0.8949 - val_loss: 0.4446 - val_accuracy: 0.9783\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4159 - accuracy: 0.9193 - val_loss: 0.3991 - val_accuracy: 0.9783\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3772 - accuracy: 0.9315 - val_loss: 0.3588 - val_accuracy: 0.9783\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3421 - accuracy: 0.9340 - val_loss: 0.3234 - val_accuracy: 0.9565\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3115 - accuracy: 0.9340 - val_loss: 0.2921 - val_accuracy: 0.9565\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2840 - accuracy: 0.9389 - val_loss: 0.2652 - val_accuracy: 0.9565\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2596 - accuracy: 0.9462 - val_loss: 0.2415 - val_accuracy: 0.9565\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2380 - accuracy: 0.9462 - val_loss: 0.2206 - val_accuracy: 0.9565\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2184 - accuracy: 0.9511 - val_loss: 0.2024 - val_accuracy: 0.9565\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2014 - accuracy: 0.9535 - val_loss: 0.1869 - val_accuracy: 0.9565\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1862 - accuracy: 0.9584 - val_loss: 0.1736 - val_accuracy: 0.9565\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.1730 - accuracy: 0.9609 - val_loss: 0.1617 - val_accuracy: 0.9565\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1609 - accuracy: 0.9609 - val_loss: 0.1510 - val_accuracy: 0.9783\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1503 - accuracy: 0.9658 - val_loss: 0.1420 - val_accuracy: 0.9783\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1410 - accuracy: 0.9658 - val_loss: 0.1343 - val_accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1326 - accuracy: 0.9658 - val_loss: 0.1267 - val_accuracy: 0.9783\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1251 - accuracy: 0.9658 - val_loss: 0.1203 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.1183 - accuracy: 0.9658 - val_loss: 0.1154 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 77ms/step - loss: 0.8938 - accuracy: 0.3301 - val_loss: 0.9759 - val_accuracy: 0.2391\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.7948 - accuracy: 0.3985 - val_loss: 0.8690 - val_accuracy: 0.3261\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.7121 - accuracy: 0.4670 - val_loss: 0.7807 - val_accuracy: 0.3478\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.6393 - accuracy: 0.5721 - val_loss: 0.7076 - val_accuracy: 0.3913\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.5766 - accuracy: 0.6748 - val_loss: 0.6462 - val_accuracy: 0.4783\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.5203 - accuracy: 0.7555 - val_loss: 0.5947 - val_accuracy: 0.5435\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.4729 - accuracy: 0.8191 - val_loss: 0.5477 - val_accuracy: 0.7174\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4318 - accuracy: 0.8778 - val_loss: 0.5068 - val_accuracy: 0.7609\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.3966 - accuracy: 0.8973 - val_loss: 0.4698 - val_accuracy: 0.8043\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3649 - accuracy: 0.9169 - val_loss: 0.4367 - val_accuracy: 0.8043\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3356 - accuracy: 0.9364 - val_loss: 0.4063 - val_accuracy: 0.8696\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.3093 - accuracy: 0.9511 - val_loss: 0.3776 - val_accuracy: 0.8696\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.2846 - accuracy: 0.9560 - val_loss: 0.3492 - val_accuracy: 0.9348\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.2621 - accuracy: 0.9560 - val_loss: 0.3213 - val_accuracy: 0.9348\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2413 - accuracy: 0.9560 - val_loss: 0.2970 - val_accuracy: 0.9348\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.2228 - accuracy: 0.9560 - val_loss: 0.2746 - val_accuracy: 0.9565\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2065 - accuracy: 0.9584 - val_loss: 0.2542 - val_accuracy: 0.9565\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1921 - accuracy: 0.9609 - val_loss: 0.2353 - val_accuracy: 0.9783\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1796 - accuracy: 0.9609 - val_loss: 0.2192 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1683 - accuracy: 0.9633 - val_loss: 0.2056 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 81ms/step - loss: 0.5355 - accuracy: 0.7262 - val_loss: 0.4555 - val_accuracy: 0.8261\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4501 - accuracy: 0.8142 - val_loss: 0.3828 - val_accuracy: 0.8913\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.3860 - accuracy: 0.8802 - val_loss: 0.3285 - val_accuracy: 0.9130\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3355 - accuracy: 0.9071 - val_loss: 0.2863 - val_accuracy: 0.9565\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.2956 - accuracy: 0.9071 - val_loss: 0.2534 - val_accuracy: 0.9565\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2635 - accuracy: 0.9095 - val_loss: 0.2282 - val_accuracy: 0.9565\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.2376 - accuracy: 0.9169 - val_loss: 0.2071 - val_accuracy: 0.9565\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2159 - accuracy: 0.9267 - val_loss: 0.1909 - val_accuracy: 0.9565\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.1984 - accuracy: 0.9291 - val_loss: 0.1778 - val_accuracy: 0.9565\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1831 - accuracy: 0.9389 - val_loss: 0.1679 - val_accuracy: 0.9565\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1695 - accuracy: 0.9438 - val_loss: 0.1595 - val_accuracy: 0.9565\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1579 - accuracy: 0.9511 - val_loss: 0.1526 - val_accuracy: 0.9565\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1474 - accuracy: 0.9609 - val_loss: 0.1468 - val_accuracy: 0.9565\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1380 - accuracy: 0.9633 - val_loss: 0.1423 - val_accuracy: 0.9565\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1298 - accuracy: 0.9658 - val_loss: 0.1399 - val_accuracy: 0.9565\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1224 - accuracy: 0.9682 - val_loss: 0.1381 - val_accuracy: 0.9565\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.1156 - accuracy: 0.9731 - val_loss: 0.1361 - val_accuracy: 0.9565\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.1097 - accuracy: 0.9731 - val_loss: 0.1344 - val_accuracy: 0.9565\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1041 - accuracy: 0.9756 - val_loss: 0.1329 - val_accuracy: 0.9565\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.0992 - accuracy: 0.9804 - val_loss: 0.1319 - val_accuracy: 0.9565\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 82ms/step - loss: 0.8608 - accuracy: 0.4010 - val_loss: 0.8766 - val_accuracy: 0.3043\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.7540 - accuracy: 0.5623 - val_loss: 0.7514 - val_accuracy: 0.5217\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6663 - accuracy: 0.6724 - val_loss: 0.6512 - val_accuracy: 0.6522\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5957 - accuracy: 0.7677 - val_loss: 0.5730 - val_accuracy: 0.7174\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5392 - accuracy: 0.8068 - val_loss: 0.5115 - val_accuracy: 0.7609\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4922 - accuracy: 0.8386 - val_loss: 0.4601 - val_accuracy: 0.8696\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.4534 - accuracy: 0.8655 - val_loss: 0.4160 - val_accuracy: 0.9130\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.4180 - accuracy: 0.8753 - val_loss: 0.3787 - val_accuracy: 0.9348\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3873 - accuracy: 0.8949 - val_loss: 0.3454 - val_accuracy: 0.9565\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3586 - accuracy: 0.8998 - val_loss: 0.3149 - val_accuracy: 0.9565\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3327 - accuracy: 0.9144 - val_loss: 0.2878 - val_accuracy: 0.9565\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.3089 - accuracy: 0.9144 - val_loss: 0.2638 - val_accuracy: 0.9565\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.2870 - accuracy: 0.9242 - val_loss: 0.2413 - val_accuracy: 0.9565\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2672 - accuracy: 0.9315 - val_loss: 0.2211 - val_accuracy: 0.9783\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.2489 - accuracy: 0.9340 - val_loss: 0.2039 - val_accuracy: 0.9783\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.2326 - accuracy: 0.9438 - val_loss: 0.1893 - val_accuracy: 0.9783\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2182 - accuracy: 0.9462 - val_loss: 0.1758 - val_accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.2048 - accuracy: 0.9462 - val_loss: 0.1641 - val_accuracy: 0.9783\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1925 - accuracy: 0.9487 - val_loss: 0.1537 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1811 - accuracy: 0.9511 - val_loss: 0.1452 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 1s 114ms/step - loss: 0.6919 - accuracy: 0.4561 - val_loss: 0.6822 - val_accuracy: 0.4783\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.5852 - accuracy: 0.6537 - val_loss: 0.5783 - val_accuracy: 0.6739\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.4972 - accuracy: 0.8024 - val_loss: 0.4887 - val_accuracy: 0.8478\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.4243 - accuracy: 0.8878 - val_loss: 0.4139 - val_accuracy: 0.9130\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.3647 - accuracy: 0.9293 - val_loss: 0.3532 - val_accuracy: 0.9783\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.3175 - accuracy: 0.9415 - val_loss: 0.3039 - val_accuracy: 0.9783\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.2792 - accuracy: 0.9439 - val_loss: 0.2647 - val_accuracy: 0.9783\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.2490 - accuracy: 0.9463 - val_loss: 0.2331 - val_accuracy: 0.9783\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.2238 - accuracy: 0.9463 - val_loss: 0.2075 - val_accuracy: 0.9783\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2034 - accuracy: 0.9488 - val_loss: 0.1859 - val_accuracy: 0.9783\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.1865 - accuracy: 0.9512 - val_loss: 0.1680 - val_accuracy: 0.9783\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1725 - accuracy: 0.9512 - val_loss: 0.1536 - val_accuracy: 0.9783\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1610 - accuracy: 0.9512 - val_loss: 0.1416 - val_accuracy: 0.9783\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1510 - accuracy: 0.9537 - val_loss: 0.1322 - val_accuracy: 0.9783\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.1424 - accuracy: 0.9537 - val_loss: 0.1246 - val_accuracy: 0.9783\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.1350 - accuracy: 0.9537 - val_loss: 0.1180 - val_accuracy: 0.9783\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.1286 - accuracy: 0.9537 - val_loss: 0.1131 - val_accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.1227 - accuracy: 0.9537 - val_loss: 0.1089 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.1177 - accuracy: 0.9561 - val_loss: 0.1057 - val_accuracy: 0.9783\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1130 - accuracy: 0.9561 - val_loss: 0.1031 - val_accuracy: 0.9783\n",
            "4/4 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newdata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "05ryfvyLzFVK",
        "outputId": "cd280f57-d0d4-4903-8bd3-e04f88e2af66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SVM  KNN  RF  LR  ANN  XGB  y_test\n",
              "0    1    1   1   1    1    0       1\n",
              "1    1    1   1   1    1    1       1\n",
              "2    1    1   1   1    1    1       1\n",
              "3    1    1   1   1    1    1       1\n",
              "4    1    1   1   1    1    0       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd052f83-da08-438c-9551-ae8a71d9929b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SVM</th>\n",
              "      <th>KNN</th>\n",
              "      <th>RF</th>\n",
              "      <th>LR</th>\n",
              "      <th>ANN</th>\n",
              "      <th>XGB</th>\n",
              "      <th>y_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd052f83-da08-438c-9551-ae8a71d9929b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd052f83-da08-438c-9551-ae8a71d9929b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd052f83-da08-438c-9551-ae8a71d9929b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DNN model\n",
        "DNNX = newdata.drop(columns=['y_test']).copy()\n",
        "DNNY = newdata.y_test.copy()\n",
        "DX_train, DX_test, Dy_train, Dy_test = train_test_split(\n",
        "    DNNX, DNNY, test_size=0.2, random_state=123)\n",
        "\n",
        "st = time.time()\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(30, activation='relu', input_shape=DNNX.shape[1:]),\n",
        "    tf.keras.layers.Dense(25, activation='relu'),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dense(15, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(DX_train, Dy_train, epochs=500, batch_size=64, validation_split=0.2)\n",
        "dend = time.time() - st"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4kf97a3yX15",
        "outputId": "93dbfff4-c8d4-4fdb-c3b2-5eec2d8d8ab1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 2s 52ms/step - loss: 0.6920 - accuracy: 0.9066 - val_loss: 0.6912 - val_accuracy: 0.9121\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.6905 - accuracy: 0.9286 - val_loss: 0.6897 - val_accuracy: 0.9231\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.6887 - accuracy: 0.9313 - val_loss: 0.6876 - val_accuracy: 0.9231\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.6863 - accuracy: 0.9368 - val_loss: 0.6848 - val_accuracy: 0.9121\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.6831 - accuracy: 0.9396 - val_loss: 0.6810 - val_accuracy: 0.9121\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.6789 - accuracy: 0.9341 - val_loss: 0.6754 - val_accuracy: 0.9011\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.6728 - accuracy: 0.9286 - val_loss: 0.6676 - val_accuracy: 0.9011\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.6645 - accuracy: 0.9231 - val_loss: 0.6562 - val_accuracy: 0.8901\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.6520 - accuracy: 0.9231 - val_loss: 0.6401 - val_accuracy: 0.8901\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.6356 - accuracy: 0.9176 - val_loss: 0.6184 - val_accuracy: 0.8791\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.6134 - accuracy: 0.9011 - val_loss: 0.5891 - val_accuracy: 0.8791\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.5858 - accuracy: 0.8956 - val_loss: 0.5556 - val_accuracy: 0.8791\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.5552 - accuracy: 0.8984 - val_loss: 0.5244 - val_accuracy: 0.8791\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.5297 - accuracy: 0.8984 - val_loss: 0.5025 - val_accuracy: 0.8791\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.5134 - accuracy: 0.8984 - val_loss: 0.4924 - val_accuracy: 0.8791\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.5076 - accuracy: 0.8984 - val_loss: 0.4883 - val_accuracy: 0.8901\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.5014 - accuracy: 0.9148 - val_loss: 0.4839 - val_accuracy: 0.8901\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.4964 - accuracy: 0.9258 - val_loss: 0.4794 - val_accuracy: 0.8901\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4911 - accuracy: 0.9258 - val_loss: 0.4750 - val_accuracy: 0.8901\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.4875 - accuracy: 0.9258 - val_loss: 0.4718 - val_accuracy: 0.8901\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4833 - accuracy: 0.9286 - val_loss: 0.4685 - val_accuracy: 0.8901\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.4788 - accuracy: 0.9396 - val_loss: 0.4654 - val_accuracy: 0.8901\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4747 - accuracy: 0.9396 - val_loss: 0.4626 - val_accuracy: 0.8901\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.4721 - accuracy: 0.9396 - val_loss: 0.4617 - val_accuracy: 0.8901\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.4685 - accuracy: 0.9396 - val_loss: 0.4577 - val_accuracy: 0.8901\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4653 - accuracy: 0.9451 - val_loss: 0.4542 - val_accuracy: 0.8901\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4629 - accuracy: 0.9451 - val_loss: 0.4517 - val_accuracy: 0.8901\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4604 - accuracy: 0.9478 - val_loss: 0.4481 - val_accuracy: 0.8901\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4576 - accuracy: 0.9478 - val_loss: 0.4459 - val_accuracy: 0.8901\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4549 - accuracy: 0.9478 - val_loss: 0.4449 - val_accuracy: 0.8901\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4527 - accuracy: 0.9478 - val_loss: 0.4434 - val_accuracy: 0.8901\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4503 - accuracy: 0.9478 - val_loss: 0.4403 - val_accuracy: 0.8901\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.4479 - accuracy: 0.9478 - val_loss: 0.4372 - val_accuracy: 0.8901\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.4455 - accuracy: 0.9478 - val_loss: 0.4347 - val_accuracy: 0.8901\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4435 - accuracy: 0.9478 - val_loss: 0.4346 - val_accuracy: 0.8901\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4405 - accuracy: 0.9478 - val_loss: 0.4325 - val_accuracy: 0.8901\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4382 - accuracy: 0.9478 - val_loss: 0.4295 - val_accuracy: 0.9011\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.4359 - accuracy: 0.9505 - val_loss: 0.4281 - val_accuracy: 0.9011\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4335 - accuracy: 0.9505 - val_loss: 0.4276 - val_accuracy: 0.9011\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.4313 - accuracy: 0.9505 - val_loss: 0.4268 - val_accuracy: 0.9011\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.4295 - accuracy: 0.9505 - val_loss: 0.4234 - val_accuracy: 0.9011\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4268 - accuracy: 0.9505 - val_loss: 0.4233 - val_accuracy: 0.9011\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.4247 - accuracy: 0.9505 - val_loss: 0.4235 - val_accuracy: 0.9011\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4223 - accuracy: 0.9505 - val_loss: 0.4216 - val_accuracy: 0.9011\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4203 - accuracy: 0.9505 - val_loss: 0.4206 - val_accuracy: 0.9011\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.4184 - accuracy: 0.9505 - val_loss: 0.4192 - val_accuracy: 0.9011\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4162 - accuracy: 0.9505 - val_loss: 0.4186 - val_accuracy: 0.9011\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.4138 - accuracy: 0.9505 - val_loss: 0.4197 - val_accuracy: 0.9011\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4122 - accuracy: 0.9505 - val_loss: 0.4198 - val_accuracy: 0.9011\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4103 - accuracy: 0.9505 - val_loss: 0.4189 - val_accuracy: 0.9011\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4082 - accuracy: 0.9505 - val_loss: 0.4174 - val_accuracy: 0.9011\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.4065 - accuracy: 0.9505 - val_loss: 0.4164 - val_accuracy: 0.9011\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.4047 - accuracy: 0.9505 - val_loss: 0.4178 - val_accuracy: 0.9011\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.4026 - accuracy: 0.9505 - val_loss: 0.4184 - val_accuracy: 0.9011\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.4012 - accuracy: 0.9505 - val_loss: 0.4186 - val_accuracy: 0.9011\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.3991 - accuracy: 0.9505 - val_loss: 0.4203 - val_accuracy: 0.9011\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.3973 - accuracy: 0.9560 - val_loss: 0.4183 - val_accuracy: 0.9011\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.3959 - accuracy: 0.9560 - val_loss: 0.4190 - val_accuracy: 0.9011\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3925 - accuracy: 0.9560 - val_loss: 0.4160 - val_accuracy: 0.9011\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.3912 - accuracy: 0.9588 - val_loss: 0.4139 - val_accuracy: 0.9121\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.3868 - accuracy: 0.9615 - val_loss: 0.4164 - val_accuracy: 0.9011\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.3846 - accuracy: 0.9588 - val_loss: 0.4189 - val_accuracy: 0.9011\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.3811 - accuracy: 0.9588 - val_loss: 0.4115 - val_accuracy: 0.9121\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3775 - accuracy: 0.9588 - val_loss: 0.4096 - val_accuracy: 0.9121\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.3742 - accuracy: 0.9588 - val_loss: 0.4105 - val_accuracy: 0.9011\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3687 - accuracy: 0.9588 - val_loss: 0.4056 - val_accuracy: 0.9121\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.3610 - accuracy: 0.9588 - val_loss: 0.3974 - val_accuracy: 0.9121\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.3537 - accuracy: 0.9588 - val_loss: 0.3909 - val_accuracy: 0.9121\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.3402 - accuracy: 0.9588 - val_loss: 0.3833 - val_accuracy: 0.9121\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.3236 - accuracy: 0.9588 - val_loss: 0.3693 - val_accuracy: 0.9121\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.2978 - accuracy: 0.9588 - val_loss: 0.3416 - val_accuracy: 0.9121\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.2630 - accuracy: 0.9588 - val_loss: 0.3070 - val_accuracy: 0.9121\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.2220 - accuracy: 0.9588 - val_loss: 0.2813 - val_accuracy: 0.9121\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1879 - accuracy: 0.9588 - val_loss: 0.2828 - val_accuracy: 0.9121\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1655 - accuracy: 0.9588 - val_loss: 0.2970 - val_accuracy: 0.9011\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1648 - accuracy: 0.9588 - val_loss: 0.3089 - val_accuracy: 0.9011\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1652 - accuracy: 0.9588 - val_loss: 0.3082 - val_accuracy: 0.9011\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1623 - accuracy: 0.9588 - val_loss: 0.2981 - val_accuracy: 0.9121\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1606 - accuracy: 0.9588 - val_loss: 0.2994 - val_accuracy: 0.9011\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1599 - accuracy: 0.9588 - val_loss: 0.2906 - val_accuracy: 0.9011\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1593 - accuracy: 0.9560 - val_loss: 0.2879 - val_accuracy: 0.9121\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1576 - accuracy: 0.9560 - val_loss: 0.2924 - val_accuracy: 0.9011\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1571 - accuracy: 0.9588 - val_loss: 0.2941 - val_accuracy: 0.9011\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1560 - accuracy: 0.9588 - val_loss: 0.2944 - val_accuracy: 0.9011\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1589 - accuracy: 0.9560 - val_loss: 0.2943 - val_accuracy: 0.9121\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1556 - accuracy: 0.9588 - val_loss: 0.3019 - val_accuracy: 0.9011\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1559 - accuracy: 0.9588 - val_loss: 0.3034 - val_accuracy: 0.9011\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1562 - accuracy: 0.9588 - val_loss: 0.3006 - val_accuracy: 0.9011\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1564 - accuracy: 0.9588 - val_loss: 0.3034 - val_accuracy: 0.9011\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1566 - accuracy: 0.9560 - val_loss: 0.2987 - val_accuracy: 0.9011\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1551 - accuracy: 0.9588 - val_loss: 0.3079 - val_accuracy: 0.9011\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1546 - accuracy: 0.9588 - val_loss: 0.3098 - val_accuracy: 0.9011\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1538 - accuracy: 0.9588 - val_loss: 0.3081 - val_accuracy: 0.9011\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1556 - accuracy: 0.9588 - val_loss: 0.3061 - val_accuracy: 0.9011\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1549 - accuracy: 0.9588 - val_loss: 0.3087 - val_accuracy: 0.9011\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1576 - accuracy: 0.9588 - val_loss: 0.3166 - val_accuracy: 0.9011\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1556 - accuracy: 0.9588 - val_loss: 0.3064 - val_accuracy: 0.9011\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1539 - accuracy: 0.9588 - val_loss: 0.3058 - val_accuracy: 0.9011\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1553 - accuracy: 0.9560 - val_loss: 0.3047 - val_accuracy: 0.9121\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1559 - accuracy: 0.9588 - val_loss: 0.3126 - val_accuracy: 0.9011\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1538 - accuracy: 0.9588 - val_loss: 0.3112 - val_accuracy: 0.9011\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1533 - accuracy: 0.9588 - val_loss: 0.3077 - val_accuracy: 0.9011\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1535 - accuracy: 0.9588 - val_loss: 0.3102 - val_accuracy: 0.9011\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1535 - accuracy: 0.9588 - val_loss: 0.3114 - val_accuracy: 0.9011\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1526 - accuracy: 0.9588 - val_loss: 0.3123 - val_accuracy: 0.9011\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1524 - accuracy: 0.9588 - val_loss: 0.3147 - val_accuracy: 0.9011\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1526 - accuracy: 0.9588 - val_loss: 0.3171 - val_accuracy: 0.9011\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1521 - accuracy: 0.9588 - val_loss: 0.3154 - val_accuracy: 0.9121\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1532 - accuracy: 0.9560 - val_loss: 0.3171 - val_accuracy: 0.9011\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1522 - accuracy: 0.9588 - val_loss: 0.3172 - val_accuracy: 0.9011\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1520 - accuracy: 0.9588 - val_loss: 0.3185 - val_accuracy: 0.9011\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1518 - accuracy: 0.9588 - val_loss: 0.3208 - val_accuracy: 0.9011\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1521 - accuracy: 0.9588 - val_loss: 0.3196 - val_accuracy: 0.9011\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1525 - accuracy: 0.9588 - val_loss: 0.3164 - val_accuracy: 0.9121\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1544 - accuracy: 0.9588 - val_loss: 0.3123 - val_accuracy: 0.9121\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1531 - accuracy: 0.9588 - val_loss: 0.3159 - val_accuracy: 0.9121\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1520 - accuracy: 0.9588 - val_loss: 0.3178 - val_accuracy: 0.9121\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1516 - accuracy: 0.9588 - val_loss: 0.3236 - val_accuracy: 0.9011\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1522 - accuracy: 0.9588 - val_loss: 0.3242 - val_accuracy: 0.9011\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1519 - accuracy: 0.9588 - val_loss: 0.3226 - val_accuracy: 0.9011\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1522 - accuracy: 0.9588 - val_loss: 0.3268 - val_accuracy: 0.9011\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1513 - accuracy: 0.9588 - val_loss: 0.3239 - val_accuracy: 0.9011\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1515 - accuracy: 0.9588 - val_loss: 0.3198 - val_accuracy: 0.9011\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1513 - accuracy: 0.9588 - val_loss: 0.3217 - val_accuracy: 0.9011\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1517 - accuracy: 0.9588 - val_loss: 0.3224 - val_accuracy: 0.9011\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1508 - accuracy: 0.9588 - val_loss: 0.3231 - val_accuracy: 0.9011\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1507 - accuracy: 0.9588 - val_loss: 0.3248 - val_accuracy: 0.9011\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1510 - accuracy: 0.9588 - val_loss: 0.3284 - val_accuracy: 0.9011\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1518 - accuracy: 0.9533 - val_loss: 0.3308 - val_accuracy: 0.9011\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1511 - accuracy: 0.9533 - val_loss: 0.3274 - val_accuracy: 0.9011\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1516 - accuracy: 0.9533 - val_loss: 0.3265 - val_accuracy: 0.9011\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1534 - accuracy: 0.9560 - val_loss: 0.3292 - val_accuracy: 0.9011\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1515 - accuracy: 0.9560 - val_loss: 0.3270 - val_accuracy: 0.9121\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1522 - accuracy: 0.9560 - val_loss: 0.3248 - val_accuracy: 0.9121\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1521 - accuracy: 0.9533 - val_loss: 0.3286 - val_accuracy: 0.9121\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1506 - accuracy: 0.9560 - val_loss: 0.3304 - val_accuracy: 0.9011\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1508 - accuracy: 0.9588 - val_loss: 0.3313 - val_accuracy: 0.9011\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1509 - accuracy: 0.9588 - val_loss: 0.3289 - val_accuracy: 0.9011\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1516 - accuracy: 0.9588 - val_loss: 0.3270 - val_accuracy: 0.9121\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1516 - accuracy: 0.9560 - val_loss: 0.3337 - val_accuracy: 0.9011\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1507 - accuracy: 0.9588 - val_loss: 0.3340 - val_accuracy: 0.9011\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1502 - accuracy: 0.9588 - val_loss: 0.3344 - val_accuracy: 0.9011\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1500 - accuracy: 0.9588 - val_loss: 0.3347 - val_accuracy: 0.9011\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1503 - accuracy: 0.9588 - val_loss: 0.3340 - val_accuracy: 0.9011\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1500 - accuracy: 0.9588 - val_loss: 0.3329 - val_accuracy: 0.9011\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1497 - accuracy: 0.9588 - val_loss: 0.3294 - val_accuracy: 0.9121\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1517 - accuracy: 0.9588 - val_loss: 0.3274 - val_accuracy: 0.9121\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1501 - accuracy: 0.9588 - val_loss: 0.3350 - val_accuracy: 0.9011\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1498 - accuracy: 0.9588 - val_loss: 0.3431 - val_accuracy: 0.9011\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1512 - accuracy: 0.9588 - val_loss: 0.3433 - val_accuracy: 0.9011\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1506 - accuracy: 0.9588 - val_loss: 0.3346 - val_accuracy: 0.9011\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1502 - accuracy: 0.9588 - val_loss: 0.3338 - val_accuracy: 0.9011\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1495 - accuracy: 0.9588 - val_loss: 0.3363 - val_accuracy: 0.9011\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1495 - accuracy: 0.9588 - val_loss: 0.3388 - val_accuracy: 0.9011\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1493 - accuracy: 0.9588 - val_loss: 0.3406 - val_accuracy: 0.9011\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1492 - accuracy: 0.9588 - val_loss: 0.3413 - val_accuracy: 0.9011\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1497 - accuracy: 0.9560 - val_loss: 0.3398 - val_accuracy: 0.9011\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1495 - accuracy: 0.9588 - val_loss: 0.3460 - val_accuracy: 0.9011\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1497 - accuracy: 0.9588 - val_loss: 0.3431 - val_accuracy: 0.9011\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1492 - accuracy: 0.9588 - val_loss: 0.3446 - val_accuracy: 0.9011\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1502 - accuracy: 0.9588 - val_loss: 0.3460 - val_accuracy: 0.9011\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1522 - accuracy: 0.9588 - val_loss: 0.3373 - val_accuracy: 0.9121\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1501 - accuracy: 0.9588 - val_loss: 0.3409 - val_accuracy: 0.9011\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1501 - accuracy: 0.9588 - val_loss: 0.3496 - val_accuracy: 0.9011\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1491 - accuracy: 0.9588 - val_loss: 0.3455 - val_accuracy: 0.9011\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3429 - val_accuracy: 0.9011\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1485 - accuracy: 0.9560 - val_loss: 0.3463 - val_accuracy: 0.9011\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1497 - accuracy: 0.9588 - val_loss: 0.3498 - val_accuracy: 0.9011\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1488 - accuracy: 0.9588 - val_loss: 0.3481 - val_accuracy: 0.9011\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1482 - accuracy: 0.9588 - val_loss: 0.3490 - val_accuracy: 0.9011\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1483 - accuracy: 0.9588 - val_loss: 0.3483 - val_accuracy: 0.9121\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1501 - accuracy: 0.9588 - val_loss: 0.3445 - val_accuracy: 0.9121\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1477 - accuracy: 0.9588 - val_loss: 0.3529 - val_accuracy: 0.9011\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1512 - accuracy: 0.9533 - val_loss: 0.3591 - val_accuracy: 0.9011\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1497 - accuracy: 0.9533 - val_loss: 0.3547 - val_accuracy: 0.9011\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1487 - accuracy: 0.9588 - val_loss: 0.3479 - val_accuracy: 0.9011\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1492 - accuracy: 0.9588 - val_loss: 0.3469 - val_accuracy: 0.9121\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1479 - accuracy: 0.9588 - val_loss: 0.3518 - val_accuracy: 0.9011\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1478 - accuracy: 0.9588 - val_loss: 0.3542 - val_accuracy: 0.9011\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3559 - val_accuracy: 0.9011\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1480 - accuracy: 0.9588 - val_loss: 0.3539 - val_accuracy: 0.9011\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3555 - val_accuracy: 0.9011\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1477 - accuracy: 0.9588 - val_loss: 0.3529 - val_accuracy: 0.9121\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1491 - accuracy: 0.9588 - val_loss: 0.3488 - val_accuracy: 0.9121\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1503 - accuracy: 0.9560 - val_loss: 0.3600 - val_accuracy: 0.9011\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1479 - accuracy: 0.9588 - val_loss: 0.3565 - val_accuracy: 0.9011\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1478 - accuracy: 0.9588 - val_loss: 0.3540 - val_accuracy: 0.9011\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1494 - accuracy: 0.9588 - val_loss: 0.3583 - val_accuracy: 0.9011\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1467 - accuracy: 0.9588 - val_loss: 0.3540 - val_accuracy: 0.9011\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1484 - accuracy: 0.9560 - val_loss: 0.3521 - val_accuracy: 0.9121\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1472 - accuracy: 0.9615 - val_loss: 0.3570 - val_accuracy: 0.9011\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1476 - accuracy: 0.9588 - val_loss: 0.3620 - val_accuracy: 0.9011\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1471 - accuracy: 0.9588 - val_loss: 0.3631 - val_accuracy: 0.9011\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1472 - accuracy: 0.9588 - val_loss: 0.3621 - val_accuracy: 0.9011\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1477 - accuracy: 0.9588 - val_loss: 0.3654 - val_accuracy: 0.9011\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1491 - accuracy: 0.9560 - val_loss: 0.3620 - val_accuracy: 0.9011\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1484 - accuracy: 0.9588 - val_loss: 0.3700 - val_accuracy: 0.9011\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1470 - accuracy: 0.9588 - val_loss: 0.3669 - val_accuracy: 0.9011\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1469 - accuracy: 0.9588 - val_loss: 0.3612 - val_accuracy: 0.9011\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1474 - accuracy: 0.9588 - val_loss: 0.3641 - val_accuracy: 0.9011\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1469 - accuracy: 0.9588 - val_loss: 0.3665 - val_accuracy: 0.9011\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1475 - accuracy: 0.9588 - val_loss: 0.3649 - val_accuracy: 0.9011\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1472 - accuracy: 0.9588 - val_loss: 0.3621 - val_accuracy: 0.9121\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1478 - accuracy: 0.9588 - val_loss: 0.3642 - val_accuracy: 0.9121\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1470 - accuracy: 0.9560 - val_loss: 0.3680 - val_accuracy: 0.9011\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1468 - accuracy: 0.9588 - val_loss: 0.3685 - val_accuracy: 0.9011\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1472 - accuracy: 0.9588 - val_loss: 0.3695 - val_accuracy: 0.9011\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1469 - accuracy: 0.9588 - val_loss: 0.3695 - val_accuracy: 0.9011\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1465 - accuracy: 0.9588 - val_loss: 0.3669 - val_accuracy: 0.9011\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1465 - accuracy: 0.9588 - val_loss: 0.3659 - val_accuracy: 0.9011\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1470 - accuracy: 0.9588 - val_loss: 0.3666 - val_accuracy: 0.9011\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1469 - accuracy: 0.9588 - val_loss: 0.3642 - val_accuracy: 0.9121\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1465 - accuracy: 0.9588 - val_loss: 0.3693 - val_accuracy: 0.9011\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1465 - accuracy: 0.9588 - val_loss: 0.3705 - val_accuracy: 0.9011\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1467 - accuracy: 0.9588 - val_loss: 0.3727 - val_accuracy: 0.9011\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1467 - accuracy: 0.9588 - val_loss: 0.3736 - val_accuracy: 0.9011\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1460 - accuracy: 0.9588 - val_loss: 0.3708 - val_accuracy: 0.9011\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1470 - accuracy: 0.9588 - val_loss: 0.3643 - val_accuracy: 0.9121\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1463 - accuracy: 0.9588 - val_loss: 0.3703 - val_accuracy: 0.9121\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1463 - accuracy: 0.9588 - val_loss: 0.3756 - val_accuracy: 0.9011\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1463 - accuracy: 0.9588 - val_loss: 0.3730 - val_accuracy: 0.9011\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1464 - accuracy: 0.9560 - val_loss: 0.3731 - val_accuracy: 0.9011\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1458 - accuracy: 0.9588 - val_loss: 0.3774 - val_accuracy: 0.9011\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1463 - accuracy: 0.9588 - val_loss: 0.3746 - val_accuracy: 0.9011\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1470 - accuracy: 0.9560 - val_loss: 0.3675 - val_accuracy: 0.9121\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1463 - accuracy: 0.9560 - val_loss: 0.3706 - val_accuracy: 0.9011\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.1460 - accuracy: 0.9588 - val_loss: 0.3749 - val_accuracy: 0.9011\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1460 - accuracy: 0.9588 - val_loss: 0.3746 - val_accuracy: 0.9011\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1468 - accuracy: 0.9588 - val_loss: 0.3716 - val_accuracy: 0.9121\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1458 - accuracy: 0.9560 - val_loss: 0.3752 - val_accuracy: 0.9011\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1454 - accuracy: 0.9588 - val_loss: 0.3762 - val_accuracy: 0.9011\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1455 - accuracy: 0.9588 - val_loss: 0.3743 - val_accuracy: 0.9121\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1460 - accuracy: 0.9588 - val_loss: 0.3737 - val_accuracy: 0.9011\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1454 - accuracy: 0.9560 - val_loss: 0.3773 - val_accuracy: 0.9011\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1455 - accuracy: 0.9588 - val_loss: 0.3802 - val_accuracy: 0.9011\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.3807 - val_accuracy: 0.9011\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.3772 - val_accuracy: 0.9011\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1455 - accuracy: 0.9588 - val_loss: 0.3775 - val_accuracy: 0.9011\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1450 - accuracy: 0.9588 - val_loss: 0.3788 - val_accuracy: 0.9011\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1452 - accuracy: 0.9588 - val_loss: 0.3818 - val_accuracy: 0.9011\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1463 - accuracy: 0.9588 - val_loss: 0.3793 - val_accuracy: 0.9011\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.3814 - val_accuracy: 0.9011\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1464 - accuracy: 0.9588 - val_loss: 0.3837 - val_accuracy: 0.9011\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.3817 - val_accuracy: 0.9011\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1455 - accuracy: 0.9588 - val_loss: 0.3801 - val_accuracy: 0.9011\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1465 - accuracy: 0.9615 - val_loss: 0.3755 - val_accuracy: 0.9121\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1481 - accuracy: 0.9560 - val_loss: 0.3818 - val_accuracy: 0.9011\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1452 - accuracy: 0.9588 - val_loss: 0.3794 - val_accuracy: 0.9011\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1478 - accuracy: 0.9588 - val_loss: 0.3735 - val_accuracy: 0.9121\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1470 - accuracy: 0.9588 - val_loss: 0.3782 - val_accuracy: 0.9011\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.3825 - val_accuracy: 0.9011\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1452 - accuracy: 0.9588 - val_loss: 0.3808 - val_accuracy: 0.9011\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1452 - accuracy: 0.9588 - val_loss: 0.3807 - val_accuracy: 0.9011\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.3833 - val_accuracy: 0.9011\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1451 - accuracy: 0.9588 - val_loss: 0.3852 - val_accuracy: 0.9011\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1458 - accuracy: 0.9588 - val_loss: 0.3792 - val_accuracy: 0.9121\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1458 - accuracy: 0.9588 - val_loss: 0.3805 - val_accuracy: 0.9121\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1460 - accuracy: 0.9588 - val_loss: 0.3887 - val_accuracy: 0.9011\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1452 - accuracy: 0.9588 - val_loss: 0.3874 - val_accuracy: 0.9011\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1460 - accuracy: 0.9560 - val_loss: 0.3884 - val_accuracy: 0.9011\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.3916 - val_accuracy: 0.9011\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1460 - accuracy: 0.9588 - val_loss: 0.3924 - val_accuracy: 0.9011\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1460 - accuracy: 0.9588 - val_loss: 0.3855 - val_accuracy: 0.9011\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1451 - accuracy: 0.9588 - val_loss: 0.3844 - val_accuracy: 0.9011\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1448 - accuracy: 0.9588 - val_loss: 0.3862 - val_accuracy: 0.9011\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.3855 - val_accuracy: 0.9011\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1446 - accuracy: 0.9588 - val_loss: 0.3866 - val_accuracy: 0.9011\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1451 - accuracy: 0.9588 - val_loss: 0.3906 - val_accuracy: 0.9011\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1448 - accuracy: 0.9588 - val_loss: 0.3916 - val_accuracy: 0.9011\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.3874 - val_accuracy: 0.9121\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.3860 - val_accuracy: 0.9121\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1454 - accuracy: 0.9588 - val_loss: 0.3903 - val_accuracy: 0.9011\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1444 - accuracy: 0.9588 - val_loss: 0.3910 - val_accuracy: 0.9011\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1442 - accuracy: 0.9588 - val_loss: 0.3916 - val_accuracy: 0.9011\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1452 - accuracy: 0.9588 - val_loss: 0.3946 - val_accuracy: 0.9011\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1446 - accuracy: 0.9560 - val_loss: 0.3943 - val_accuracy: 0.9011\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.3961 - val_accuracy: 0.9011\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.3951 - val_accuracy: 0.9011\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.3952 - val_accuracy: 0.9011\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.3940 - val_accuracy: 0.9011\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1449 - accuracy: 0.9588 - val_loss: 0.3936 - val_accuracy: 0.9011\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1440 - accuracy: 0.9588 - val_loss: 0.3955 - val_accuracy: 0.9011\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.3965 - val_accuracy: 0.9011\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.3984 - val_accuracy: 0.9011\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1442 - accuracy: 0.9588 - val_loss: 0.3965 - val_accuracy: 0.9011\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1451 - accuracy: 0.9560 - val_loss: 0.3971 - val_accuracy: 0.9121\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1443 - accuracy: 0.9588 - val_loss: 0.4017 - val_accuracy: 0.9011\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1444 - accuracy: 0.9588 - val_loss: 0.3992 - val_accuracy: 0.9011\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1446 - accuracy: 0.9588 - val_loss: 0.4000 - val_accuracy: 0.9011\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.4012 - val_accuracy: 0.9011\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1444 - accuracy: 0.9588 - val_loss: 0.3990 - val_accuracy: 0.9011\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1443 - accuracy: 0.9588 - val_loss: 0.3992 - val_accuracy: 0.9011\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1455 - accuracy: 0.9560 - val_loss: 0.3984 - val_accuracy: 0.9011\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.4014 - val_accuracy: 0.9011\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.3981 - val_accuracy: 0.9011\n",
            "Epoch 296/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1442 - accuracy: 0.9588 - val_loss: 0.3950 - val_accuracy: 0.9011\n",
            "Epoch 297/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1451 - accuracy: 0.9560 - val_loss: 0.3950 - val_accuracy: 0.9011\n",
            "Epoch 298/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1461 - accuracy: 0.9588 - val_loss: 0.4009 - val_accuracy: 0.9011\n",
            "Epoch 299/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1446 - accuracy: 0.9588 - val_loss: 0.3956 - val_accuracy: 0.9011\n",
            "Epoch 300/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1440 - accuracy: 0.9588 - val_loss: 0.3938 - val_accuracy: 0.9011\n",
            "Epoch 301/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1452 - accuracy: 0.9588 - val_loss: 0.3936 - val_accuracy: 0.9121\n",
            "Epoch 302/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1443 - accuracy: 0.9588 - val_loss: 0.4014 - val_accuracy: 0.9011\n",
            "Epoch 303/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.4036 - val_accuracy: 0.9011\n",
            "Epoch 304/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.4002 - val_accuracy: 0.9011\n",
            "Epoch 305/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1444 - accuracy: 0.9588 - val_loss: 0.4010 - val_accuracy: 0.9011\n",
            "Epoch 306/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1441 - accuracy: 0.9588 - val_loss: 0.3996 - val_accuracy: 0.9011\n",
            "Epoch 307/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1440 - accuracy: 0.9588 - val_loss: 0.3994 - val_accuracy: 0.9011\n",
            "Epoch 308/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1439 - accuracy: 0.9588 - val_loss: 0.4022 - val_accuracy: 0.9011\n",
            "Epoch 309/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1440 - accuracy: 0.9588 - val_loss: 0.4012 - val_accuracy: 0.9011\n",
            "Epoch 310/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1440 - accuracy: 0.9588 - val_loss: 0.4021 - val_accuracy: 0.9011\n",
            "Epoch 311/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.3994 - val_accuracy: 0.9011\n",
            "Epoch 312/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1443 - accuracy: 0.9588 - val_loss: 0.4031 - val_accuracy: 0.9011\n",
            "Epoch 313/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1444 - accuracy: 0.9588 - val_loss: 0.4031 - val_accuracy: 0.9011\n",
            "Epoch 314/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1442 - accuracy: 0.9588 - val_loss: 0.4016 - val_accuracy: 0.9121\n",
            "Epoch 315/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1457 - accuracy: 0.9560 - val_loss: 0.4079 - val_accuracy: 0.9011\n",
            "Epoch 316/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4059 - val_accuracy: 0.9011\n",
            "Epoch 317/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4050 - val_accuracy: 0.9011\n",
            "Epoch 318/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1441 - accuracy: 0.9588 - val_loss: 0.4048 - val_accuracy: 0.9011\n",
            "Epoch 319/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1443 - accuracy: 0.9588 - val_loss: 0.4030 - val_accuracy: 0.9011\n",
            "Epoch 320/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1442 - accuracy: 0.9588 - val_loss: 0.4082 - val_accuracy: 0.9011\n",
            "Epoch 321/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1446 - accuracy: 0.9588 - val_loss: 0.4093 - val_accuracy: 0.9011\n",
            "Epoch 322/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1451 - accuracy: 0.9588 - val_loss: 0.4035 - val_accuracy: 0.9121\n",
            "Epoch 323/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1453 - accuracy: 0.9588 - val_loss: 0.4029 - val_accuracy: 0.9121\n",
            "Epoch 324/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1455 - accuracy: 0.9588 - val_loss: 0.4131 - val_accuracy: 0.9011\n",
            "Epoch 325/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.4056 - val_accuracy: 0.9011\n",
            "Epoch 326/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1442 - accuracy: 0.9588 - val_loss: 0.4012 - val_accuracy: 0.9121\n",
            "Epoch 327/500\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1444 - accuracy: 0.9560 - val_loss: 0.4051 - val_accuracy: 0.9011\n",
            "Epoch 328/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4078 - val_accuracy: 0.9011\n",
            "Epoch 329/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4068 - val_accuracy: 0.9011\n",
            "Epoch 330/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1439 - accuracy: 0.9588 - val_loss: 0.4055 - val_accuracy: 0.9011\n",
            "Epoch 331/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1450 - accuracy: 0.9588 - val_loss: 0.4135 - val_accuracy: 0.9011\n",
            "Epoch 332/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1441 - accuracy: 0.9588 - val_loss: 0.4127 - val_accuracy: 0.9011\n",
            "Epoch 333/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1439 - accuracy: 0.9588 - val_loss: 0.4090 - val_accuracy: 0.9011\n",
            "Epoch 334/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1438 - accuracy: 0.9588 - val_loss: 0.4104 - val_accuracy: 0.9011\n",
            "Epoch 335/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1443 - accuracy: 0.9588 - val_loss: 0.4049 - val_accuracy: 0.9121\n",
            "Epoch 336/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1439 - accuracy: 0.9588 - val_loss: 0.4067 - val_accuracy: 0.9121\n",
            "Epoch 337/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4119 - val_accuracy: 0.9011\n",
            "Epoch 338/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1435 - accuracy: 0.9588 - val_loss: 0.4116 - val_accuracy: 0.9121\n",
            "Epoch 339/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1449 - accuracy: 0.9588 - val_loss: 0.4114 - val_accuracy: 0.9121\n",
            "Epoch 340/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1438 - accuracy: 0.9588 - val_loss: 0.4123 - val_accuracy: 0.9121\n",
            "Epoch 341/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1436 - accuracy: 0.9588 - val_loss: 0.4125 - val_accuracy: 0.9121\n",
            "Epoch 342/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1445 - accuracy: 0.9560 - val_loss: 0.4186 - val_accuracy: 0.9011\n",
            "Epoch 343/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1440 - accuracy: 0.9588 - val_loss: 0.4177 - val_accuracy: 0.9011\n",
            "Epoch 344/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1439 - accuracy: 0.9588 - val_loss: 0.4122 - val_accuracy: 0.9121\n",
            "Epoch 345/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.4114 - val_accuracy: 0.9121\n",
            "Epoch 346/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1449 - accuracy: 0.9560 - val_loss: 0.4195 - val_accuracy: 0.9011\n",
            "Epoch 347/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1455 - accuracy: 0.9588 - val_loss: 0.4160 - val_accuracy: 0.9011\n",
            "Epoch 348/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1441 - accuracy: 0.9588 - val_loss: 0.4065 - val_accuracy: 0.9011\n",
            "Epoch 349/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1460 - accuracy: 0.9588 - val_loss: 0.4074 - val_accuracy: 0.9121\n",
            "Epoch 350/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1455 - accuracy: 0.9560 - val_loss: 0.4151 - val_accuracy: 0.9011\n",
            "Epoch 351/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1439 - accuracy: 0.9588 - val_loss: 0.4159 - val_accuracy: 0.9011\n",
            "Epoch 352/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1435 - accuracy: 0.9588 - val_loss: 0.4184 - val_accuracy: 0.9011\n",
            "Epoch 353/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1439 - accuracy: 0.9588 - val_loss: 0.4158 - val_accuracy: 0.9011\n",
            "Epoch 354/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1433 - accuracy: 0.9588 - val_loss: 0.4163 - val_accuracy: 0.9011\n",
            "Epoch 355/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1449 - accuracy: 0.9560 - val_loss: 0.4120 - val_accuracy: 0.9011\n",
            "Epoch 356/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.4137 - val_accuracy: 0.9011\n",
            "Epoch 357/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1453 - accuracy: 0.9588 - val_loss: 0.4210 - val_accuracy: 0.9011\n",
            "Epoch 358/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1435 - accuracy: 0.9588 - val_loss: 0.4165 - val_accuracy: 0.9011\n",
            "Epoch 359/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.4150 - val_accuracy: 0.9011\n",
            "Epoch 360/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4158 - val_accuracy: 0.9011\n",
            "Epoch 361/500\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.1433 - accuracy: 0.9588 - val_loss: 0.4195 - val_accuracy: 0.9011\n",
            "Epoch 362/500\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.1436 - accuracy: 0.9588 - val_loss: 0.4229 - val_accuracy: 0.9011\n",
            "Epoch 363/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1435 - accuracy: 0.9588 - val_loss: 0.4180 - val_accuracy: 0.9011\n",
            "Epoch 364/500\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1435 - accuracy: 0.9588 - val_loss: 0.4173 - val_accuracy: 0.9011\n",
            "Epoch 365/500\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4172 - val_accuracy: 0.9011\n",
            "Epoch 366/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.4157 - val_accuracy: 0.9011\n",
            "Epoch 367/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1435 - accuracy: 0.9588 - val_loss: 0.4145 - val_accuracy: 0.9011\n",
            "Epoch 368/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1433 - accuracy: 0.9588 - val_loss: 0.4141 - val_accuracy: 0.9011\n",
            "Epoch 369/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4186 - val_accuracy: 0.9011\n",
            "Epoch 370/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1431 - accuracy: 0.9588 - val_loss: 0.4196 - val_accuracy: 0.9011\n",
            "Epoch 371/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1441 - accuracy: 0.9560 - val_loss: 0.4216 - val_accuracy: 0.9011\n",
            "Epoch 372/500\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4250 - val_accuracy: 0.9011\n",
            "Epoch 373/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4261 - val_accuracy: 0.9011\n",
            "Epoch 374/500\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4209 - val_accuracy: 0.9011\n",
            "Epoch 375/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4216 - val_accuracy: 0.9011\n",
            "Epoch 376/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4187 - val_accuracy: 0.9011\n",
            "Epoch 377/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1428 - accuracy: 0.9588 - val_loss: 0.4203 - val_accuracy: 0.9011\n",
            "Epoch 378/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1435 - accuracy: 0.9588 - val_loss: 0.4233 - val_accuracy: 0.9011\n",
            "Epoch 379/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4234 - val_accuracy: 0.9011\n",
            "Epoch 380/500\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.4206 - val_accuracy: 0.9011\n",
            "Epoch 381/500\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1431 - accuracy: 0.9588 - val_loss: 0.4210 - val_accuracy: 0.9011\n",
            "Epoch 382/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1425 - accuracy: 0.9588 - val_loss: 0.4244 - val_accuracy: 0.9011\n",
            "Epoch 383/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4244 - val_accuracy: 0.9011\n",
            "Epoch 384/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1435 - accuracy: 0.9588 - val_loss: 0.4245 - val_accuracy: 0.9011\n",
            "Epoch 385/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1441 - accuracy: 0.9560 - val_loss: 0.4210 - val_accuracy: 0.9011\n",
            "Epoch 386/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1431 - accuracy: 0.9588 - val_loss: 0.4243 - val_accuracy: 0.9011\n",
            "Epoch 387/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4234 - val_accuracy: 0.9011\n",
            "Epoch 388/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1432 - accuracy: 0.9588 - val_loss: 0.4262 - val_accuracy: 0.9011\n",
            "Epoch 389/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1428 - accuracy: 0.9588 - val_loss: 0.4291 - val_accuracy: 0.9011\n",
            "Epoch 390/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4290 - val_accuracy: 0.9011\n",
            "Epoch 391/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4286 - val_accuracy: 0.9011\n",
            "Epoch 392/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1426 - accuracy: 0.9588 - val_loss: 0.4276 - val_accuracy: 0.9011\n",
            "Epoch 393/500\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4260 - val_accuracy: 0.9011\n",
            "Epoch 394/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.4257 - val_accuracy: 0.9011\n",
            "Epoch 395/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.4233 - val_accuracy: 0.9011\n",
            "Epoch 396/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1428 - accuracy: 0.9588 - val_loss: 0.4263 - val_accuracy: 0.9011\n",
            "Epoch 397/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4263 - val_accuracy: 0.9011\n",
            "Epoch 398/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1425 - accuracy: 0.9588 - val_loss: 0.4277 - val_accuracy: 0.9011\n",
            "Epoch 399/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4297 - val_accuracy: 0.9011\n",
            "Epoch 400/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4303 - val_accuracy: 0.9011\n",
            "Epoch 401/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4296 - val_accuracy: 0.9011\n",
            "Epoch 402/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4336 - val_accuracy: 0.9011\n",
            "Epoch 403/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1428 - accuracy: 0.9588 - val_loss: 0.4302 - val_accuracy: 0.9011\n",
            "Epoch 404/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4286 - val_accuracy: 0.9011\n",
            "Epoch 405/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1433 - accuracy: 0.9588 - val_loss: 0.4289 - val_accuracy: 0.9011\n",
            "Epoch 406/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1428 - accuracy: 0.9588 - val_loss: 0.4289 - val_accuracy: 0.9011\n",
            "Epoch 407/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1431 - accuracy: 0.9588 - val_loss: 0.4300 - val_accuracy: 0.9011\n",
            "Epoch 408/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4344 - val_accuracy: 0.9011\n",
            "Epoch 409/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1433 - accuracy: 0.9588 - val_loss: 0.4351 - val_accuracy: 0.9011\n",
            "Epoch 410/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1437 - accuracy: 0.9560 - val_loss: 0.4318 - val_accuracy: 0.9011\n",
            "Epoch 411/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1425 - accuracy: 0.9588 - val_loss: 0.4334 - val_accuracy: 0.9011\n",
            "Epoch 412/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4355 - val_accuracy: 0.9011\n",
            "Epoch 413/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1432 - accuracy: 0.9588 - val_loss: 0.4345 - val_accuracy: 0.9011\n",
            "Epoch 414/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.4277 - val_accuracy: 0.9121\n",
            "Epoch 415/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1426 - accuracy: 0.9560 - val_loss: 0.4301 - val_accuracy: 0.9011\n",
            "Epoch 416/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4342 - val_accuracy: 0.9011\n",
            "Epoch 417/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4308 - val_accuracy: 0.9011\n",
            "Epoch 418/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1425 - accuracy: 0.9588 - val_loss: 0.4310 - val_accuracy: 0.9011\n",
            "Epoch 419/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1439 - accuracy: 0.9560 - val_loss: 0.4284 - val_accuracy: 0.9011\n",
            "Epoch 420/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4348 - val_accuracy: 0.9011\n",
            "Epoch 421/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4375 - val_accuracy: 0.9011\n",
            "Epoch 422/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1424 - accuracy: 0.9588 - val_loss: 0.4331 - val_accuracy: 0.9011\n",
            "Epoch 423/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4326 - val_accuracy: 0.9011\n",
            "Epoch 424/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1422 - accuracy: 0.9588 - val_loss: 0.4345 - val_accuracy: 0.9011\n",
            "Epoch 425/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1431 - accuracy: 0.9588 - val_loss: 0.4377 - val_accuracy: 0.9011\n",
            "Epoch 426/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1433 - accuracy: 0.9588 - val_loss: 0.4338 - val_accuracy: 0.9011\n",
            "Epoch 427/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4302 - val_accuracy: 0.9011\n",
            "Epoch 428/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4301 - val_accuracy: 0.9011\n",
            "Epoch 429/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1426 - accuracy: 0.9588 - val_loss: 0.4330 - val_accuracy: 0.9011\n",
            "Epoch 430/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1438 - accuracy: 0.9588 - val_loss: 0.4389 - val_accuracy: 0.9011\n",
            "Epoch 431/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1431 - accuracy: 0.9588 - val_loss: 0.4347 - val_accuracy: 0.9011\n",
            "Epoch 432/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4331 - val_accuracy: 0.9011\n",
            "Epoch 433/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1432 - accuracy: 0.9588 - val_loss: 0.4336 - val_accuracy: 0.9011\n",
            "Epoch 434/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1428 - accuracy: 0.9588 - val_loss: 0.4347 - val_accuracy: 0.9011\n",
            "Epoch 435/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.4362 - val_accuracy: 0.9011\n",
            "Epoch 436/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1421 - accuracy: 0.9588 - val_loss: 0.4356 - val_accuracy: 0.9011\n",
            "Epoch 437/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1422 - accuracy: 0.9588 - val_loss: 0.4351 - val_accuracy: 0.9011\n",
            "Epoch 438/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.4361 - val_accuracy: 0.9011\n",
            "Epoch 439/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1426 - accuracy: 0.9588 - val_loss: 0.4335 - val_accuracy: 0.9011\n",
            "Epoch 440/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1422 - accuracy: 0.9588 - val_loss: 0.4337 - val_accuracy: 0.9011\n",
            "Epoch 441/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1429 - accuracy: 0.9560 - val_loss: 0.4339 - val_accuracy: 0.9011\n",
            "Epoch 442/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1418 - accuracy: 0.9588 - val_loss: 0.4400 - val_accuracy: 0.9011\n",
            "Epoch 443/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1437 - accuracy: 0.9588 - val_loss: 0.4400 - val_accuracy: 0.9011\n",
            "Epoch 444/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1423 - accuracy: 0.9588 - val_loss: 0.4392 - val_accuracy: 0.9011\n",
            "Epoch 445/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1423 - accuracy: 0.9588 - val_loss: 0.4397 - val_accuracy: 0.9011\n",
            "Epoch 446/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1426 - accuracy: 0.9588 - val_loss: 0.4375 - val_accuracy: 0.9011\n",
            "Epoch 447/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1419 - accuracy: 0.9588 - val_loss: 0.4368 - val_accuracy: 0.9011\n",
            "Epoch 448/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4397 - val_accuracy: 0.9011\n",
            "Epoch 449/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4394 - val_accuracy: 0.9011\n",
            "Epoch 450/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.4362 - val_accuracy: 0.9011\n",
            "Epoch 451/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1434 - accuracy: 0.9588 - val_loss: 0.4402 - val_accuracy: 0.9011\n",
            "Epoch 452/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1424 - accuracy: 0.9588 - val_loss: 0.4373 - val_accuracy: 0.9011\n",
            "Epoch 453/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1432 - accuracy: 0.9588 - val_loss: 0.4403 - val_accuracy: 0.9011\n",
            "Epoch 454/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1421 - accuracy: 0.9588 - val_loss: 0.4381 - val_accuracy: 0.9011\n",
            "Epoch 455/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1420 - accuracy: 0.9588 - val_loss: 0.4367 - val_accuracy: 0.9011\n",
            "Epoch 456/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1426 - accuracy: 0.9588 - val_loss: 0.4405 - val_accuracy: 0.9011\n",
            "Epoch 457/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1423 - accuracy: 0.9588 - val_loss: 0.4373 - val_accuracy: 0.9011\n",
            "Epoch 458/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1422 - accuracy: 0.9588 - val_loss: 0.4362 - val_accuracy: 0.9011\n",
            "Epoch 459/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1423 - accuracy: 0.9588 - val_loss: 0.4395 - val_accuracy: 0.9011\n",
            "Epoch 460/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4408 - val_accuracy: 0.9011\n",
            "Epoch 461/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1431 - accuracy: 0.9588 - val_loss: 0.4429 - val_accuracy: 0.9011\n",
            "Epoch 462/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1420 - accuracy: 0.9588 - val_loss: 0.4396 - val_accuracy: 0.9011\n",
            "Epoch 463/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1424 - accuracy: 0.9588 - val_loss: 0.4382 - val_accuracy: 0.9011\n",
            "Epoch 464/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1423 - accuracy: 0.9588 - val_loss: 0.4411 - val_accuracy: 0.9011\n",
            "Epoch 465/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1420 - accuracy: 0.9588 - val_loss: 0.4413 - val_accuracy: 0.9011\n",
            "Epoch 466/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1421 - accuracy: 0.9588 - val_loss: 0.4434 - val_accuracy: 0.9011\n",
            "Epoch 467/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1424 - accuracy: 0.9588 - val_loss: 0.4449 - val_accuracy: 0.9011\n",
            "Epoch 468/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1423 - accuracy: 0.9588 - val_loss: 0.4431 - val_accuracy: 0.9011\n",
            "Epoch 469/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1424 - accuracy: 0.9588 - val_loss: 0.4437 - val_accuracy: 0.9011\n",
            "Epoch 470/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1421 - accuracy: 0.9588 - val_loss: 0.4440 - val_accuracy: 0.9011\n",
            "Epoch 471/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4452 - val_accuracy: 0.9011\n",
            "Epoch 472/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1447 - accuracy: 0.9588 - val_loss: 0.4522 - val_accuracy: 0.9011\n",
            "Epoch 473/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1428 - accuracy: 0.9615 - val_loss: 0.4443 - val_accuracy: 0.9121\n",
            "Epoch 474/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.4390 - val_accuracy: 0.9121\n",
            "Epoch 475/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1421 - accuracy: 0.9588 - val_loss: 0.4406 - val_accuracy: 0.9011\n",
            "Epoch 476/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1425 - accuracy: 0.9588 - val_loss: 0.4419 - val_accuracy: 0.9011\n",
            "Epoch 477/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1429 - accuracy: 0.9588 - val_loss: 0.4413 - val_accuracy: 0.9011\n",
            "Epoch 478/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1424 - accuracy: 0.9588 - val_loss: 0.4420 - val_accuracy: 0.9011\n",
            "Epoch 479/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1421 - accuracy: 0.9588 - val_loss: 0.4445 - val_accuracy: 0.9011\n",
            "Epoch 480/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1420 - accuracy: 0.9588 - val_loss: 0.4472 - val_accuracy: 0.9011\n",
            "Epoch 481/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1417 - accuracy: 0.9588 - val_loss: 0.4473 - val_accuracy: 0.9011\n",
            "Epoch 482/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1419 - accuracy: 0.9588 - val_loss: 0.4464 - val_accuracy: 0.9011\n",
            "Epoch 483/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1435 - accuracy: 0.9588 - val_loss: 0.4491 - val_accuracy: 0.9011\n",
            "Epoch 484/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.4438 - val_accuracy: 0.9121\n",
            "Epoch 485/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1430 - accuracy: 0.9588 - val_loss: 0.4440 - val_accuracy: 0.9011\n",
            "Epoch 486/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1415 - accuracy: 0.9588 - val_loss: 0.4490 - val_accuracy: 0.9011\n",
            "Epoch 487/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1431 - accuracy: 0.9588 - val_loss: 0.4500 - val_accuracy: 0.9011\n",
            "Epoch 488/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1423 - accuracy: 0.9588 - val_loss: 0.4462 - val_accuracy: 0.9011\n",
            "Epoch 489/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1422 - accuracy: 0.9588 - val_loss: 0.4421 - val_accuracy: 0.9011\n",
            "Epoch 490/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1439 - accuracy: 0.9560 - val_loss: 0.4436 - val_accuracy: 0.9011\n",
            "Epoch 491/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1422 - accuracy: 0.9588 - val_loss: 0.4507 - val_accuracy: 0.9011\n",
            "Epoch 492/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1422 - accuracy: 0.9588 - val_loss: 0.4476 - val_accuracy: 0.9011\n",
            "Epoch 493/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1418 - accuracy: 0.9588 - val_loss: 0.4475 - val_accuracy: 0.9011\n",
            "Epoch 494/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1424 - accuracy: 0.9588 - val_loss: 0.4481 - val_accuracy: 0.9011\n",
            "Epoch 495/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1422 - accuracy: 0.9588 - val_loss: 0.4469 - val_accuracy: 0.9011\n",
            "Epoch 496/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1417 - accuracy: 0.9588 - val_loss: 0.4464 - val_accuracy: 0.9011\n",
            "Epoch 497/500\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1419 - accuracy: 0.9588 - val_loss: 0.4467 - val_accuracy: 0.9011\n",
            "Epoch 498/500\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1418 - accuracy: 0.9588 - val_loss: 0.4480 - val_accuracy: 0.9011\n",
            "Epoch 499/500\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.1426 - accuracy: 0.9588 - val_loss: 0.4500 - val_accuracy: 0.9011\n",
            "Epoch 500/500\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1426 - accuracy: 0.9588 - val_loss: 0.4472 - val_accuracy: 0.9011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred_DNN = (model.predict(DNNX) > 0.5).astype(\"int32\")\n",
        "DTr = model.evaluate(DX_train, Dy_train,verbose=0)[1]\n",
        "DTe = model.evaluate(DX_test, Dy_test,verbose=0)[1]\n",
        "DTr,DTe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhG0YCbhASH9",
        "outputId": "63417c60-5f0c-4a09-b2b7-1233114a2e64"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9472527503967285, 0.9210526347160339)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_DNN = (model.predict(DX_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKbxrS57kOBE",
        "outputId": "f69c0d78-8bb3-4754-e69f-5b3e0ab1a860"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"DNN\":[DTr,DTe]})\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "v_YjlCdOO1JY",
        "outputId": "1c2327a7-f92b-4d00-f12d-b6a3403408ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        train      test\n",
              "SVM  0.951648  0.956140\n",
              "KNN  0.980220  0.991228\n",
              "RF   0.975824  0.991228\n",
              "LR   0.980220  0.973684\n",
              "ANN  0.960440  0.973684\n",
              "XGB  0.993407  0.982456\n",
              "DNN  0.947253  0.921053"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08e767bc-f4b6-44ca-b062-43a206a8aadd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.991228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>0.975824</td>\n",
              "      <td>0.991228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.973684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>0.960440</td>\n",
              "      <td>0.973684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>0.993407</td>\n",
              "      <td>0.982456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>0.947253</td>\n",
              "      <td>0.921053</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08e767bc-f4b6-44ca-b062-43a206a8aadd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-08e767bc-f4b6-44ca-b062-43a206a8aadd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-08e767bc-f4b6-44ca-b062-43a206a8aadd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AutoML Individual and AutoML DNN**"
      ],
      "metadata": {
        "id": "nJaAMLDKKfKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H2O AutoML"
      ],
      "metadata": {
        "id": "EhNt_UkjDKcV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h2o\n",
        "import h2o\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "h2o.init()\n",
        "from h2o.model.segment_models import H2OFrame\n",
        "from h2o.automl import H2OAutoML\n",
        "print(\"All Library Loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "iwMF9ROeDOzF",
        "outputId": "6b78433e-f63b-4aae-9b59-ef0660253cea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h2o\n",
            "  Downloading h2o-3.40.0.4.tar.gz (177.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.8.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from h2o) (0.18.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.4)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.40.0.4-py2.py3-none-any.whl size=177697886 sha256=473cf63585f1e94c2e55133f2096c1553d94adc01f535b6e36c4e606b8126b28\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/f2/b0/5bb4d702a0467e82d77c45088db3eef25114c26b0eec8e7f6a\n",
            "Successfully built h2o\n",
            "Installing collected packages: h2o\n",
            "Successfully installed h2o-3.40.0.4\n",
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.19\" 2023-04-18; OpenJDK Runtime Environment (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1); OpenJDK 64-Bit Server VM (build 11.0.19+7-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.10/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpgwnu0z28\n",
            "  JVM stdout: /tmp/tmpgwnu0z28/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpgwnu0z28/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         03 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.40.0.4\n",
              "H2O_cluster_version_age:    2 months and 5 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_rkm179\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.170 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-1.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-1 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-1 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-1 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table th,\n",
              "#h2o-table-1 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>03 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.40.0.4</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>2 months and 5 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_rkm179</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.170 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Library Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWdoOLbsF2qs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-QiglXeF_XE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8P1HTOHbugE-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asKywHvIu83f"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5wVvB0pvWHe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train, valid = hdf.split_frame(ratios=[.8], seed=123)\n",
        "#hdf = h2o.H2OFrame(df)\n",
        "#hdf[\"diagnosis\"] = hdf[\"diagnosis\"].asfactor()\n",
        "hy = \"diagnosis\"\n",
        "hx = list(df.columns)\n",
        "hx.remove(hy)\n",
        "hdf  = df.copy()\n",
        "hdf.iloc[:,1:] = StandardScaler().fit_transform(hdf.iloc[:,1:])\n",
        "hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n",
        "hdf.iloc[:,0] = hdf.iloc[:,0].astype('category')\n",
        "train1, valid1 = train_test_split(hdf, test_size=0.2,random_state=123)\n",
        "train = h2o.H2OFrame(train1)\n",
        "valid = h2o.H2OFrame(valid1)\n",
        "train[\"diagnosis\"] = train[\"diagnosis\"].asfactor()\n",
        "valid[\"diagnosis\"] = valid[\"diagnosis\"].asfactor()"
      ],
      "metadata": {
        "id": "NVoFNbYCGxGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ab1392-e82f-44d2-9071-eb55deb88ff8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-460708a37676>:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  hdf.iloc[:,0] = LabelEncoder().fit_transform(hdf.iloc[:,0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = time.time()\n",
        "aml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\",\n",
        "                nfolds=10, sort_metric='accuracy')\n",
        "aml.train(x = hx, y = hy, training_frame = train,\n",
        "          validation_frame = valid)\n",
        "autoend = time.time() - st"
      ],
      "metadata": {
        "id": "7JNO6XnVHH5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c4ca0e-0cf3-41f5-88e8-39b92350bb22"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoML progress: |\n",
            "10:42:57.961: Project: AutoML_1_20230704_104257\n",
            "10:42:57.964: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "10:42:57.966: Setting stopping tolerance adaptively based on the training frame: 0.04688072309384954\n",
            "10:42:57.968: Build control seed: 123\n",
            "10:42:57.969: training frame: Frame key: AutoML_1_20230704_104257_training_py_1_sid_a072    cols: 31    rows: 455  chunks: 1    size: 111919  checksum: -6999049652048799504\n",
            "10:42:57.976: validation frame: Frame key: py_2_sid_a072    cols: 31    rows: 114  chunks: 1    size: 30037  checksum: 1244986154056216624\n",
            "10:42:57.976: leaderboard frame: NULL\n",
            "10:42:57.976: blending frame: NULL\n",
            "10:42:57.976: response column: diagnosis\n",
            "10:42:57.976: fold column: null\n",
            "10:42:57.976: weights column: null\n",
            "10:42:58.11: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "10:42:58.53: AutoML job created: 2023.07.04 10:42:57.907\n",
            "10:42:58.56: AutoML build started: 2023.07.04 10:42:58.55\n",
            "10:42:58.106: AutoML: starting XGBoost_1_AutoML_1_20230704_104257 model training\n",
            "\n",
            "█\n",
            "10:43:04.343: New leader: XGBoost_1_AutoML_1_20230704_104257, accuracy: 0.9428571428571428\n",
            "10:43:04.369: AutoML: starting GLM_1_AutoML_1_20230704_104257 model training\n",
            "\n",
            "██\n",
            "10:43:11.0: AutoML: starting GBM_1_AutoML_1_20230704_104257 model training\n",
            "\n",
            "██\n",
            "10:43:28.393: AutoML: starting XGBoost_2_AutoML_1_20230704_104257 model training\n",
            "\n",
            "█\n",
            "10:43:33.188: AutoML: starting DRF_1_AutoML_1_20230704_104257 model training\n",
            "\n",
            "\n",
            "10:43:37.860: AutoML: starting GBM_2_AutoML_1_20230704_104257 model training\n",
            "\n",
            "███\n",
            "10:43:51.263: AutoML: starting GBM_3_AutoML_1_20230704_104257 model training\n",
            "\n",
            "█\n",
            "10:44:03.692: AutoML: starting GBM_4_AutoML_1_20230704_104257 model training\n",
            "\n",
            "█\n",
            "10:44:14.736: AutoML: starting XGBoost_3_AutoML_1_20230704_104257 model training\n",
            "\n",
            "█\n",
            "10:44:18.983: AutoML: starting XRT_1_AutoML_1_20230704_104257 model training\n",
            "\n",
            "█\n",
            "10:44:25.695: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "10:44:25.706: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_1_20230704_104257 model training\n",
            "\n",
            "██\n",
            "10:44:29.67: AutoML: starting StackedEnsemble_AllModels_1_AutoML_1_20230704_104257 model training\n",
            "\n",
            "████████████████████████████████████████████████| (done) 100%\n",
            "\n",
            "10:44:32.633: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "10:44:32.633: AutoML build stopped: 2023.07.04 10:44:32.633\n",
            "10:44:32.633: AutoML build done: built 10 models\n",
            "10:44:32.633: AutoML duration:  1 min 34.578 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lbo606kFH4Zc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lb = aml.leaderboard\n",
        "lb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "gmCXw_D_4y1D",
        "outputId": "9942bb18-b470-41ca-f68d-b0723ed9b441"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_id                              accuracy       auc    logloss     aucpr    mean_per_class_error      rmse        mse\n",
              "----------------------------------  ----------  --------  ---------  --------  ----------------------  --------  ---------\n",
              "XGBoost_1_AutoML_1_20230704_104257    0.942857  0.984114  0.166379   0.979636               0.0539185  0.210251  0.0442054\n",
              "XRT_1_AutoML_1_20230704_104257        0.96044   0.991619  0.116734   0.98782                0.0421609  0.184096  0.0338912\n",
              "DRF_1_AutoML_1_20230704_104257        0.96044   0.987058  0.249487   0.985869               0.0421609  0.18258   0.0333355\n",
              "XGBoost_2_AutoML_1_20230704_104257    0.962637  0.990394  0.11331    0.987678               0.0392369  0.174879  0.0305826\n",
              "GBM_2_AutoML_1_20230704_104257        0.967033  0.994255  0.0933299  0.991681               0.0380426  0.162663  0.0264594\n",
              "GBM_4_AutoML_1_20230704_104257        0.967033  0.993246  0.0998609  0.990397               0.0333889  0.170342  0.0290164\n",
              "XGBoost_3_AutoML_1_20230704_104257    0.969231  0.992999  0.0965668  0.990609               0.0316284  0.161568  0.0261043\n",
              "GBM_1_AutoML_1_20230704_104257        0.971429  0.994461  0.0911605  0.992401               0.0287044  0.158892  0.0252468\n",
              "GBM_3_AutoML_1_20230704_104257        0.971429  0.994461  0.0902413  0.991937               0.027541   0.157908  0.0249349\n",
              "GLM_1_AutoML_1_20230704_104257        0.978022  0.996479  0.0739483  0.994833               0.0269129  0.1449    0.0209961\n",
              "[10 rows x 8 columns]\n"
            ],
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th>model_id                          </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">      mse</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>XGBoost_1_AutoML_1_20230704_104257</td><td style=\"text-align: right;\">  0.942857</td><td style=\"text-align: right;\">0.984114</td><td style=\"text-align: right;\">0.166379 </td><td style=\"text-align: right;\">0.979636</td><td style=\"text-align: right;\">             0.0539185</td><td style=\"text-align: right;\">0.210251</td><td style=\"text-align: right;\">0.0442054</td></tr>\n",
              "<tr><td>XRT_1_AutoML_1_20230704_104257    </td><td style=\"text-align: right;\">  0.96044 </td><td style=\"text-align: right;\">0.991619</td><td style=\"text-align: right;\">0.116734 </td><td style=\"text-align: right;\">0.98782 </td><td style=\"text-align: right;\">             0.0421609</td><td style=\"text-align: right;\">0.184096</td><td style=\"text-align: right;\">0.0338912</td></tr>\n",
              "<tr><td>DRF_1_AutoML_1_20230704_104257    </td><td style=\"text-align: right;\">  0.96044 </td><td style=\"text-align: right;\">0.987058</td><td style=\"text-align: right;\">0.249487 </td><td style=\"text-align: right;\">0.985869</td><td style=\"text-align: right;\">             0.0421609</td><td style=\"text-align: right;\">0.18258 </td><td style=\"text-align: right;\">0.0333355</td></tr>\n",
              "<tr><td>XGBoost_2_AutoML_1_20230704_104257</td><td style=\"text-align: right;\">  0.962637</td><td style=\"text-align: right;\">0.990394</td><td style=\"text-align: right;\">0.11331  </td><td style=\"text-align: right;\">0.987678</td><td style=\"text-align: right;\">             0.0392369</td><td style=\"text-align: right;\">0.174879</td><td style=\"text-align: right;\">0.0305826</td></tr>\n",
              "<tr><td>GBM_2_AutoML_1_20230704_104257    </td><td style=\"text-align: right;\">  0.967033</td><td style=\"text-align: right;\">0.994255</td><td style=\"text-align: right;\">0.0933299</td><td style=\"text-align: right;\">0.991681</td><td style=\"text-align: right;\">             0.0380426</td><td style=\"text-align: right;\">0.162663</td><td style=\"text-align: right;\">0.0264594</td></tr>\n",
              "<tr><td>GBM_4_AutoML_1_20230704_104257    </td><td style=\"text-align: right;\">  0.967033</td><td style=\"text-align: right;\">0.993246</td><td style=\"text-align: right;\">0.0998609</td><td style=\"text-align: right;\">0.990397</td><td style=\"text-align: right;\">             0.0333889</td><td style=\"text-align: right;\">0.170342</td><td style=\"text-align: right;\">0.0290164</td></tr>\n",
              "<tr><td>XGBoost_3_AutoML_1_20230704_104257</td><td style=\"text-align: right;\">  0.969231</td><td style=\"text-align: right;\">0.992999</td><td style=\"text-align: right;\">0.0965668</td><td style=\"text-align: right;\">0.990609</td><td style=\"text-align: right;\">             0.0316284</td><td style=\"text-align: right;\">0.161568</td><td style=\"text-align: right;\">0.0261043</td></tr>\n",
              "<tr><td>GBM_1_AutoML_1_20230704_104257    </td><td style=\"text-align: right;\">  0.971429</td><td style=\"text-align: right;\">0.994461</td><td style=\"text-align: right;\">0.0911605</td><td style=\"text-align: right;\">0.992401</td><td style=\"text-align: right;\">             0.0287044</td><td style=\"text-align: right;\">0.158892</td><td style=\"text-align: right;\">0.0252468</td></tr>\n",
              "<tr><td>GBM_3_AutoML_1_20230704_104257    </td><td style=\"text-align: right;\">  0.971429</td><td style=\"text-align: right;\">0.994461</td><td style=\"text-align: right;\">0.0902413</td><td style=\"text-align: right;\">0.991937</td><td style=\"text-align: right;\">             0.027541 </td><td style=\"text-align: right;\">0.157908</td><td style=\"text-align: right;\">0.0249349</td></tr>\n",
              "<tr><td>GLM_1_AutoML_1_20230704_104257    </td><td style=\"text-align: right;\">  0.978022</td><td style=\"text-align: right;\">0.996479</td><td style=\"text-align: right;\">0.0739483</td><td style=\"text-align: right;\">0.994833</td><td style=\"text-align: right;\">             0.0269129</td><td style=\"text-align: right;\">0.1449  </td><td style=\"text-align: right;\">0.0209961</td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10 rows x 8 columns]</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "best_model"
      ],
      "metadata": {
        "id": "vlH8zwtisQU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "787ed9d3-8769-49a3-8638-d7c91f070d76"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230704_104257\n",
              "\n",
              "\n",
              "Model Summary: \n",
              "    number_of_trees\n",
              "--  -----------------\n",
              "    36\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.029074514855628383\n",
              "RMSE: 0.17051250644931704\n",
              "LogLoss: 0.12394929039565182\n",
              "Mean Per-Class Error: 0.029270653158718393\n",
              "AUC: 0.9940799769376494\n",
              "AUCPR: 0.992056368504806\n",
              "Gini: 0.9881599538752988\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4925362765789032\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  ------------\n",
              "0      279  5    0.0176   (5.0/284.0)\n",
              "1      7    164  0.0409   (7.0/171.0)\n",
              "Total  286  169  0.0264   (12.0/455.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.492536     0.964706  38\n",
              "max f2                       0.317989     0.971098  49\n",
              "max f0point5                 0.514766     0.971395  37\n",
              "max accuracy                 0.514766     0.973626  37\n",
              "max precision                0.968002     1         0\n",
              "max recall                   0.0588847    1         100\n",
              "max specificity              0.968002     1         0\n",
              "max absolute_mcc             0.492536     0.943696  38\n",
              "max min_per_class_accuracy   0.402806     0.964912  42\n",
              "max mean_per_class_accuracy  0.492536     0.970729  38\n",
              "max tns                      0.968002     284       0\n",
              "max fns                      0.968002     110       0\n",
              "max fps                      0.0148769    284       134\n",
              "max tps                      0.0588847    171       100\n",
              "max tnr                      0.968002     1         0\n",
              "max fnr                      0.968002     0.643275  0\n",
              "max fpr                      0.0148769    1         134\n",
              "max tpr                      0.0588847    1         100\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.29 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.134066                    0.968002           2.66082    2.66082            1                0.968002   1                           0.968002            0.356725        0.356725                   166.082   166.082            0.356725\n",
              "2        0.162637                    0.957612           2.66082    2.66082            1                0.957612   1                           0.966177            0.0760234       0.432749                   166.082   166.082            0.432749\n",
              "3        0.2                         0.950501           2.66082    2.66082            1                0.953488   1                           0.963806            0.0994152       0.532164                   166.082   166.082            0.532164\n",
              "4        0.301099                    0.807568           2.66082    2.66082            1                0.897179   1                           0.941435            0.269006        0.80117                    166.082   166.082            0.80117\n",
              "5        0.4                         0.314704           1.83301    2.45614            0.688889         0.582323   0.923077                    0.852644            0.181287        0.982456                   83.3008   145.614            0.93316\n",
              "6        0.501099                    0.076693           0.0578439  1.97227            0.0217391        0.161579   0.741228                    0.713218            0.00584795      0.988304                   -94.2156  97.2274            0.780558\n",
              "7        0.624176                    0.0503081          0.0950292  1.60211            0.0357143        0.058398   0.602113                    0.584099            0.0116959       1                          -90.4971  60.2113            0.602113\n",
              "8        0.698901                    0.0279195          0          1.43082            0                0.0365022  0.537736                    0.525551            0               1                          -100      43.0818            0.482394\n",
              "9        0.804396                    0.0177071          0          1.24317            0                0.0225316  0.467213                    0.459581            0               1                          -100      24.3169            0.31338\n",
              "10       0.931868                    0.0175245          0          1.07311            0                0.0175245  0.403302                    0.399111            0               1                          -100      7.31132            0.109155\n",
              "11       1                           0.0148769          0          1                  0                0.0148769  0.375824                    0.372932            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.03762067586537018\n",
              "RMSE: 0.19396050078655236\n",
              "LogLoss: 0.1512542647982658\n",
              "Mean Per-Class Error: 0.03274306715669896\n",
              "AUC: 0.976445038422987\n",
              "AUCPR: 0.9791961628675718\n",
              "Gini: 0.952890076845974\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25368359684944153\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  -----------\n",
              "0      70   3    0.0411   (3.0/73.0)\n",
              "1      1    40   0.0244   (1.0/41.0)\n",
              "Total  71   43   0.0351   (4.0/114.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.253684     0.952381  20\n",
              "max f2                       0.253684     0.966184  20\n",
              "max f0point5                 0.705651     0.972973  15\n",
              "max accuracy                 0.253684     0.964912  20\n",
              "max precision                0.968002     1         0\n",
              "max recall                   0.0175245    1         46\n",
              "max specificity              0.968002     1         0\n",
              "max absolute_mcc             0.253684     0.925285  20\n",
              "max min_per_class_accuracy   0.253684     0.958904  20\n",
              "max mean_per_class_accuracy  0.253684     0.967257  20\n",
              "max tns                      0.968002     73        0\n",
              "max fns                      0.968002     27        0\n",
              "max fps                      0.0148769    73        47\n",
              "max tps                      0.0175245    41        46\n",
              "max tnr                      0.968002     1         0\n",
              "max fnr                      0.968002     0.658537  0\n",
              "max fpr                      0.0148769    1         47\n",
              "max tpr                      0.0175245    1         46\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 35.96 %, avg score: 34.85 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.122807                    0.968002           2.78049    2.78049            1                0.968002   1                           0.968002            0.341463        0.341463                   178.049   178.049            0.341463\n",
              "2        0.157895                    0.954363           2.78049    2.78049            1                0.955175   1                           0.965152            0.097561        0.439024                   178.049   178.049            0.439024\n",
              "3        0.210526                    0.932284           2.78049    2.78049            1                0.935408   1                           0.957716            0.146341        0.585366                   178.049   178.049            0.585366\n",
              "4        0.298246                    0.717271           2.78049    2.78049            1                0.870593   1                           0.932091            0.243902        0.829268                   178.049   178.049            0.829268\n",
              "5        0.403509                    0.169029           1.39024    2.41782            0.5              0.45112    0.869565                    0.806621            0.146341        0.97561                    39.0244   141.782            0.893418\n",
              "6        0.508772                    0.0588847          0          1.91758            0                0.093715   0.689655                    0.659123            0               0.97561                    -100      91.7578            0.729034\n",
              "7        0.596491                    0.0359404          0          1.63558            0                0.0470055  0.588235                    0.569106            0               0.97561                    -100      63.5581            0.592048\n",
              "8        0.701754                    0.023509           0          1.39024            0                0.0319946  0.5                         0.488539            0               0.97561                    -100      39.0244            0.427665\n",
              "9        0.973684                    0.0175245          0.0896932  1.02703            0.0322581        0.0192596  0.369369                    0.357479            0.0243902       1                          -91.0307  2.7027             0.0410959\n",
              "10       1                           0.0148769          0          1                  0                0.0148769  0.359649                    0.348463            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.044205368519838424\n",
              "RMSE: 0.21025072775103162\n",
              "LogLoss: 0.16637884924436225\n",
              "Mean Per-Class Error: 0.05391854048266205\n",
              "AUC: 0.9841137468083354\n",
              "AUCPR: 0.9796363289807396\n",
              "Gini: 0.9682274936166708\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35231223702430725\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  ------------\n",
              "0      265  19   0.0669   (19.0/284.0)\n",
              "1      7    164  0.0409   (7.0/171.0)\n",
              "Total  272  183  0.0571   (26.0/455.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.352312     0.926554  103\n",
              "max f2                       0.352312     0.94579   103\n",
              "max f0point5                 0.648854     0.953307  71\n",
              "max accuracy                 0.624354     0.942857  74\n",
              "max precision                0.968105     1         0\n",
              "max recall                   0.0317377    1         212\n",
              "max specificity              0.968105     1         0\n",
              "max absolute_mcc             0.352312     0.881234  103\n",
              "max min_per_class_accuracy   0.393351     0.940141  98\n",
              "max mean_per_class_accuracy  0.352312     0.946081  103\n",
              "max tns                      0.968105     284       0\n",
              "max fns                      0.968105     168       0\n",
              "max fps                      0.0161675    284       239\n",
              "max tps                      0.0317377    171       212\n",
              "max tnr                      0.968105     1         0\n",
              "max fnr                      0.968105     0.982456  0\n",
              "max fpr                      0.0161675    1         239\n",
              "max tpr                      0.0317377    1         212\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.58 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.0153846                   0.964642           2.66082    2.66082            1                0.966126   1                           0.966126            0.0409357       0.0409357                  166.082   166.082            0.0409357\n",
              "2        0.032967                    0.964165           2.66082    2.66082            1                0.964165   1                           0.96508             0.0467836       0.0877193                  166.082   166.082            0.0877193\n",
              "3        0.0461538                   0.96322            2.66082    2.66082            1                0.96322    1                           0.964549            0.0350877       0.122807                   166.082   166.082            0.122807\n",
              "4        0.0659341                   0.961647           2.66082    2.66082            1                0.961647   1                           0.963678            0.0526316       0.175439                   166.082   166.082            0.175439\n",
              "5        0.101099                    0.959238           2.66082    2.66082            1                0.960176   1                           0.96246             0.0935673       0.269006                   166.082   166.082            0.269006\n",
              "6        0.151648                    0.95194            2.66082    2.66082            1                0.954832   1                           0.959917            0.134503        0.403509                   166.082   166.082            0.403509\n",
              "7        0.2                         0.934824           2.66082    2.66082            1                0.944351   1                           0.956154            0.128655        0.532164                   166.082   166.082            0.532164\n",
              "8        0.301099                    0.781079           2.54513    2.62197            0.956522         0.887324   0.985401                    0.933043            0.25731         0.789474                   154.513   162.197            0.782431\n",
              "9        0.4                         0.354154           1.65562    2.38304            0.622222         0.566931   0.895604                    0.842521            0.163743        0.953216                   65.5621   138.304            0.886315\n",
              "10       0.501099                    0.0901423          0.289219   1.9606             0.108696         0.19796    0.736842                    0.712478            0.0292398       0.982456                   -71.0781  96.0603            0.771189\n",
              "11       0.6                         0.0496738          0.0591293  1.64717            0.0222222        0.0669556  0.619048                    0.606073            0.00584795      0.988304                   -94.0871  64.7173            0.622107\n",
              "12       0.698901                    0.0362709          0.0591293  1.42245            0.0222222        0.0437602  0.534591                    0.526501            0.00584795      0.994152                   -94.0871  42.245             0.473025\n",
              "13       0.8                         0.0304273          0.0578439  1.25               0.0217391        0.0321457  0.46978                     0.464027            0.00584795      1                          -94.2156  25                 0.320423\n",
              "14       0.907692                    0.0233827          0          1.10169            0                0.0257063  0.414044                    0.412023            0               1                          -100      10.1695            0.147887\n",
              "15       1                           0.0161675          0          1                  0                0.0195924  0.375824                    0.375799            0               1                          -100      0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                         mean       sd         cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid    cv_6_valid    cv_7_valid    cv_8_valid    cv_9_valid    cv_10_valid\n",
              "-----------------------  ---------  ---------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  -------------\n",
              "accuracy                 0.96715    0.0235589  0.934783      0.978261      0.978261      0.956522      0.934783      0.955556      1             0.955556      1             0.977778\n",
              "auc                      0.983879   0.0178793  0.960938      0.995833      0.997972      0.984615      0.975446      0.980159      1             0.948276      1             0.995556\n",
              "err                      0.0328502  0.0235589  0.0652174     0.0217391     0.0217391     0.0434783     0.0652174     0.0444444     0             0.0444444     0             0.0222222\n",
              "err_count                1.5        1.08012    3             1             1             2             3             2             0             2             0             1\n",
              "f0point5                 0.958851   0.0287485  0.948276      0.986842      0.955056      0.95          0.909091      0.952381      1             0.9375        1             0.949367\n",
              "f1                       0.951568   0.0406347  0.88          0.967742      0.971429      0.95          0.888889      0.952381      1             0.9375        1             0.967742\n",
              "f2                       0.945492   0.0586199  0.820896      0.949367      0.988372      0.95          0.869565      0.952381      1             0.9375        1             0.986842\n",
              "lift_top_group           2.7497     0.495565   3.28571       2.875         2.70588       2.3           3.28571       2.14286       1.875         2.8125        3.21429       3\n",
              "logloss                  0.166222   0.0649039  0.212998      0.147144      0.106434      0.204719      0.231138      0.199252      0.0746679     0.255789      0.0740798     0.156001\n",
              "max_per_class_error      0.0647578  0.0660564  0.214286      0.0625        0.0344828     0.05          0.142857      0.0476191     0             0.0625        0             0.0333333\n",
              "mcc                      0.927621   0.0547608  0.847566      0.952501      0.954923      0.911539      0.843984      0.910714      1             0.903017      1             0.951972\n",
              "mean_per_class_accuracy  0.960328   0.035232   0.892857      0.96875       0.982759      0.955769      0.912946      0.955357      1             0.951509      1             0.983333\n",
              "mean_per_class_error     0.0396719  0.035232   0.107143      0.03125       0.0172414     0.0442308     0.0870536     0.0446429     0             0.0484914     0             0.0166667\n",
              "mse                      0.0441377  0.0226749  0.0620417     0.0377012     0.0215032     0.0596129     0.0706184     0.0543299     0.0114468     0.0693428     0.0124989     0.0422811\n",
              "pr_auc                   0.977882   0.023669   0.938531      0.992855      0.996638      0.980331      0.952928      0.982007      1             0.944435      1             0.991098\n",
              "precision                0.96449    0.0315774  1             1             0.944444      0.95          0.923077      0.952381      1             0.9375        1             0.9375\n",
              "r2                       0.805685   0.102998   0.706964      0.833801      0.907706      0.757421      0.666454      0.78171       0.954008      0.697372      0.941681      0.809735\n",
              "recall                   0.942024   0.0709415  0.785714      0.9375        1             0.95          0.857143      0.952381      1             0.9375        1             1\n",
              "rmse                     0.202062   0.0606329  0.249082      0.194168      0.14664       0.244158      0.265741      0.233088      0.10699       0.26333       0.111799      0.205624\n",
              "specificity              0.978632   0.0186053  1             1             0.965517      0.961538      0.96875       0.958333      1             0.965517      1             0.966667\n",
              "\n",
              "Scoring History: \n",
              "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
              "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
              "    2023-07-04 10:43:03  5.449 sec   0                  0.5              0.693147            0.5             0.375824           1                0.624176                         0.5                0.693147              0.5               0.359649             1                  0.640351\n",
              "    2023-07-04 10:43:03  5.492 sec   5                  0.225101         0.220985            0.991465        0.987958           2.66082          0.0307692                        0.225428           0.222521              0.976779          0.978796             2.78049            0.0438596\n",
              "    2023-07-04 10:43:03  5.526 sec   10                 0.182175         0.144104            0.993503        0.991125           2.66082          0.0307692                        0.194942           0.160143              0.97728           0.979975             2.78049            0.0350877\n",
              "    2023-07-04 10:43:03  5.597 sec   15                 0.172352         0.130611            0.994533        0.992725           2.66082          0.0241758                        0.191048           0.152637              0.977113          0.980463             2.78049            0.0350877\n",
              "    2023-07-04 10:43:03  5.661 sec   20                 0.170296         0.1239              0.99408         0.992056           2.66082          0.0263736                        0.193492           0.150659              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-07-04 10:43:03  5.721 sec   25                 0.170296         0.1239              0.99408         0.992056           2.66082          0.0263736                        0.193495           0.150663              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-07-04 10:43:03  5.771 sec   30                 0.170295         0.123902            0.99408         0.992056           2.66082          0.0263736                        0.193487           0.150653              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-07-04 10:43:03  5.857 sec   35                 0.17029          0.123951            0.99408         0.992056           2.66082          0.0263736                        0.193367           0.150506              0.976445          0.979196             2.78049            0.0350877\n",
              "    2023-07-04 10:43:04  5.916 sec   36                 0.170513         0.123949            0.99408         0.992056           2.66082          0.0263736                        0.193961           0.151254              0.976445          0.979196             2.78049            0.0350877\n",
              "\n",
              "Variable Importances: \n",
              "variable              relative_importance    scaled_importance    percentage\n",
              "--------------------  ---------------------  -------------------  ------------\n",
              "concave points_worst  204.282                1                    0.341069\n",
              "area_worst            111.787                0.547218             0.186639\n",
              "concave points_mean   80.5573                0.394343             0.134498\n",
              "radius_worst          72.7003                0.355881             0.12138\n",
              "perimeter_worst       63.8343                0.312481             0.106577\n",
              "texture_worst         22.0758                0.108065             0.0368576\n",
              "concavity_worst       20.4532                0.100122             0.0341486\n",
              "texture_mean          10.5059                0.0514284            0.0175406\n",
              "radius_mean           8.47355                0.0414796            0.0141474\n",
              "concavity_mean        3.36487                0.0164716            0.00561796\n",
              "compactness_se        0.913376               0.00447114           0.00152497\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ],
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OXGBoostEstimator : XGBoost\n",
              "Model Key: XGBoost_1_AutoML_1_20230704_104257\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-2.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-2 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-2 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-2 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table th,\n",
              "#h2o-table-2 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>number_of_trees</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>36.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.029074514855628383\n",
              "RMSE: 0.17051250644931704\n",
              "LogLoss: 0.12394929039565182\n",
              "Mean Per-Class Error: 0.029270653158718393\n",
              "AUC: 0.9940799769376494\n",
              "AUCPR: 0.992056368504806\n",
              "Gini: 0.9881599538752988</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-3.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-3 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-3 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-3 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table th,\n",
              "#h2o-table-3 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4925362765789032</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>279.0</td>\n",
              "<td>5.0</td>\n",
              "<td>0.0176</td>\n",
              "<td> (5.0/284.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>7.0</td>\n",
              "<td>164.0</td>\n",
              "<td>0.0409</td>\n",
              "<td> (7.0/171.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>286.0</td>\n",
              "<td>169.0</td>\n",
              "<td>0.0264</td>\n",
              "<td> (12.0/455.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-4.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-4 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-4 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-4 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table th,\n",
              "#h2o-table-4 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-4 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-4\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9647059</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.3179893</td>\n",
              "<td>0.9710983</td>\n",
              "<td>49.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.5147656</td>\n",
              "<td>0.9713945</td>\n",
              "<td>37.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.5147656</td>\n",
              "<td>0.9736264</td>\n",
              "<td>37.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0588847</td>\n",
              "<td>1.0</td>\n",
              "<td>100.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9436960</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.4028056</td>\n",
              "<td>0.9649123</td>\n",
              "<td>42.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.4925363</td>\n",
              "<td>0.9707293</td>\n",
              "<td>38.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>284.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>110.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0148769</td>\n",
              "<td>284.0</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0588847</td>\n",
              "<td>171.0</td>\n",
              "<td>100.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.6432749</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0148769</td>\n",
              "<td>1.0</td>\n",
              "<td>134.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0588847</td>\n",
              "<td>1.0</td>\n",
              "<td>100.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-5.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-5 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-5 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-5 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table th,\n",
              "#h2o-table-5 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.29 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.1340659</td>\n",
              "<td>0.9680020</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.3567251</td>\n",
              "<td>0.3567251</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.3567251</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.1626374</td>\n",
              "<td>0.9576122</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9576122</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661767</td>\n",
              "<td>0.0760234</td>\n",
              "<td>0.4327485</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.4327485</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.2</td>\n",
              "<td>0.9505008</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9534877</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9638063</td>\n",
              "<td>0.0994152</td>\n",
              "<td>0.5321637</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.5321637</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.3010989</td>\n",
              "<td>0.8075677</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.8971791</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9414351</td>\n",
              "<td>0.2690058</td>\n",
              "<td>0.8011696</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.8011696</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.4</td>\n",
              "<td>0.3147044</td>\n",
              "<td>1.8330084</td>\n",
              "<td>2.4561404</td>\n",
              "<td>0.6888889</td>\n",
              "<td>0.5823229</td>\n",
              "<td>0.9230769</td>\n",
              "<td>0.8526436</td>\n",
              "<td>0.1812865</td>\n",
              "<td>0.9824561</td>\n",
              "<td>83.3008447</td>\n",
              "<td>145.6140351</td>\n",
              "<td>0.9331604</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.5010989</td>\n",
              "<td>0.0766930</td>\n",
              "<td>0.0578439</td>\n",
              "<td>1.9722735</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.1615786</td>\n",
              "<td>0.7412281</td>\n",
              "<td>0.7132182</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9883041</td>\n",
              "<td>-94.2156115</td>\n",
              "<td>97.2273520</td>\n",
              "<td>0.7805576</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.6241758</td>\n",
              "<td>0.0503081</td>\n",
              "<td>0.0950292</td>\n",
              "<td>1.6021127</td>\n",
              "<td>0.0357143</td>\n",
              "<td>0.0583980</td>\n",
              "<td>0.6021127</td>\n",
              "<td>0.5840988</td>\n",
              "<td>0.0116959</td>\n",
              "<td>1.0</td>\n",
              "<td>-90.4970760</td>\n",
              "<td>60.2112676</td>\n",
              "<td>0.6021127</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.6989011</td>\n",
              "<td>0.0279195</td>\n",
              "<td>0.0</td>\n",
              "<td>1.4308176</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0365022</td>\n",
              "<td>0.5377358</td>\n",
              "<td>0.5255507</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>43.0817610</td>\n",
              "<td>0.4823944</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.8043956</td>\n",
              "<td>0.0177071</td>\n",
              "<td>0.0</td>\n",
              "<td>1.2431694</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0225316</td>\n",
              "<td>0.4672131</td>\n",
              "<td>0.4595810</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>24.3169399</td>\n",
              "<td>0.3133803</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.9318681</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0731132</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.4033019</td>\n",
              "<td>0.3991110</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>7.3113208</td>\n",
              "<td>0.1091549</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.3758242</td>\n",
              "<td>0.3729324</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on validation data. **\n",
              "\n",
              "MSE: 0.03762067586537018\n",
              "RMSE: 0.19396050078655236\n",
              "LogLoss: 0.1512542647982658\n",
              "Mean Per-Class Error: 0.03274306715669896\n",
              "AUC: 0.976445038422987\n",
              "AUCPR: 0.9791961628675718\n",
              "Gini: 0.952890076845974</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-6.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-6 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-6 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-6 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table th,\n",
              "#h2o-table-6 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25368359684944153</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>70.0</td>\n",
              "<td>3.0</td>\n",
              "<td>0.0411</td>\n",
              "<td> (3.0/73.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>1.0</td>\n",
              "<td>40.0</td>\n",
              "<td>0.0244</td>\n",
              "<td> (1.0/41.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>71.0</td>\n",
              "<td>43.0</td>\n",
              "<td>0.0351</td>\n",
              "<td> (4.0/114.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-7.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-7 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-7 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-7 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table th,\n",
              "#h2o-table-7 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-7 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-7\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9523810</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9661836</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.7056506</td>\n",
              "<td>0.9729730</td>\n",
              "<td>15.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9649123</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0175245</td>\n",
              "<td>1.0</td>\n",
              "<td>46.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9252854</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9589041</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.2536836</td>\n",
              "<td>0.9672569</td>\n",
              "<td>20.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>73.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9680020</td>\n",
              "<td>27.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0148769</td>\n",
              "<td>73.0</td>\n",
              "<td>47.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0175245</td>\n",
              "<td>41.0</td>\n",
              "<td>46.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.6585366</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0148769</td>\n",
              "<td>1.0</td>\n",
              "<td>47.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0175245</td>\n",
              "<td>1.0</td>\n",
              "<td>46.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-8.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-8 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-8 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-8 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table th,\n",
              "#h2o-table-8 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-8 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-8\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 35.96 %, avg score: 34.85 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.1228070</td>\n",
              "<td>0.9680020</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9680020</td>\n",
              "<td>0.3414634</td>\n",
              "<td>0.3414634</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.3414634</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.1578947</td>\n",
              "<td>0.9543629</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9551752</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9651516</td>\n",
              "<td>0.0975610</td>\n",
              "<td>0.4390244</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.4390244</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.2105263</td>\n",
              "<td>0.9322835</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9354083</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9577157</td>\n",
              "<td>0.1463415</td>\n",
              "<td>0.5853659</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.5853659</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.2982456</td>\n",
              "<td>0.7172713</td>\n",
              "<td>2.7804878</td>\n",
              "<td>2.7804878</td>\n",
              "<td>1.0</td>\n",
              "<td>0.8705932</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9320915</td>\n",
              "<td>0.2439024</td>\n",
              "<td>0.8292683</td>\n",
              "<td>178.0487805</td>\n",
              "<td>178.0487805</td>\n",
              "<td>0.8292683</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.4035088</td>\n",
              "<td>0.1690287</td>\n",
              "<td>1.3902439</td>\n",
              "<td>2.4178155</td>\n",
              "<td>0.5</td>\n",
              "<td>0.4511201</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.8066207</td>\n",
              "<td>0.1463415</td>\n",
              "<td>0.9756098</td>\n",
              "<td>39.0243902</td>\n",
              "<td>141.7815483</td>\n",
              "<td>0.8934180</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.5087719</td>\n",
              "<td>0.0588847</td>\n",
              "<td>0.0</td>\n",
              "<td>1.9175778</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0937150</td>\n",
              "<td>0.6896552</td>\n",
              "<td>0.6591230</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>91.7577796</td>\n",
              "<td>0.7290344</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.5964912</td>\n",
              "<td>0.0359404</td>\n",
              "<td>0.0</td>\n",
              "<td>1.6355811</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0470055</td>\n",
              "<td>0.5882353</td>\n",
              "<td>0.5691057</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>63.5581062</td>\n",
              "<td>0.5920481</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.7017544</td>\n",
              "<td>0.0235090</td>\n",
              "<td>0.0</td>\n",
              "<td>1.3902439</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0319946</td>\n",
              "<td>0.5</td>\n",
              "<td>0.4885390</td>\n",
              "<td>0.0</td>\n",
              "<td>0.9756098</td>\n",
              "<td>-100.0</td>\n",
              "<td>39.0243902</td>\n",
              "<td>0.4276646</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.9736842</td>\n",
              "<td>0.0175245</td>\n",
              "<td>0.0896932</td>\n",
              "<td>1.0270270</td>\n",
              "<td>0.0322581</td>\n",
              "<td>0.0192596</td>\n",
              "<td>0.3693694</td>\n",
              "<td>0.3574790</td>\n",
              "<td>0.0243902</td>\n",
              "<td>1.0</td>\n",
              "<td>-91.0306845</td>\n",
              "<td>2.7027027</td>\n",
              "<td>0.0410959</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0148769</td>\n",
              "<td>0.3596491</td>\n",
              "<td>0.3484632</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: xgboost\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.044205368519838424\n",
              "RMSE: 0.21025072775103162\n",
              "LogLoss: 0.16637884924436225\n",
              "Mean Per-Class Error: 0.05391854048266205\n",
              "AUC: 0.9841137468083354\n",
              "AUCPR: 0.9796363289807396\n",
              "Gini: 0.9682274936166708</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-9.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-9 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-9 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-9 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table th,\n",
              "#h2o-table-9 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-9 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-9\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35231223702430725</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>265.0</td>\n",
              "<td>19.0</td>\n",
              "<td>0.0669</td>\n",
              "<td> (19.0/284.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>7.0</td>\n",
              "<td>164.0</td>\n",
              "<td>0.0409</td>\n",
              "<td> (7.0/171.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>272.0</td>\n",
              "<td>183.0</td>\n",
              "<td>0.0571</td>\n",
              "<td> (26.0/455.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-10.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-10 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-10 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-10 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-10 .h2o-table th,\n",
              "#h2o-table-10 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-10 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-10\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9265537</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9457901</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6488545</td>\n",
              "<td>0.9533074</td>\n",
              "<td>71.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.6243538</td>\n",
              "<td>0.9428571</td>\n",
              "<td>74.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0317377</td>\n",
              "<td>1.0</td>\n",
              "<td>212.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.8812343</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3933513</td>\n",
              "<td>0.9401408</td>\n",
              "<td>98.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3523122</td>\n",
              "<td>0.9460815</td>\n",
              "<td>103.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9681047</td>\n",
              "<td>284.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9681047</td>\n",
              "<td>168.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0161675</td>\n",
              "<td>284.0</td>\n",
              "<td>239.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0317377</td>\n",
              "<td>171.0</td>\n",
              "<td>212.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9681047</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9681047</td>\n",
              "<td>0.9824561</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0161675</td>\n",
              "<td>1.0</td>\n",
              "<td>239.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0317377</td>\n",
              "<td>1.0</td>\n",
              "<td>212.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-11.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-11 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-11 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-11 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-11 .h2o-table th,\n",
              "#h2o-table-11 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-11 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-11\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 37.58 %, avg score: 37.58 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.0153846</td>\n",
              "<td>0.9646416</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661258</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9661258</td>\n",
              "<td>0.0409357</td>\n",
              "<td>0.0409357</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.0409357</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.0329670</td>\n",
              "<td>0.9641652</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9641652</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9650801</td>\n",
              "<td>0.0467836</td>\n",
              "<td>0.0877193</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.0877193</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.0461538</td>\n",
              "<td>0.9632205</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9632205</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9645488</td>\n",
              "<td>0.0350877</td>\n",
              "<td>0.1228070</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.1228070</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.0659341</td>\n",
              "<td>0.9616467</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9616467</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9636782</td>\n",
              "<td>0.0526316</td>\n",
              "<td>0.1754386</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.1754386</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.1010989</td>\n",
              "<td>0.9592377</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9601760</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9624600</td>\n",
              "<td>0.0935673</td>\n",
              "<td>0.2690058</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.2690058</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1516484</td>\n",
              "<td>0.9519403</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9548322</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9599174</td>\n",
              "<td>0.1345029</td>\n",
              "<td>0.4035088</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.4035088</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.2</td>\n",
              "<td>0.9348235</td>\n",
              "<td>2.6608187</td>\n",
              "<td>2.6608187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9443512</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9561542</td>\n",
              "<td>0.1286550</td>\n",
              "<td>0.5321637</td>\n",
              "<td>166.0818713</td>\n",
              "<td>166.0818713</td>\n",
              "<td>0.5321637</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.3010989</td>\n",
              "<td>0.7810790</td>\n",
              "<td>2.5451309</td>\n",
              "<td>2.6219746</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.8873240</td>\n",
              "<td>0.9854015</td>\n",
              "<td>0.9330433</td>\n",
              "<td>0.2573099</td>\n",
              "<td>0.7894737</td>\n",
              "<td>154.5130943</td>\n",
              "<td>162.1974645</td>\n",
              "<td>0.7824314</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.4</td>\n",
              "<td>0.3541536</td>\n",
              "<td>1.6556205</td>\n",
              "<td>2.3830409</td>\n",
              "<td>0.6222222</td>\n",
              "<td>0.5669310</td>\n",
              "<td>0.8956044</td>\n",
              "<td>0.8425210</td>\n",
              "<td>0.1637427</td>\n",
              "<td>0.9532164</td>\n",
              "<td>65.5620533</td>\n",
              "<td>138.3040936</td>\n",
              "<td>0.8863150</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.5010989</td>\n",
              "<td>0.0901423</td>\n",
              "<td>0.2892194</td>\n",
              "<td>1.9606033</td>\n",
              "<td>0.1086957</td>\n",
              "<td>0.1979604</td>\n",
              "<td>0.7368421</td>\n",
              "<td>0.7124781</td>\n",
              "<td>0.0292398</td>\n",
              "<td>0.9824561</td>\n",
              "<td>-71.0780575</td>\n",
              "<td>96.0603263</td>\n",
              "<td>0.7711885</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.6</td>\n",
              "<td>0.0496738</td>\n",
              "<td>0.0591293</td>\n",
              "<td>1.6471735</td>\n",
              "<td>0.0222222</td>\n",
              "<td>0.0669556</td>\n",
              "<td>0.6190476</td>\n",
              "<td>0.6060733</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9883041</td>\n",
              "<td>-94.0870695</td>\n",
              "<td>64.7173489</td>\n",
              "<td>0.6221069</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6989011</td>\n",
              "<td>0.0362709</td>\n",
              "<td>0.0591293</td>\n",
              "<td>1.4224503</td>\n",
              "<td>0.0222222</td>\n",
              "<td>0.0437602</td>\n",
              "<td>0.5345912</td>\n",
              "<td>0.5265007</td>\n",
              "<td>0.0058480</td>\n",
              "<td>0.9941520</td>\n",
              "<td>-94.0870695</td>\n",
              "<td>42.2450256</td>\n",
              "<td>0.4730253</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.8</td>\n",
              "<td>0.0304273</td>\n",
              "<td>0.0578439</td>\n",
              "<td>1.25</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0321457</td>\n",
              "<td>0.4697802</td>\n",
              "<td>0.4640272</td>\n",
              "<td>0.0058480</td>\n",
              "<td>1.0</td>\n",
              "<td>-94.2156115</td>\n",
              "<td>25.0</td>\n",
              "<td>0.3204225</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.9076923</td>\n",
              "<td>0.0233827</td>\n",
              "<td>0.0</td>\n",
              "<td>1.1016949</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0257063</td>\n",
              "<td>0.4140436</td>\n",
              "<td>0.4120231</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>10.1694915</td>\n",
              "<td>0.1478873</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0161675</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0195924</td>\n",
              "<td>0.3758242</td>\n",
              "<td>0.3757987</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-12.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-12 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-12 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-12 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-12 .h2o-table th,\n",
              "#h2o-table-12 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-12 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-12\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th>\n",
              "<th>cv_6_valid</th>\n",
              "<th>cv_7_valid</th>\n",
              "<th>cv_8_valid</th>\n",
              "<th>cv_9_valid</th>\n",
              "<th>cv_10_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.9671497</td>\n",
              "<td>0.0235589</td>\n",
              "<td>0.9347826</td>\n",
              "<td>0.9782609</td>\n",
              "<td>0.9782609</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.9347826</td>\n",
              "<td>0.9555556</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9555556</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9777778</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.9838794</td>\n",
              "<td>0.0178793</td>\n",
              "<td>0.9609375</td>\n",
              "<td>0.9958333</td>\n",
              "<td>0.9979716</td>\n",
              "<td>0.9846154</td>\n",
              "<td>0.9754464</td>\n",
              "<td>0.9801587</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9482759</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9955556</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.0328502</td>\n",
              "<td>0.0235589</td>\n",
              "<td>0.0652174</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0217391</td>\n",
              "<td>0.0434783</td>\n",
              "<td>0.0652174</td>\n",
              "<td>0.0444444</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0444444</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0222222</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>1.5</td>\n",
              "<td>1.0801234</td>\n",
              "<td>3.0</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>2.0</td>\n",
              "<td>3.0</td>\n",
              "<td>2.0</td>\n",
              "<td>0.0</td>\n",
              "<td>2.0</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.9588513</td>\n",
              "<td>0.0287485</td>\n",
              "<td>0.9482759</td>\n",
              "<td>0.9868421</td>\n",
              "<td>0.9550562</td>\n",
              "<td>0.95</td>\n",
              "<td>0.9090909</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9493671</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.9515683</td>\n",
              "<td>0.0406347</td>\n",
              "<td>0.88</td>\n",
              "<td>0.9677419</td>\n",
              "<td>0.9714286</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8888889</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9677419</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.9454923</td>\n",
              "<td>0.0586199</td>\n",
              "<td>0.8208955</td>\n",
              "<td>0.9493671</td>\n",
              "<td>0.9883721</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9868421</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.7496953</td>\n",
              "<td>0.4955653</td>\n",
              "<td>3.2857144</td>\n",
              "<td>2.875</td>\n",
              "<td>2.7058823</td>\n",
              "<td>2.3</td>\n",
              "<td>3.2857144</td>\n",
              "<td>2.142857</td>\n",
              "<td>1.875</td>\n",
              "<td>2.8125</td>\n",
              "<td>3.2142856</td>\n",
              "<td>3.0</td></tr>\n",
              "<tr><td>logloss</td>\n",
              "<td>0.1662221</td>\n",
              "<td>0.0649039</td>\n",
              "<td>0.2129980</td>\n",
              "<td>0.1471438</td>\n",
              "<td>0.1064336</td>\n",
              "<td>0.2047186</td>\n",
              "<td>0.2311378</td>\n",
              "<td>0.199252</td>\n",
              "<td>0.0746679</td>\n",
              "<td>0.2557887</td>\n",
              "<td>0.0740798</td>\n",
              "<td>0.1560008</td></tr>\n",
              "<tr><td>max_per_class_error</td>\n",
              "<td>0.0647578</td>\n",
              "<td>0.0660564</td>\n",
              "<td>0.2142857</td>\n",
              "<td>0.0625</td>\n",
              "<td>0.0344828</td>\n",
              "<td>0.05</td>\n",
              "<td>0.1428571</td>\n",
              "<td>0.0476191</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0625</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0333333</td></tr>\n",
              "<tr><td>mcc</td>\n",
              "<td>0.9276215</td>\n",
              "<td>0.0547608</td>\n",
              "<td>0.8475655</td>\n",
              "<td>0.9525009</td>\n",
              "<td>0.9549227</td>\n",
              "<td>0.9115385</td>\n",
              "<td>0.8439837</td>\n",
              "<td>0.9107143</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9030172</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9519716</td></tr>\n",
              "<tr><td>mean_per_class_accuracy</td>\n",
              "<td>0.9603280</td>\n",
              "<td>0.0352320</td>\n",
              "<td>0.8928571</td>\n",
              "<td>0.96875</td>\n",
              "<td>0.9827586</td>\n",
              "<td>0.9557692</td>\n",
              "<td>0.9129464</td>\n",
              "<td>0.9553571</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9515086</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9833333</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.0396719</td>\n",
              "<td>0.0352320</td>\n",
              "<td>0.1071429</td>\n",
              "<td>0.03125</td>\n",
              "<td>0.0172414</td>\n",
              "<td>0.0442308</td>\n",
              "<td>0.0870536</td>\n",
              "<td>0.0446429</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0484914</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0166667</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.0441377</td>\n",
              "<td>0.0226749</td>\n",
              "<td>0.0620417</td>\n",
              "<td>0.0377012</td>\n",
              "<td>0.0215032</td>\n",
              "<td>0.0596129</td>\n",
              "<td>0.0706184</td>\n",
              "<td>0.0543299</td>\n",
              "<td>0.0114468</td>\n",
              "<td>0.0693428</td>\n",
              "<td>0.0124989</td>\n",
              "<td>0.0422811</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.9778822</td>\n",
              "<td>0.0236690</td>\n",
              "<td>0.9385305</td>\n",
              "<td>0.9928552</td>\n",
              "<td>0.9966378</td>\n",
              "<td>0.9803309</td>\n",
              "<td>0.9529279</td>\n",
              "<td>0.9820066</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9444352</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9910979</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.9644902</td>\n",
              "<td>0.0315774</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9444444</td>\n",
              "<td>0.95</td>\n",
              "<td>0.9230769</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.8056853</td>\n",
              "<td>0.1029978</td>\n",
              "<td>0.7069637</td>\n",
              "<td>0.8338005</td>\n",
              "<td>0.9077061</td>\n",
              "<td>0.7574214</td>\n",
              "<td>0.6664543</td>\n",
              "<td>0.7817103</td>\n",
              "<td>0.9540083</td>\n",
              "<td>0.6973723</td>\n",
              "<td>0.9416813</td>\n",
              "<td>0.8097349</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.9420238</td>\n",
              "<td>0.0709415</td>\n",
              "<td>0.7857143</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>0.95</td>\n",
              "<td>0.8571429</td>\n",
              "<td>0.9523810</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9375</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.2020618</td>\n",
              "<td>0.0606329</td>\n",
              "<td>0.2490817</td>\n",
              "<td>0.1941680</td>\n",
              "<td>0.1466398</td>\n",
              "<td>0.2441575</td>\n",
              "<td>0.2657412</td>\n",
              "<td>0.2330877</td>\n",
              "<td>0.1069898</td>\n",
              "<td>0.2633303</td>\n",
              "<td>0.1117986</td>\n",
              "<td>0.2056237</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.9786323</td>\n",
              "<td>0.0186053</td>\n",
              "<td>1.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9655172</td>\n",
              "<td>0.9615384</td>\n",
              "<td>0.96875</td>\n",
              "<td>0.9583333</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9655172</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9666666</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-13.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-13 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-13 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-13 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table th,\n",
              "#h2o-table-13 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-13 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-13\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Scoring History: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>timestamp</th>\n",
              "<th>duration</th>\n",
              "<th>number_of_trees</th>\n",
              "<th>training_rmse</th>\n",
              "<th>training_logloss</th>\n",
              "<th>training_auc</th>\n",
              "<th>training_pr_auc</th>\n",
              "<th>training_lift</th>\n",
              "<th>training_classification_error</th>\n",
              "<th>validation_rmse</th>\n",
              "<th>validation_logloss</th>\n",
              "<th>validation_auc</th>\n",
              "<th>validation_pr_auc</th>\n",
              "<th>validation_lift</th>\n",
              "<th>validation_classification_error</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>2023-07-04 10:43:03</td>\n",
              "<td> 5.449 sec</td>\n",
              "<td>0.0</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3758242</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6241758</td>\n",
              "<td>0.5</td>\n",
              "<td>0.6931472</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3596491</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6403509</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:43:03</td>\n",
              "<td> 5.492 sec</td>\n",
              "<td>5.0</td>\n",
              "<td>0.2251009</td>\n",
              "<td>0.2209849</td>\n",
              "<td>0.9914649</td>\n",
              "<td>0.9879580</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0307692</td>\n",
              "<td>0.2254283</td>\n",
              "<td>0.2225214</td>\n",
              "<td>0.9767792</td>\n",
              "<td>0.9787964</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0438596</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:43:03</td>\n",
              "<td> 5.526 sec</td>\n",
              "<td>10.0</td>\n",
              "<td>0.1821748</td>\n",
              "<td>0.1441039</td>\n",
              "<td>0.9935034</td>\n",
              "<td>0.9911247</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0307692</td>\n",
              "<td>0.1949417</td>\n",
              "<td>0.1601430</td>\n",
              "<td>0.9772803</td>\n",
              "<td>0.9799751</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:43:03</td>\n",
              "<td> 5.597 sec</td>\n",
              "<td>15.0</td>\n",
              "<td>0.1723523</td>\n",
              "<td>0.1306105</td>\n",
              "<td>0.9945330</td>\n",
              "<td>0.9927252</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0241758</td>\n",
              "<td>0.1910482</td>\n",
              "<td>0.1526369</td>\n",
              "<td>0.9771133</td>\n",
              "<td>0.9804633</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:43:03</td>\n",
              "<td> 5.661 sec</td>\n",
              "<td>20.0</td>\n",
              "<td>0.1702960</td>\n",
              "<td>0.1239005</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934925</td>\n",
              "<td>0.1506594</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:43:03</td>\n",
              "<td> 5.721 sec</td>\n",
              "<td>25.0</td>\n",
              "<td>0.1702965</td>\n",
              "<td>0.1238999</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934951</td>\n",
              "<td>0.1506626</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:43:03</td>\n",
              "<td> 5.771 sec</td>\n",
              "<td>30.0</td>\n",
              "<td>0.1702950</td>\n",
              "<td>0.1239018</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1934870</td>\n",
              "<td>0.1506526</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:43:03</td>\n",
              "<td> 5.857 sec</td>\n",
              "<td>35.0</td>\n",
              "<td>0.1702896</td>\n",
              "<td>0.1239514</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1933670</td>\n",
              "<td>0.1505057</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2023-07-04 10:43:04</td>\n",
              "<td> 5.916 sec</td>\n",
              "<td>36.0</td>\n",
              "<td>0.1705125</td>\n",
              "<td>0.1239493</td>\n",
              "<td>0.9940800</td>\n",
              "<td>0.9920564</td>\n",
              "<td>2.6608187</td>\n",
              "<td>0.0263736</td>\n",
              "<td>0.1939605</td>\n",
              "<td>0.1512543</td>\n",
              "<td>0.9764450</td>\n",
              "<td>0.9791962</td>\n",
              "<td>2.7804878</td>\n",
              "<td>0.0350877</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-14.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-14 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-14 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-14 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table th,\n",
              "#h2o-table-14 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-14 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-14\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Variable Importances: </caption>\n",
              "    <thead><tr><th>variable</th>\n",
              "<th>relative_importance</th>\n",
              "<th>scaled_importance</th>\n",
              "<th>percentage</th></tr></thead>\n",
              "    <tbody><tr><td>concave points_worst</td>\n",
              "<td>204.2824097</td>\n",
              "<td>1.0</td>\n",
              "<td>0.3410687</td></tr>\n",
              "<tr><td>area_worst</td>\n",
              "<td>111.7869415</td>\n",
              "<td>0.5472177</td>\n",
              "<td>0.1866388</td></tr>\n",
              "<tr><td>concave points_mean</td>\n",
              "<td>80.5573425</td>\n",
              "<td>0.3943430</td>\n",
              "<td>0.1344981</td></tr>\n",
              "<tr><td>radius_worst</td>\n",
              "<td>72.7002869</td>\n",
              "<td>0.3558813</td>\n",
              "<td>0.1213800</td></tr>\n",
              "<tr><td>perimeter_worst</td>\n",
              "<td>63.8343277</td>\n",
              "<td>0.3124808</td>\n",
              "<td>0.1065774</td></tr>\n",
              "<tr><td>texture_worst</td>\n",
              "<td>22.0757656</td>\n",
              "<td>0.1080649</td>\n",
              "<td>0.0368576</td></tr>\n",
              "<tr><td>concavity_worst</td>\n",
              "<td>20.4532471</td>\n",
              "<td>0.1001224</td>\n",
              "<td>0.0341486</td></tr>\n",
              "<tr><td>texture_mean</td>\n",
              "<td>10.5059090</td>\n",
              "<td>0.0514284</td>\n",
              "<td>0.0175406</td></tr>\n",
              "<tr><td>radius_mean</td>\n",
              "<td>8.4735489</td>\n",
              "<td>0.0414796</td>\n",
              "<td>0.0141474</td></tr>\n",
              "<tr><td>concavity_mean</td>\n",
              "<td>3.3648682</td>\n",
              "<td>0.0164716</td>\n",
              "<td>0.0056180</td></tr>\n",
              "<tr><td>compactness_se</td>\n",
              "<td>0.9133759</td>\n",
              "<td>0.0044711</td>\n",
              "<td>0.0015250</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.model_performance(train).accuracy()"
      ],
      "metadata": {
        "id": "G_PyYFNMs97_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c1eaa7-6bd6-482a-a4a7-3ee366ec3de2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.5147656202316284, 0.9736263736263736]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.get_best_model()\n",
        "HATr  = best_model.model_performance(train)\n",
        "HATe  = best_model.model_performance(valid)"
      ],
      "metadata": {
        "id": "jKquKjVVJBL9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_h2o = pd.DataFrame(h2o.as_list(best_model.predict(valid)))['predict']\n",
        "y_test_h2o = np.array(valid1['diagnosis']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-caBWGPlp8P",
        "outputId": "56dd3942-3bfa-46a1-c745-b2c3741753d8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SFOLD DATA AUTOML\n",
        "#strain, svalid = shdf.split_frame(ratios=[.8], seed=123)\n",
        "shdf  = newdata.copy()\n",
        "#shdf['y_test'] = shdf['y_test'].replace(0,\"B\")\n",
        "#shdf['y_test'] = shdf['y_test'].replace(1,\"M\")\n",
        "shy = \"y_test\"\n",
        "shx = list(shdf.columns)\n",
        "shx.remove(shy)\n",
        "\n",
        "shdf.iloc[:,0:6] = StandardScaler().fit_transform(shdf.iloc[:,0:6])\n",
        "#shdf.iloc[:,-1] = LabelEncoder().fit_transform(shdf.iloc[:,-1])\n",
        "strain1, svalid1 = train_test_split(shdf, test_size=0.2,random_state=123)\n",
        "strain = h2o.H2OFrame(strain1)\n",
        "svalid = h2o.H2OFrame(svalid1)\n",
        "strain[\"y_test\"] = strain[\"y_test\"].asfactor()\n",
        "svalid[\"y_test\"] = svalid[\"y_test\"].asfactor()\n",
        "\n",
        "st = time.time()\n",
        "saml = H2OAutoML(max_models = 10, seed = 123, verbosity=\"info\", nfolds=10, sort_metric='accuracy')\n",
        "saml.train(x = shx, y = shy, training_frame = strain, validation_frame = svalid)\n",
        "sautoend = time.time() - st\n",
        "sbest_model = saml.get_best_model()\n",
        "sHATr  = sbest_model.model_performance(strain)\n",
        "sHATe  = sbest_model.model_performance(svalid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMDqylVK7svS",
        "outputId": "a7d83793-05a3-4ea6-aa13-aa45c75089c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "AutoML progress: |\n",
            "10:44:39.20: Project: AutoML_2_20230704_104439\n",
            "10:44:39.20: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
            "10:44:39.20: Setting stopping tolerance adaptively based on the training frame: 0.04688072309384954\n",
            "10:44:39.20: Build control seed: 123\n",
            "10:44:39.21: training frame: Frame key: AutoML_2_20230704_104439_training_py_11_sid_a072    cols: 7    rows: 455  chunks: 1    size: 6691  checksum: -4836358475592077104\n",
            "10:44:39.21: validation frame: Frame key: py_12_sid_a072    cols: 7    rows: 114  chunks: 1    size: 2553  checksum: -5131178475415970944\n",
            "10:44:39.21: leaderboard frame: NULL\n",
            "10:44:39.21: blending frame: NULL\n",
            "10:44:39.21: response column: y_test\n",
            "10:44:39.21: fold column: null\n",
            "10:44:39.21: weights column: null\n",
            "10:44:39.21: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "10:44:39.24: AutoML job created: 2023.07.04 10:44:39.19\n",
            "10:44:39.25: AutoML build started: 2023.07.04 10:44:39.25\n",
            "10:44:39.26: AutoML: starting XGBoost_1_AutoML_2_20230704_104439 model training\n",
            "\n",
            "█\n",
            "10:44:41.576: New leader: XGBoost_1_AutoML_2_20230704_104439, accuracy: 0.9362637362637363\n",
            "10:44:41.577: AutoML: starting GLM_1_AutoML_2_20230704_104439 model training\n",
            "\n",
            "\n",
            "10:44:42.489: AutoML: starting GBM_1_AutoML_2_20230704_104439 model training\n",
            "\n",
            "██\n",
            "10:44:45.210: New leader: GBM_1_AutoML_2_20230704_104439, accuracy: 0.9340659340659341\n",
            "10:44:45.213: AutoML: starting XGBoost_2_AutoML_2_20230704_104439 model training\n",
            "\n",
            "██\n",
            "10:44:47.820: AutoML: starting DRF_1_AutoML_2_20230704_104439 model training\n",
            "\n",
            "██\n",
            "10:44:50.157: AutoML: starting GBM_2_AutoML_2_20230704_104439 model training\n",
            "\n",
            "\n",
            "10:44:52.453: AutoML: starting GBM_3_AutoML_2_20230704_104439 model training\n",
            "\n",
            "██\n",
            "10:44:54.652: AutoML: starting GBM_4_AutoML_2_20230704_104439 model training\n",
            "\n",
            "█\n",
            "10:44:57.88: AutoML: starting XGBoost_3_AutoML_2_20230704_104439 model training\n",
            "\n",
            "██\n",
            "10:44:59.880: New leader: XGBoost_3_AutoML_2_20230704_104439, accuracy: 0.9340659340659341\n",
            "10:44:59.883: AutoML: starting XRT_1_AutoML_2_20230704_104439 model training\n",
            "\n",
            "█\n",
            "10:45:02.267: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
            "10:45:02.272: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_2_20230704_104439 model training\n",
            "\n",
            "███\n",
            "10:45:04.707: AutoML: starting StackedEnsemble_AllModels_1_AutoML_2_20230704_104439 model training\n",
            "\n",
            "███████████████████████████████████████████████| (done) 100%\n",
            "\n",
            "10:45:07.511: Actual modeling steps: [{XGBoost : [def_2 (1g, 10w)]}, {GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {XGBoost : [def_1 (2g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {XGBoost : [def_3 (3g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
            "10:45:07.511: AutoML build stopped: 2023.07.04 10:45:07.511\n",
            "10:45:07.511: AutoML build done: built 10 models\n",
            "10:45:07.511: AutoML duration: 28.486 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_sh2o = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))['predict']\n",
        "y_test_sh2o = np.array(svalid1['y_test']).copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivMaaDNNmYY2",
        "outputId": "ff5dcff5-6585-4ec9-be2b-fa052d544b2a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "nJrcI3Gn8we2",
        "outputId": "cb6291ac-d04d-4e15-80e5-df0bd75a324a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 7 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]\n",
        "\n",
        "label = [\"ArtificialNeuralNetwork\", 'DeepNeuralNetwork',\n",
        "         'KNearestNeighborsClassifier', 'LogisticRegression',\n",
        "         'RandomForestClassifier', 'SupportVectorClassifier',\n",
        "         'XGBoost', type(best_model).__name__, type(sbest_model).__name__ ]\n",
        "\n",
        "acc = pd.DataFrame(\n",
        "    {\n",
        "    \"ANN\":[ATr,ATe],\n",
        "    \"DNN\":[DTr,DTe],\n",
        "    \"KNN\":[KTr,KTe],\n",
        "    \"LR\" :[LTr,LTe],\n",
        "    \"RF\" :[RTr,RTe],\n",
        "    \"SVM\":[STr,STe],\n",
        "    \"XGB\":[XTr,XTe],\n",
        "    \"H_OD\":[HATr.accuracy()[0][1],HATe.accuracy()[0][1]],\n",
        "    \"H_SOD\":[sHATr.accuracy()[0][1],sHATe.accuracy()[0][1]]\n",
        "    })\n",
        "acc.index = [\"train\", \"test\"]\n",
        "acc = acc.T\n",
        "acc['Model'] = label\n",
        "\n",
        "acc = acc[['Model', 'train', 'test']]\n",
        "acc['avg'] = round((acc['train'] + acc['test'])/2, 6)\n",
        "acc[acc[\"avg\"] == acc[\"avg\"].max()]\n",
        "acc['BestModel'] = 0\n",
        "for i in range(len(acc)):\n",
        "  if acc['avg'][i] >= 90 and acc['avg'][i] < acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"good\"\n",
        "  elif acc['avg'][i] == acc['avg'].max():\n",
        "    acc.iloc[i,-1] = \"best\"\n",
        "  else:\n",
        "    acc.iloc[i,-1] = \"not good\"\n",
        "\n",
        "acc[\"Precision\"] = np.zeros(len(acc))\n",
        "acc[\"Recall\"]    = np.zeros(len(acc))\n",
        "acc[\"F1_Score\"]  = np.zeros(len(acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ba6kRO-eI60S"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ANNn = []\n",
        "y_pred_DNNn = []\n",
        "for i in range(len(y_pred_ANN)):\n",
        "  y_pred_ANNn.append(y_pred_ANN[i][0])\n",
        "  y_pred_DNNn.append(y_pred_DNN[i][0])"
      ],
      "metadata": {
        "id": "S7pf8G6ao4oF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [np.array(y_pred_ANNn), np.array(y_pred_DNNn), y_pred_knn,\n",
        "        y_pred_lr, y_pred_rf,\n",
        "        y_pred_svm, y_pred_xgb, y_pred_h2o, y_pred_sh2o]\n",
        "\n",
        "tes  = [y_test_indi_ML, np.array(Dy_test), y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_indi_ML, y_test_indi_ML, y_test_indi_ML,\n",
        "        y_test_h2o.copy(), y_test_sh2o.copy()]"
      ],
      "metadata": {
        "id": "D2n2VnW1jKp4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pred)):\n",
        "  p,r,f,_ = precision_recall_fscore_support(tes[i], pred[i],\n",
        "                                            average='macro')\n",
        "  acc.iloc[i,5]= p\n",
        "  acc.iloc[i,6]= r\n",
        "  acc.iloc[i,7]= f\n",
        "  p = 0\n",
        "  r = 0\n",
        "  f = 0\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "2DlSQ29moN9T",
        "outputId": "744d451a-7849-4426-8040-8eec6ab8fef2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Model     train      test       avg BestModel  \\\n",
              "ANN        ArtificialNeuralNetwork  0.960440  0.973684  0.967062  not good   \n",
              "DNN              DeepNeuralNetwork  0.947253  0.921053  0.934153  not good   \n",
              "KNN    KNearestNeighborsClassifier  0.980220  0.991228  0.985724  not good   \n",
              "LR              LogisticRegression  0.980220  0.973684  0.976952  not good   \n",
              "RF          RandomForestClassifier  0.975824  0.991228  0.983526  not good   \n",
              "SVM        SupportVectorClassifier  0.951648  0.956140  0.953894  not good   \n",
              "XGB                        XGBoost  0.993407  0.982456  0.987931      best   \n",
              "H_OD           H2OXGBoostEstimator  0.973626  0.964912  0.969269  not good   \n",
              "H_SOD          H2OXGBoostEstimator  0.938462  0.912281  0.925371  not good   \n",
              "\n",
              "       Precision    Recall  F1_Score  \n",
              "ANN     0.980263  0.963415  0.970946  \n",
              "DNN     0.927350  0.900936  0.911757  \n",
              "KNN     0.993243  0.987805  0.990426  \n",
              "LR      0.980263  0.963415  0.970946  \n",
              "RF      0.993243  0.987805  0.990426  \n",
              "SVM     0.967949  0.939024  0.950976  \n",
              "XGB     0.980956  0.980956  0.980956  \n",
              "H_OD    0.958074  0.967257  0.962302  \n",
              "H_SOD   0.896959  0.892583  0.894684  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ca082f3-5bd8-4fb0-8cc6-07d869d0e51f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>avg</th>\n",
              "      <th>BestModel</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>ArtificialNeuralNetwork</td>\n",
              "      <td>0.960440</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.967062</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.963415</td>\n",
              "      <td>0.970946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>DeepNeuralNetwork</td>\n",
              "      <td>0.947253</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.934153</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.927350</td>\n",
              "      <td>0.900936</td>\n",
              "      <td>0.911757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>KNearestNeighborsClassifier</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.985724</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.976952</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.963415</td>\n",
              "      <td>0.970946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.975824</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.983526</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>SupportVectorClassifier</td>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.953894</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.967949</td>\n",
              "      <td>0.939024</td>\n",
              "      <td>0.950976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.993407</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.987931</td>\n",
              "      <td>best</td>\n",
              "      <td>0.980956</td>\n",
              "      <td>0.980956</td>\n",
              "      <td>0.980956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_OD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.973626</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.969269</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.958074</td>\n",
              "      <td>0.967257</td>\n",
              "      <td>0.962302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_SOD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.938462</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.925371</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.896959</td>\n",
              "      <td>0.892583</td>\n",
              "      <td>0.894684</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ca082f3-5bd8-4fb0-8cc6-07d869d0e51f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ca082f3-5bd8-4fb0-8cc6-07d869d0e51f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ca082f3-5bd8-4fb0-8cc6-07d869d0e51f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(acc.index, acc['train'], color ='black',width = 0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ru4oH6-YzGPw",
        "outputId": "827cc9e1-4f78-4d59-bdf2-2ee3a1272cd5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 9 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve"
      ],
      "metadata": {
        "id": "Qma6hWrmDBvf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = [ANNmodel, model, knn, lr, rf, svm, xgb, best_model]"
      ],
      "metadata": {
        "id": "03rwpiYpoAGs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['diagnosis']).copy()\n",
        "X = df.drop(columns=['diagnosis']).copy()\n",
        "X = StandardScaler().fit_transform(X).copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n",
        "y_test_indi_ML = y_test.copy()"
      ],
      "metadata": {
        "id": "x5wrzn-qPaXw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "C80ESe__xB5x"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
        "\n",
        "y_pred = ANNmodel.predict(X_test).ravel()\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[0]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = model.predict(DX_test).ravel()\n",
        "y_test = Dy_test.copy()\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[1]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = knn.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[2]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = lr.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[3]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = rf.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[4]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = svm.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[5]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = xgb.predict_proba(X_test)[:, 1]\n",
        "y_test = y_test_indi_ML\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = metrics.roc_auc_score(y_test,y_pred)\n",
        "plt.plot(fpr, tpr, label=str(label[6]) + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(best_model.predict(valid)))\n",
        "y_test = h2o.as_list(valid['diagnosis'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(best_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "y_pred = pd.DataFrame(h2o.as_list(sbest_model.predict(svalid)))\n",
        "y_test = h2o.as_list(svalid['y_test'])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred[\"p1\"])\n",
        "auc = metrics.roc_auc_score(y_test, y_pred[\"p1\"])\n",
        "plt.plot(fpr, tpr,label=type(sbest_model).__name__ + '(area = {:.3f})'.format(auc))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('AUC ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "4Bi4CMUJm6yH",
        "outputId": "4037e656-de60-42e9-c08c-6cdb4565b293"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n",
            "xgboost prediction progress: |███████████████████████████████████████████████████| (done) 100%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tm = [aend,dend,kend,lend,rend,send,xend,autoend,sautoend]"
      ],
      "metadata": {
        "id": "6vUubKKMxLIP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['time'] = 0"
      ],
      "metadata": {
        "id": "_1wKoAe9wIPn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['time'] = tm"
      ],
      "metadata": {
        "id": "P-K86caJxhvX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Oc880RvD9ibh",
        "outputId": "d8f7b338-1c2e-4108-a9af-1a551d4dfe0f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Model     train      test       avg BestModel  \\\n",
              "ANN        ArtificialNeuralNetwork  0.960440  0.973684  0.967062  not good   \n",
              "DNN              DeepNeuralNetwork  0.947253  0.921053  0.934153  not good   \n",
              "KNN    KNearestNeighborsClassifier  0.980220  0.991228  0.985724  not good   \n",
              "LR              LogisticRegression  0.980220  0.973684  0.976952  not good   \n",
              "RF          RandomForestClassifier  0.975824  0.991228  0.983526  not good   \n",
              "SVM        SupportVectorClassifier  0.951648  0.956140  0.953894  not good   \n",
              "XGB                        XGBoost  0.993407  0.982456  0.987931      best   \n",
              "H_OD           H2OXGBoostEstimator  0.973626  0.964912  0.969269  not good   \n",
              "H_SOD          H2OXGBoostEstimator  0.938462  0.912281  0.925371  not good   \n",
              "\n",
              "       Precision    Recall  F1_Score       time  \n",
              "ANN     0.980263  0.963415  0.970946   3.653083  \n",
              "DNN     0.927350  0.900936  0.911757  83.814371  \n",
              "KNN     0.993243  0.987805  0.990426   0.001131  \n",
              "LR      0.980263  0.963415  0.970946   0.005294  \n",
              "RF      0.993243  0.987805  0.990426   0.927072  \n",
              "SVM     0.967949  0.939024  0.950976   0.046389  \n",
              "XGB     0.980956  0.980956  0.980956   1.767434  \n",
              "H_OD    0.958074  0.967257  0.962302  98.671937  \n",
              "H_SOD   0.896959  0.892583  0.894684  31.444488  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-451fc27f-7f18-40e2-8f01-e408264967af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>avg</th>\n",
              "      <th>BestModel</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1_Score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>ArtificialNeuralNetwork</td>\n",
              "      <td>0.960440</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.967062</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.963415</td>\n",
              "      <td>0.970946</td>\n",
              "      <td>3.653083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DNN</th>\n",
              "      <td>DeepNeuralNetwork</td>\n",
              "      <td>0.947253</td>\n",
              "      <td>0.921053</td>\n",
              "      <td>0.934153</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.927350</td>\n",
              "      <td>0.900936</td>\n",
              "      <td>0.911757</td>\n",
              "      <td>83.814371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>KNearestNeighborsClassifier</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.985724</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "      <td>0.001131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.980220</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.976952</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.980263</td>\n",
              "      <td>0.963415</td>\n",
              "      <td>0.970946</td>\n",
              "      <td>0.005294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.975824</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.983526</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>0.987805</td>\n",
              "      <td>0.990426</td>\n",
              "      <td>0.927072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>SupportVectorClassifier</td>\n",
              "      <td>0.951648</td>\n",
              "      <td>0.956140</td>\n",
              "      <td>0.953894</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.967949</td>\n",
              "      <td>0.939024</td>\n",
              "      <td>0.950976</td>\n",
              "      <td>0.046389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGB</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.993407</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.987931</td>\n",
              "      <td>best</td>\n",
              "      <td>0.980956</td>\n",
              "      <td>0.980956</td>\n",
              "      <td>0.980956</td>\n",
              "      <td>1.767434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_OD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.973626</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.969269</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.958074</td>\n",
              "      <td>0.967257</td>\n",
              "      <td>0.962302</td>\n",
              "      <td>98.671937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H_SOD</th>\n",
              "      <td>H2OXGBoostEstimator</td>\n",
              "      <td>0.938462</td>\n",
              "      <td>0.912281</td>\n",
              "      <td>0.925371</td>\n",
              "      <td>not good</td>\n",
              "      <td>0.896959</td>\n",
              "      <td>0.892583</td>\n",
              "      <td>0.894684</td>\n",
              "      <td>31.444488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-451fc27f-7f18-40e2-8f01-e408264967af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-451fc27f-7f18-40e2-8f01-e408264967af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-451fc27f-7f18-40e2-8f01-e408264967af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}